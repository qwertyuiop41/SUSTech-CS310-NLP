{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import jieba\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import SST2 # SST2 is the sentiment analysis dataset, binary\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                # Tokenize and convert to indices\n",
    "                tokens = [vocab[token] if token in vocab else vocab[\"<unk>\"] for token in re.findall(r'[\\u4e00-\\u9fff]+', item['sentence'])]\n",
    "                self.data.append((tokens, item['label'][0]))\n",
    "                \n",
    "        # Prepare for EmbeddingBag: concatenate all tokens and compute offsets\n",
    "        self.tokens = [token for tokens, _ in self.data for token in tokens]\n",
    "        self.offsets = [0] + [len(tokens) for tokens, _ in self.data]\n",
    "        self.offsets = torch.tensor(self.offsets[:-1]).cumsum(dim=0)\n",
    "        self.labels = [label for _, label in self.data]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens, self.offsets[idx], self.labels[idx]\n",
    "\n",
    "def build_vocab(file_path):\n",
    "    vocab = {\"<PAD>\": 0}  # Padding Token\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            for token in re.findall(r'[\\u4e00-\\u9fff]+', item['sentence']):\n",
    "                if token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    tokens, offsets, labels = [], [0], []  # Initialize offsets with 0 for the first sequence\n",
    "    for tokens_batch, _, label in batch:\n",
    "        labels.append(label)\n",
    "        tokens.extend(tokens_batch)\n",
    "        offsets.append(len(tokens))  # The next sequence starts after the current one ends\n",
    "    # Convert lists to tensors\n",
    "    tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "    offsets_tensor = torch.tensor(offsets[:-1], dtype=torch.long)  # Exclude the last offset\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    return tokens_tensor, offsets_tensor, labels_tensor\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(iter(SST2(split='train'))), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# vocab = build_vocab('train.jsonl')\n",
    "# Initialize dataset and dataloader\n",
    "train_dataset = TextDataset('train.jsonl', vocab)\n",
    "test_dataset=TextDataset('test.jsonl', vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader=DataLoader(test_dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class BoWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        self.embedding_bag = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding_bag(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "num_classes = 2\n",
    "model = BoWClassifier(vocab_size, embed_dim, num_classes)\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 0.001  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "sparse_parameters = [params for params in model.embedding_bag.parameters()]\n",
    "dense_parameters = [params for params in model.fc.parameters()]\n",
    "\n",
    "optimizer_sparse = optim.SparseAdam(sparse_parameters, lr=LR)\n",
    "optimizer_dense = optim.Adam(dense_parameters, lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "sparse_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_sparse, 1.0, gamma=0.1)\n",
    "dense_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_dense, 1.0, gamma=0.1)\n",
    "\n",
    "# predictions = []\n",
    "# true_labels = []\n",
    "# with torch.no_grad():  # No gradients needed for evaluation\n",
    "#     for tokens, offsets, labels in test_loader:\n",
    "#         output = model(tokens, offsets)\n",
    "#         pred_labels = output.argmax(dim=1)\n",
    "#         predictions.extend(pred_labels.cpu().numpy())\n",
    "#         true_labels.extend(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 1585 batches | accuracy    0.702\n",
      "| epoch   1 |   200/ 1585 batches | accuracy    0.709\n",
      "| epoch   1 |   300/ 1585 batches | accuracy    0.714\n",
      "| epoch   1 |   400/ 1585 batches | accuracy    0.730\n",
      "| epoch   1 |   500/ 1585 batches | accuracy    0.715\n",
      "| epoch   1 |   600/ 1585 batches | accuracy    0.734\n",
      "| epoch   1 |   700/ 1585 batches | accuracy    0.706\n",
      "| epoch   1 |   800/ 1585 batches | accuracy    0.736\n",
      "| epoch   1 |   900/ 1585 batches | accuracy    0.696\n",
      "| epoch   1 |  1000/ 1585 batches | accuracy    0.686\n",
      "| epoch   1 |  1100/ 1585 batches | accuracy    0.724\n",
      "| epoch   1 |  1200/ 1585 batches | accuracy    0.713\n",
      "| epoch   1 |  1300/ 1585 batches | accuracy    0.699\n",
      "| epoch   1 |  1400/ 1585 batches | accuracy    0.721\n",
      "| epoch   1 |  1500/ 1585 batches | accuracy    0.724\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 91.31s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   100/ 1585 batches | accuracy    0.735\n",
      "| epoch   2 |   200/ 1585 batches | accuracy    0.700\n",
      "| epoch   2 |   300/ 1585 batches | accuracy    0.708\n",
      "| epoch   2 |   400/ 1585 batches | accuracy    0.740\n",
      "| epoch   2 |   500/ 1585 batches | accuracy    0.709\n",
      "| epoch   2 |   600/ 1585 batches | accuracy    0.719\n",
      "| epoch   2 |   700/ 1585 batches | accuracy    0.670\n",
      "| epoch   2 |   800/ 1585 batches | accuracy    0.699\n",
      "| epoch   2 |   900/ 1585 batches | accuracy    0.716\n",
      "| epoch   2 |  1000/ 1585 batches | accuracy    0.700\n",
      "| epoch   2 |  1100/ 1585 batches | accuracy    0.688\n",
      "| epoch   2 |  1200/ 1585 batches | accuracy    0.710\n",
      "| epoch   2 |  1300/ 1585 batches | accuracy    0.719\n",
      "| epoch   2 |  1400/ 1585 batches | accuracy    0.731\n",
      "| epoch   2 |  1500/ 1585 batches | accuracy    0.714\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 106.12s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   100/ 1585 batches | accuracy    0.733\n",
      "| epoch   3 |   200/ 1585 batches | accuracy    0.706\n",
      "| epoch   3 |   300/ 1585 batches | accuracy    0.708\n",
      "| epoch   3 |   400/ 1585 batches | accuracy    0.720\n",
      "| epoch   3 |   500/ 1585 batches | accuracy    0.704\n",
      "| epoch   3 |   600/ 1585 batches | accuracy    0.696\n",
      "| epoch   3 |   700/ 1585 batches | accuracy    0.715\n",
      "| epoch   3 |   800/ 1585 batches | accuracy    0.704\n",
      "| epoch   3 |   900/ 1585 batches | accuracy    0.686\n",
      "| epoch   3 |  1000/ 1585 batches | accuracy    0.695\n",
      "| epoch   3 |  1100/ 1585 batches | accuracy    0.708\n",
      "| epoch   3 |  1200/ 1585 batches | accuracy    0.725\n",
      "| epoch   3 |  1300/ 1585 batches | accuracy    0.711\n",
      "| epoch   3 |  1400/ 1585 batches | accuracy    0.716\n",
      "| epoch   3 |  1500/ 1585 batches | accuracy    0.740\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 118.62s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   100/ 1585 batches | accuracy    0.709\n",
      "| epoch   4 |   200/ 1585 batches | accuracy    0.741\n",
      "| epoch   4 |   300/ 1585 batches | accuracy    0.745\n",
      "| epoch   4 |   400/ 1585 batches | accuracy    0.698\n",
      "| epoch   4 |   500/ 1585 batches | accuracy    0.691\n",
      "| epoch   4 |   600/ 1585 batches | accuracy    0.688\n",
      "| epoch   4 |   700/ 1585 batches | accuracy    0.690\n",
      "| epoch   4 |   800/ 1585 batches | accuracy    0.728\n",
      "| epoch   4 |   900/ 1585 batches | accuracy    0.716\n",
      "| epoch   4 |  1000/ 1585 batches | accuracy    0.718\n",
      "| epoch   4 |  1100/ 1585 batches | accuracy    0.705\n",
      "| epoch   4 |  1200/ 1585 batches | accuracy    0.708\n",
      "| epoch   4 |  1300/ 1585 batches | accuracy    0.691\n",
      "| epoch   4 |  1400/ 1585 batches | accuracy    0.721\n",
      "| epoch   4 |  1500/ 1585 batches | accuracy    0.736\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 119.47s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   100/ 1585 batches | accuracy    0.733\n",
      "| epoch   5 |   200/ 1585 batches | accuracy    0.710\n",
      "| epoch   5 |   300/ 1585 batches | accuracy    0.724\n",
      "| epoch   5 |   400/ 1585 batches | accuracy    0.671\n",
      "| epoch   5 |   500/ 1585 batches | accuracy    0.729\n",
      "| epoch   5 |   600/ 1585 batches | accuracy    0.723\n",
      "| epoch   5 |   700/ 1585 batches | accuracy    0.679\n",
      "| epoch   5 |   800/ 1585 batches | accuracy    0.730\n",
      "| epoch   5 |   900/ 1585 batches | accuracy    0.700\n",
      "| epoch   5 |  1000/ 1585 batches | accuracy    0.713\n",
      "| epoch   5 |  1100/ 1585 batches | accuracy    0.721\n",
      "| epoch   5 |  1200/ 1585 batches | accuracy    0.714\n",
      "| epoch   5 |  1300/ 1585 batches | accuracy    0.729\n",
      "| epoch   5 |  1400/ 1585 batches | accuracy    0.703\n",
      "| epoch   5 |  1500/ 1585 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 117.40s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   100/ 1585 batches | accuracy    0.699\n",
      "| epoch   6 |   200/ 1585 batches | accuracy    0.710\n",
      "| epoch   6 |   300/ 1585 batches | accuracy    0.682\n",
      "| epoch   6 |   400/ 1585 batches | accuracy    0.709\n",
      "| epoch   6 |   500/ 1585 batches | accuracy    0.706\n",
      "| epoch   6 |   600/ 1585 batches | accuracy    0.710\n",
      "| epoch   6 |   700/ 1585 batches | accuracy    0.700\n",
      "| epoch   6 |   800/ 1585 batches | accuracy    0.699\n",
      "| epoch   6 |   900/ 1585 batches | accuracy    0.728\n",
      "| epoch   6 |  1000/ 1585 batches | accuracy    0.713\n",
      "| epoch   6 |  1100/ 1585 batches | accuracy    0.710\n",
      "| epoch   6 |  1200/ 1585 batches | accuracy    0.724\n",
      "| epoch   6 |  1300/ 1585 batches | accuracy    0.719\n",
      "| epoch   6 |  1400/ 1585 batches | accuracy    0.746\n",
      "| epoch   6 |  1500/ 1585 batches | accuracy    0.733\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 117.15s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   100/ 1585 batches | accuracy    0.705\n",
      "| epoch   7 |   200/ 1585 batches | accuracy    0.720\n",
      "| epoch   7 |   300/ 1585 batches | accuracy    0.713\n",
      "| epoch   7 |   400/ 1585 batches | accuracy    0.705\n",
      "| epoch   7 |   500/ 1585 batches | accuracy    0.748\n",
      "| epoch   7 |   600/ 1585 batches | accuracy    0.734\n",
      "| epoch   7 |   700/ 1585 batches | accuracy    0.704\n",
      "| epoch   7 |   800/ 1585 batches | accuracy    0.728\n",
      "| epoch   7 |   900/ 1585 batches | accuracy    0.704\n",
      "| epoch   7 |  1000/ 1585 batches | accuracy    0.739\n",
      "| epoch   7 |  1100/ 1585 batches | accuracy    0.733\n",
      "| epoch   7 |  1200/ 1585 batches | accuracy    0.709\n",
      "| epoch   7 |  1300/ 1585 batches | accuracy    0.671\n",
      "| epoch   7 |  1400/ 1585 batches | accuracy    0.718\n",
      "| epoch   7 |  1500/ 1585 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 117.68s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   100/ 1585 batches | accuracy    0.697\n",
      "| epoch   8 |   200/ 1585 batches | accuracy    0.720\n",
      "| epoch   8 |   300/ 1585 batches | accuracy    0.694\n",
      "| epoch   8 |   400/ 1585 batches | accuracy    0.731\n",
      "| epoch   8 |   500/ 1585 batches | accuracy    0.690\n",
      "| epoch   8 |   600/ 1585 batches | accuracy    0.713\n",
      "| epoch   8 |   700/ 1585 batches | accuracy    0.735\n",
      "| epoch   8 |   800/ 1585 batches | accuracy    0.705\n",
      "| epoch   8 |   900/ 1585 batches | accuracy    0.714\n",
      "| epoch   8 |  1000/ 1585 batches | accuracy    0.713\n",
      "| epoch   8 |  1100/ 1585 batches | accuracy    0.696\n",
      "| epoch   8 |  1200/ 1585 batches | accuracy    0.694\n",
      "| epoch   8 |  1300/ 1585 batches | accuracy    0.719\n",
      "| epoch   8 |  1400/ 1585 batches | accuracy    0.721\n",
      "| epoch   8 |  1500/ 1585 batches | accuracy    0.734\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 117.05s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   100/ 1585 batches | accuracy    0.702\n",
      "| epoch   9 |   200/ 1585 batches | accuracy    0.711\n",
      "| epoch   9 |   300/ 1585 batches | accuracy    0.698\n",
      "| epoch   9 |   400/ 1585 batches | accuracy    0.706\n",
      "| epoch   9 |   500/ 1585 batches | accuracy    0.741\n",
      "| epoch   9 |   600/ 1585 batches | accuracy    0.755\n",
      "| epoch   9 |   700/ 1585 batches | accuracy    0.682\n",
      "| epoch   9 |   800/ 1585 batches | accuracy    0.713\n",
      "| epoch   9 |   900/ 1585 batches | accuracy    0.716\n",
      "| epoch   9 |  1000/ 1585 batches | accuracy    0.709\n",
      "| epoch   9 |  1100/ 1585 batches | accuracy    0.730\n",
      "| epoch   9 |  1200/ 1585 batches | accuracy    0.701\n",
      "| epoch   9 |  1300/ 1585 batches | accuracy    0.736\n",
      "| epoch   9 |  1400/ 1585 batches | accuracy    0.700\n",
      "| epoch   9 |  1500/ 1585 batches | accuracy    0.685\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 117.32s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   100/ 1585 batches | accuracy    0.683\n",
      "| epoch  10 |   200/ 1585 batches | accuracy    0.729\n",
      "| epoch  10 |   300/ 1585 batches | accuracy    0.711\n",
      "| epoch  10 |   400/ 1585 batches | accuracy    0.710\n",
      "| epoch  10 |   500/ 1585 batches | accuracy    0.714\n",
      "| epoch  10 |   600/ 1585 batches | accuracy    0.714\n",
      "| epoch  10 |   700/ 1585 batches | accuracy    0.720\n",
      "| epoch  10 |   800/ 1585 batches | accuracy    0.716\n",
      "| epoch  10 |   900/ 1585 batches | accuracy    0.711\n",
      "| epoch  10 |  1000/ 1585 batches | accuracy    0.721\n",
      "| epoch  10 |  1100/ 1585 batches | accuracy    0.724\n",
      "| epoch  10 |  1200/ 1585 batches | accuracy    0.703\n",
      "| epoch  10 |  1300/ 1585 batches | accuracy    0.694\n",
      "| epoch  10 |  1400/ 1585 batches | accuracy    0.736\n",
      "| epoch  10 |  1500/ 1585 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 93.86s | accuracy    0.739 |precision    0.546|recall    0.739\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# def eval(model,test_loader):\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "\n",
    "#     with torch.no_grad():  # No gradients needed for evaluation\n",
    "#         for tokens, offsets, labels in test_loader:\n",
    "#             output = model(tokens, offsets)\n",
    "#             pred_labels = output.argmax(dim=1)\n",
    "#             predictions.extend(pred_labels.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#     accuracy = accuracy_score(true_labels, predictions)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "#     print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "#     print(f\"Precision: {precision:.4f}\")\n",
    "#     print(f\"Recall: {recall:.4f}\")\n",
    "#     print(f\"F1 Score: {f1:.4f}\")\n",
    "def eval(model, test_loader):\n",
    "    # model.eval()\n",
    "    # total_acc, total_count = 0, 0\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     for idx, (tokens, offsets, labels) in enumerate(dataloader):\n",
    "    #         output = model(tokens, offsets)\n",
    "    #         loss = criterion(output, labels)\n",
    "    #         total_acc += (output.argmax(1) == labels).sum().item()\n",
    "    #         total_count += labels.size(0)\n",
    "    # return total_acc / total_count\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():  # No gradients needed for evaluation\n",
    "        for tokens, offsets, labels in test_loader:\n",
    "            output = model(tokens, offsets)\n",
    "            pred_labels = output.argmax(1)\n",
    "            predictions.extend(pred_labels.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f}\")\n",
    "    return accuracy,precision,recall\n",
    "# def train(model, train_loader, optimizer, criterion, epoch):\n",
    "#     model.train()\n",
    "#     log_interval=150\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         for idx, (tokens, offsets, labels) in enumerate(train_loader):\n",
    "#         # for tokens, offsets, labels in train_loader:\n",
    "#             optimizer_sparse.zero_grad()\n",
    "#             optimizer_dense.zero_grad()\n",
    "            \n",
    "#             output = model(tokens, offsets)\n",
    "#             loss = criterion(output, labels)\n",
    "            \n",
    "#             loss.backward()\n",
    "            \n",
    "#             optimizer_sparse.step()\n",
    "#             optimizer_dense.step()\n",
    "#             # print(idx)\n",
    "#             if idx % log_interval == 0 and idx > 0:\n",
    "#                 eval(model,test_loader)\n",
    "def train(model, train_loader,optimizer_sparse,optimizer_dense, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 100\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (tokens, offsets, labels) in enumerate(train_loader):\n",
    "        optimizer_sparse.zero_grad()\n",
    "        optimizer_dense.zero_grad()\n",
    "        output = model(tokens, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer_sparse.step()\n",
    "        optimizer_dense.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_loader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train(model, train_loader,optimizer_sparse,optimizer_dense, criterion, epoch)\n",
    "    accuracy,precision,recall = eval(model, test_loader)\n",
    "\n",
    "    if total_accu is not None and total_accu >= accuracy:\n",
    "        sparse_scheduler.step()\n",
    "        dense_scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"accuracy {:8.3f} |precision {:8.3f}|recall {:8.3f}\".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy,precision,recall\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7389\n",
      "Precision: 0.5459\n",
      "Recall: 0.7389\n",
      "F1 Score: 0.6279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(vocab_size, embed_dim, num_classes)\n",
    "model.load_state_dict(torch.load('model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/qd/pr_ybl6s7c7d3nbyj6yt9g0w0000gn/T/jieba.cache\n",
      "Loading model cost 0.654 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TextDataset_jieba(Dataset):\n",
    "    def __init__(self, file_path, vocab):\n",
    "        self.data = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                # Segment with Jieba\n",
    "                tokens = [vocab.get(token, 0) for token in jieba.cut(item['sentence'])]\n",
    "                self.data.append((tokens, item['label'][0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\n",
    "# Vocabulary Building Function\n",
    "def build_vocab_jieba(file_path):\n",
    "    vocab = {\"<PAD>\": 0}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            for token in jieba.cut(item['sentence']):\n",
    "                if token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Model Definition\n",
    "class BoWClassifier_jieba(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        super(BoWClassifier_jieba, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "def custom_collate_fn_jieba(batch):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    offsets = [0]  # Start with 0 offset\n",
    "\n",
    "    for data, label in batch:\n",
    "        labels.append(label)\n",
    "        tokens.extend(data)  # Flatten all tokens\n",
    "        offsets.append(len(tokens))  # Mark the end of the current sequence\n",
    "\n",
    "    # Convert to tensors\n",
    "    tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n",
    "    offsets_tensor = torch.tensor(offsets[:-1], dtype=torch.long)  # Exclude the last cumulative length\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return tokens_tensor, offsets_tensor, labels_tensor\n",
    "\n",
    "\n",
    "# Prepare Data\n",
    "vocab_jieba = build_vocab_jieba('train.jsonl')\n",
    "train_dataset_jieba = TextDataset_jieba('train.jsonl', vocab_jieba)\n",
    "train_loader_jieba = DataLoader(train_dataset_jieba, batch_size=32, shuffle=True, collate_fn=custom_collate_fn_jieba)\n",
    "test_dataset_jieba = TextDataset_jieba('test.jsonl', vocab_jieba)\n",
    "test_loader_jieba = DataLoader(test_dataset_jieba, batch_size=32, shuffle=False, collate_fn=custom_collate_fn_jieba)\n",
    "\n",
    "# Initialize Model\n",
    "model_jieba = BoWClassifier_jieba(len(vocab_jieba), embed_dim=100, num_classes=2)\n",
    "# optimizer_jieba = optim.Adam(model_jieba.parameters())\n",
    "criterion_jieba = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Assuming you have model_jieba defined\n",
    "sparse_parameters_jieba = list(filter(lambda p: p.requires_grad, model_jieba.embedding.parameters()))\n",
    "dense_parameters_jieba = list(filter(lambda p: p.requires_grad, model_jieba.fc.parameters()))\n",
    "\n",
    "optimizer_sparse_jieba = optim.SparseAdam(sparse_parameters_jieba, lr=0.001)\n",
    "optimizer_dense_jieba = optim.Adam(dense_parameters_jieba, lr=0.001)\n",
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    model_jieba.train()\n",
    "    for tokens, offsets, labels in train_loader_jieba:\n",
    "        # Zero gradients for both optimizers\n",
    "        optimizer_sparse_jieba.zero_grad()\n",
    "        optimizer_dense_jieba.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model_jieba(tokens, offsets)\n",
    "        loss = criterion_jieba(output, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer_sparse_jieba.step()\n",
    "        optimizer_dense_jieba.step()\n",
    "\n",
    "\n",
    "torch.save(model_jieba.state_dict(), 'model_jieba.pth')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7403993855606759\n",
      "Precision: 0.8078955453149003\n",
      "Recall: 0.7403993855606759\n",
      "F1 Score: 0.6315126587944339\n"
     ]
    }
   ],
   "source": [
    "model_jieba = BoWClassifier_jieba(len(vocab_jieba), embed_dim=100, num_classes=2)\n",
    "\n",
    "model_jieba.load_state_dict(torch.load('model_jieba.pth'))\n",
    "model_jieba.eval()  # Set the model to evaluation mode\n",
    "# Evaluation\n",
    "all_predictions, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for text, offsets, labels in test_loader_jieba:\n",
    "        output = model_jieba(text, offsets)\n",
    "        predictions = output.argmax(1)\n",
    "        all_predictions.extend(predictions.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "print(f\"Test Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
