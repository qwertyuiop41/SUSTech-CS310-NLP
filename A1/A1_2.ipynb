{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import jieba\n",
    "import paddle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 33\u001B[0m\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokens\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mmyDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain.jsonl\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m train_iterator \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(train_dataset)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21myield_tokens\u001B[39m(data_iter):\n",
      "Cell \u001B[0;32mIn[14], line 3\u001B[0m, in \u001B[0;36mmyDataset.__init__\u001B[0;34m(self, file_path)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, file_path):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m      4\u001B[0m         lines \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[1;32m      5\u001B[0m     processed_data \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'train.jsonl'"
     ]
    }
   ],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        processed_data = []\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            processed_data.append(json_data)\n",
    "        self.data = processed_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def improved_tokenizer(sentence):\n",
    "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "    digit_pattern = re.compile(r'\\d+')\n",
    "    english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    # 匹配除了中英文数字空格之外的特殊字符\n",
    "    punctuation_pattern = re.compile(r'[^\\u4e00-\\u9fff\\da-zA-Z\\s]')\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]|\\d+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', sentence)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield improved_tokenizer(item['sentence'])\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        #指定两个隐藏层，每个隐藏层由nn.Linear和nn.ReLU激活函数组成。最后一层是线性层，输出num_classes个类别\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        output = self.fc(embedded)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "            output = model(token_ids, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(output.argmax(1).tolist())\n",
    "\n",
    "    accuracy = total_acc / total_count\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 1  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare train, valid, and test data\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "test_dataset = myDataset('test.jsonl')\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "### Main Training Loop\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_tokenizer(sentence):\n",
    "    tokens=[]\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for seg in seg_list:\n",
    "        tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield jieba_tokenizer(item['sentence'])\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        #指定两个隐藏层，每个隐藏层由nn.Linear和nn.ReLU激活函数组成。最后一层是线性层，输出num_classes个类别\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        output = self.fc(embedded)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 1  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "# Prepare train, valid, and test data\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "test_dataset = myDataset('test.jsonl')\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "### Main Training Loop\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    accuracy, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, total_accu,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_jieba_model.pth\")\n",
    "# accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
