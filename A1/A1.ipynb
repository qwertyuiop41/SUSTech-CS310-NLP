{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import adabound as adabound\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import jieba\n",
    "import paddle\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\n",
    "import  adabound\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说']\n",
      "['保', '姆', '小', '张', '说', '：', '干', '啥', '子', '嘛', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '，', '鸟', '朦', '胧']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '，', '疲', '惫', '的', '双', '腿', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '快', '把', '我', '累', '死', '了']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说', '亲', '爱', '的', '大', '姐', '你', '贵', '姓', '啊', '？']\n",
      "['保', '姆', '小', '张', '说', '：', '我', '免', '贵', '姓', '张', '我', '叫', '张', '凤', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '凤', '姑']\n",
      "Vocabulary size: 2820\n",
      "Token: 保, Index: 74\n",
      "Token: 说, Index: 1\n",
      "Token: ，, Index: 6\n",
      "Token: 小, Index: 23\n",
      "batch 0 label: tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "batch 0 text: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "batch 0 offsets: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Number of tokens:  8\n",
      "Number of examples in one batch:  8\n"
     ]
    }
   ],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        processed_data = []\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            processed_data.append(json_data)\n",
    "        self.data = processed_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def improved_tokenizer(sentence):\n",
    "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "    digit_pattern = re.compile(r'\\d+')\n",
    "    english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    # 匹配除了中英文数字空格之外的特殊字符\n",
    "    punctuation_pattern = re.compile(r'[^\\u4e00-\\u9fff\\da-zA-Z\\s]')\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]|\\d+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', sentence)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield improved_tokenizer(item['sentence'])\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791],\n",
      "        [0.0545, 0.0791]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        #指定两个隐藏层，每个隐藏层由nn.Linear和nn.ReLU激活函数组成。最后一层是线性层，输出num_classes个类别\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        output = self.fc(embedded)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.5660)\n",
      "loss non-reduced: tensor([0.2409, 1.5415, 0.2409, 0.2409, 1.5415, 0.2409, 0.2409, 0.2409])\n",
      "mean of loss non-reduced: tensor(0.5660)\n",
      "loss manually computed: tensor(0.2409)\n",
      "| epoch   1 |   500/ 1585 batches | accuracy    0.709\n",
      "| epoch   1 |  1000/ 1585 batches | accuracy    0.714\n",
      "| epoch   1 |  1500/ 1585 batches | accuracy    0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.56s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1585 batches | accuracy    0.701\n",
      "| epoch   2 |  1000/ 1585 batches | accuracy    0.721\n",
      "| epoch   2 |  1500/ 1585 batches | accuracy    0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.48s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1585 batches | accuracy    0.721\n",
      "| epoch   3 |  1000/ 1585 batches | accuracy    0.708\n",
      "| epoch   3 |  1500/ 1585 batches | accuracy    0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.56s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1585 batches | accuracy    0.713\n",
      "| epoch   4 |  1000/ 1585 batches | accuracy    0.710\n",
      "| epoch   4 |  1500/ 1585 batches | accuracy    0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.64s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1585 batches | accuracy    0.720\n",
      "| epoch   5 |  1000/ 1585 batches | accuracy    0.706\n",
      "| epoch   5 |  1500/ 1585 batches | accuracy    0.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.58s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1585 batches | accuracy    0.721\n",
      "| epoch   6 |  1000/ 1585 batches | accuracy    0.711\n",
      "| epoch   6 |  1500/ 1585 batches | accuracy    0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 14.32s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1585 batches | accuracy    0.709\n",
      "| epoch   7 |  1000/ 1585 batches | accuracy    0.714\n",
      "| epoch   7 |  1500/ 1585 batches | accuracy    0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.90s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1585 batches | accuracy    0.705\n",
      "| epoch   8 |  1000/ 1585 batches | accuracy    0.709\n",
      "| epoch   8 |  1500/ 1585 batches | accuracy    0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.39s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1585 batches | accuracy    0.720\n",
      "| epoch   9 |  1000/ 1585 batches | accuracy    0.714\n",
      "| epoch   9 |  1500/ 1585 batches | accuracy    0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.40s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1585 batches | accuracy    0.726\n",
      "| epoch  10 |  1000/ 1585 batches | accuracy    0.717\n",
      "| epoch  10 |  1500/ 1585 batches | accuracy    0.694\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.04s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "            output = model(token_ids, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(output.argmax(1).cpu().numpy())\n",
    "\n",
    "    #         accuracy = accuracy_score(true_labels, predictions)\n",
    "    # precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    accuracy = total_acc/total_count\n",
    "    precision = precision_score(y_true, y_pred,average='weighted')\n",
    "    recall = recall_score(y_true, y_pred,average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred,average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 1  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "# scheduler=torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_dataloader), epochs=10)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare train, valid, and test data\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "test_dataset = myDataset('test.jsonl')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "### Main Training Loop\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_dataloader), epochs=10)\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model, test_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13847\n",
      "Token: 保, Index: 4142\n",
      "Token: 说, Index: 2\n",
      "Token: ，, Index: 4\n",
      "Token: 小, Index: 79\n",
      "batch 0 label: tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "batch 0 text: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "batch 0 offsets: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Number of tokens:  8\n",
      "Number of examples in one batch:  8\n",
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199],\n",
      "        [-0.0730, -0.3199]])\n",
      "loss: tensor(0.6390)\n",
      "loss non-reduced: tensor([0.5773, 0.5773, 0.8242, 0.5773, 0.5773, 0.8242, 0.5773, 0.5773])\n",
      "mean of loss non-reduced: tensor(0.6390)\n",
      "loss manually computed: tensor(0.5773)\n",
      "| epoch   1 |   500/ 1585 batches | accuracy    0.715\n",
      "| epoch   1 |  1000/ 1585 batches | accuracy    0.711\n",
      "| epoch   1 |  1500/ 1585 batches | accuracy    0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.65s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1585 batches | accuracy    0.720\n",
      "| epoch   2 |  1000/ 1585 batches | accuracy    0.710\n",
      "| epoch   2 |  1500/ 1585 batches | accuracy    0.711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.47s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1585 batches | accuracy    0.712\n",
      "| epoch   3 |  1000/ 1585 batches | accuracy    0.709\n",
      "| epoch   3 |  1500/ 1585 batches | accuracy    0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 14.93s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1585 batches | accuracy    0.709\n",
      "| epoch   4 |  1000/ 1585 batches | accuracy    0.709\n",
      "| epoch   4 |  1500/ 1585 batches | accuracy    0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.46s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1585 batches | accuracy    0.713\n",
      "| epoch   5 |  1000/ 1585 batches | accuracy    0.720\n",
      "| epoch   5 |  1500/ 1585 batches | accuracy    0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.06s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1585 batches | accuracy    0.706\n",
      "| epoch   6 |  1000/ 1585 batches | accuracy    0.704\n",
      "| epoch   6 |  1500/ 1585 batches | accuracy    0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 11.99s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1585 batches | accuracy    0.708\n",
      "| epoch   7 |  1000/ 1585 batches | accuracy    0.711\n",
      "| epoch   7 |  1500/ 1585 batches | accuracy    0.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.02s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1585 batches | accuracy    0.719\n",
      "| epoch   8 |  1000/ 1585 batches | accuracy    0.703\n",
      "| epoch   8 |  1500/ 1585 batches | accuracy    0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.04s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1585 batches | accuracy    0.710\n",
      "| epoch   9 |  1000/ 1585 batches | accuracy    0.717\n",
      "| epoch   9 |  1500/ 1585 batches | accuracy    0.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 11.95s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1585 batches | accuracy    0.725\n",
      "| epoch  10 |  1000/ 1585 batches | accuracy    0.705\n",
      "| epoch  10 |  1500/ 1585 batches | accuracy    0.708\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.42s | valid accuracy    0.739 | precision    0.546 | recall  0.739 |f1 0.628 \n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "def jieba_tokenizer(sentence):\n",
    "    tokens=[]\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for seg in seg_list:\n",
    "        tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield jieba_tokenizer(item['sentence'])\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        #指定两个隐藏层，每个隐藏层由nn.Linear和nn.ReLU激活函数组成。最后一层是线性层，输出num_classes个类别\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        output = self.fc(embedded)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model_jieba = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model_jieba.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model_jieba(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 1  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_jieba.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_dataloader), epochs=10)\n",
    "# scheduler= torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model_jieba.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model_jieba(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "# Prepare train, valid, and test data\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "test_dataset = myDataset('test.jsonl')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "### Main Training Loop\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_dataloader), epochs=10)\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model_jieba, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model_jieba, test_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, total_accu,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model_jieba.state_dict(), \"model_jieba.pth\")\n",
    "# accu_test = evaluate(model_jieba, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
