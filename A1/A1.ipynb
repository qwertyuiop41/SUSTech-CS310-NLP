{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1. Neural Text Classification\n",
    "## CS310 Natural Language Processing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from Lab 2. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import jieba\n",
    "import paddle\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说']\n",
      "['保', '姆', '小', '张', '说', '：', '干', '啥', '子', '嘛', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '你', '看', '你', '往', '星', '空', '看', '月', '朦', '胧', '，', '鸟', '朦', '胧']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '咱', '是', '不', '是', '歇', '一', '下', '这', '双', '，', '疲', '惫', '的', '双', '腿', '？']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '快', '把', '我', '累', '死', '了']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '我', '说', '亲', '爱', '的', '大', '姐', '你', '贵', '姓', '啊', '？']\n",
      "['保', '姆', '小', '张', '说', '：', '我', '免', '贵', '姓', '张', '我', '叫', '张', '凤', '姑']\n",
      "['卖', '油', '条', '小', '刘', '说', '：', '凤', '姑']\n",
      "Vocabulary size: 2820\n",
      "Token: 保, Index: 74\n",
      "Token: 说, Index: 1\n",
      "Token: ，, Index: 6\n",
      "Token: 小, Index: 23\n",
      "0\n",
      "(tensor([0, 0, 1, 0, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "1\n",
      "(tensor([0, 0, 0, 1, 1, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "2\n",
      "(tensor([0, 1, 0, 1, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "3\n",
      "(tensor([0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "4\n",
      "(tensor([0, 1, 0, 1, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "5\n",
      "(tensor([0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "6\n",
      "(tensor([0, 1, 1, 0, 1, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "7\n",
      "(tensor([0, 1, 0, 1, 0, 1, 1, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "batch 0 label: tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "batch 0 text: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "batch 0 offsets: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Number of tokens:  8\n",
      "Number of examples in one batch:  8\n",
      "Example 1:  tensor([0])\n",
      "Example 8:  tensor([0])\n"
     ]
    }
   ],
   "source": [
    "class myDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "        processed_data = []\n",
    "        for line in lines:\n",
    "            json_data = json.loads(line)\n",
    "            processed_data.append(json_data)\n",
    "        self.data = processed_data\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]', sentence)\n",
    "    return tokens\n",
    "\n",
    "def improved_tokenizer(sentence):\n",
    "    chinese_pattern = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "    digit_pattern = re.compile(r'\\d+')\n",
    "    english_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    # 匹配除了中英文数字空格之外的特殊字符\n",
    "    punctuation_pattern = re.compile(r'[^\\u4e00-\\u9fff\\da-zA-Z\\s]')\n",
    "    tokens = re.findall(r'[\\u4e00-\\u9fff]|\\d+|[a-zA-Z]+|[^\\u4e00-\\u9fff\\da-zA-Z\\s]', sentence)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield improved_tokenizer(item['sentence'])\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(improved_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "count = 0\n",
    "# Test the dataloader\n",
    "for i, item in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 1: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 8: ', token_ids[offsets[7]:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507],\n",
      "        [0.0500, 0.0507]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=False)\n",
    "        #指定两个隐藏层，每个隐藏层由nn.Linear和nn.ReLU激活函数组成。最后一层是线性层，输出num_classes个类别\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, offsets):\n",
    "        embedded = self.embedding(token_ids, offsets)\n",
    "        output = self.fc(embedded)\n",
    "        return output\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 256\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.7067)\n",
      "loss non-reduced: tensor([0.2744, 1.4272, 0.2744, 0.2744, 0.2744, 1.4272, 0.2744, 1.4272])\n",
      "mean of loss non-reduced: tensor(0.7067)\n",
      "loss manually computed: tensor(0.2744)\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   1 |   500/ 1506 batches | accuracy    0.692\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.687\n",
      "| epoch   1 |  1500/ 1506 batches | accuracy    0.698\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  7.21s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   2 |   500/ 1506 batches | accuracy    0.698\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.668\n",
      "| epoch   2 |  1500/ 1506 batches | accuracy    0.694\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  7.26s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   3 |   500/ 1506 batches | accuracy    0.706\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.679\n",
      "| epoch   3 |  1500/ 1506 batches | accuracy    0.688\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.55s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   4 |   500/ 1506 batches | accuracy    0.688\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.698\n",
      "| epoch   4 |  1500/ 1506 batches | accuracy    0.688\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.56s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   5 |   500/ 1506 batches | accuracy    0.684\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.672\n",
      "| epoch   5 |  1500/ 1506 batches | accuracy    0.707\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  6.59s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   6 |   500/ 1506 batches | accuracy    0.687\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.701\n",
      "| epoch   6 |  1500/ 1506 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  6.51s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   7 |   500/ 1506 batches | accuracy    0.696\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.689\n",
      "| epoch   7 |  1500/ 1506 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  6.51s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   8 |   500/ 1506 batches | accuracy    0.694\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.696\n",
      "| epoch   8 |  1500/ 1506 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.59s | valid accuracy    0.287 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   9 |   500/ 1506 batches | accuracy    0.710\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.720\n",
      "| epoch   9 |  1500/ 1506 batches | accuracy    0.708\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.51s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch  10 |   500/ 1506 batches | accuracy    0.716\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.712\n",
      "| epoch  10 |  1500/ 1506 batches | accuracy    0.710\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  6.59s | valid accuracy    0.713 \n",
      "-----------------------------------------------------------\n",
      "test accuracy    0.713\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion, epoch: int):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(token_ids, offsets)\n",
    "        try:\n",
    "            loss = criterion(output, labels)\n",
    "        except Exception:\n",
    "            print('Error in loss calculation')\n",
    "            print('output: ', output.size())\n",
    "            print('labels: ', labels.size())\n",
    "            # print('token_ids: ', token_ids)\n",
    "            # print('offsets: ', offsets)\n",
    "            raise\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (output.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# def evaluate(model, dataloader, criterion):\n",
    "#     model.eval()\n",
    "#     total_acc, total_count = 0, 0\n",
    "#\n",
    "#     with torch.no_grad():\n",
    "#         for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "#             output = model(text, offsets)\n",
    "#             loss = criterion(output, label)\n",
    "#             total_acc += (output.argmax(1) == label).sum().item()\n",
    "#             total_count += label.size(0)\n",
    "#\n",
    "#     return total_acc / total_count\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (labels, token_ids, offsets) in enumerate(dataloader):\n",
    "            output = model(token_ids, offsets)\n",
    "            loss = criterion(output, labels)\n",
    "            total_acc += (output.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(output.argmax(1).tolist())\n",
    "\n",
    "    accuracy = total_acc / total_count\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10  # epoch\n",
    "LR = 0.1  # learning rate\n",
    "BATCH_SIZE = 8  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare train, valid, and test data\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "test_dataset = myDataset('test.jsonl')\n",
    "# train_dataset = to_map_style_dataset(train_iter)\n",
    "# test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "### Main Training Loop\n",
    "\n",
    "# Run the training loop\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    print('$$$$$$$$$$$$$$$$$')\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_model.pth\")\n",
    "# accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG 2024-03-05 11:06:42,886 __init__.py:113] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/jieba.cache\n",
      "DEBUG 2024-03-05 11:06:42,887 __init__.py:132] Loading model from cache /var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/jieba.cache\n",
      "Loading model cost 0.494 seconds.\n",
      "DEBUG 2024-03-05 11:06:43,380 __init__.py:164] Loading model cost 0.494 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "DEBUG 2024-03-05 11:06:43,381 __init__.py:166] Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['卖', '油条', '小', '刘说', '：', '我', '说']\n",
      "['保姆', '小张', '说', '：', '干', '啥子', '嘛', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '你', '看', '你', '往', '星空', '看', '月', '朦胧', '，', '鸟', '朦胧']\n",
      "['卖', '油条', '小', '刘说', '：', '咱', '是不是', '歇', '一下', '这', '双', '，', '疲惫', '的', '双腿', '？']\n",
      "['卖', '油条', '小', '刘说', '：', '快', '把', '我', '累死', '了']\n",
      "['卖', '油条', '小', '刘说', '：', '我', '说', '亲爱', '的', '大姐', '你', '贵姓', '啊', '？']\n",
      "['保姆', '小张', '说', '：', '我免', '贵姓', '张', '我', '叫', '张凤姑']\n",
      "['卖', '油条', '小', '刘说', '：', '凤姑']\n",
      "Vocabulary size: 13844\n",
      "Token: 保, Index: 4141\n",
      "Token: 说, Index: 2\n",
      "Token: ，, Index: 4\n",
      "Token: 小, Index: 82\n",
      "0\n",
      "(tensor([0, 0, 1, 0, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "1\n",
      "(tensor([0, 0, 0, 1, 1, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "2\n",
      "(tensor([0, 1, 0, 1, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "3\n",
      "(tensor([0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "4\n",
      "(tensor([0, 1, 0, 1, 0, 1, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "5\n",
      "(tensor([0, 0, 1, 0, 0, 0, 0, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "6\n",
      "(tensor([0, 1, 1, 0, 1, 0, 0, 1]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "7\n",
      "(tensor([0, 1, 0, 1, 0, 1, 1, 0]), tensor([0, 0, 0, 0, 0, 0, 0, 0]), tensor([0, 1, 2, 3, 4, 5, 6, 7]))\n",
      "batch 0 label: tensor([0, 0, 1, 0, 0, 1, 0, 0])\n",
      "batch 0 text: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n",
      "batch 0 offsets: tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Number of tokens:  8\n",
      "Number of examples in one batch:  8\n",
      "Example 1:  tensor([0])\n",
      "Example 8:  tensor([0])\n",
      "output size: torch.Size([8, 2])\n",
      "output: tensor([[-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392],\n",
      "        [-0.0783,  0.0392]])\n",
      "loss: tensor(0.7242)\n",
      "loss non-reduced: tensor([0.7536, 0.7536, 0.6361, 0.7536, 0.7536, 0.6361, 0.7536, 0.7536])\n",
      "mean of loss non-reduced: tensor(0.7242)\n",
      "loss manually computed: tensor(0.7536)\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   1 |   500/ 1506 batches | accuracy    0.677\n",
      "| epoch   1 |  1000/ 1506 batches | accuracy    0.683\n",
      "| epoch   1 |  1500/ 1506 batches | accuracy    0.701\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.33s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   2 |   500/ 1506 batches | accuracy    0.670\n",
      "| epoch   2 |  1000/ 1506 batches | accuracy    0.710\n",
      "| epoch   2 |  1500/ 1506 batches | accuracy    0.682\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  7.67s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   3 |   500/ 1506 batches | accuracy    0.680\n",
      "| epoch   3 |  1000/ 1506 batches | accuracy    0.683\n",
      "| epoch   3 |  1500/ 1506 batches | accuracy    0.693\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  6.75s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   4 |   500/ 1506 batches | accuracy    0.688\n",
      "| epoch   4 |  1000/ 1506 batches | accuracy    0.681\n",
      "| epoch   4 |  1500/ 1506 batches | accuracy    0.683\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  6.83s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   5 |   500/ 1506 batches | accuracy    0.690\n",
      "| epoch   5 |  1000/ 1506 batches | accuracy    0.677\n",
      "| epoch   5 |  1500/ 1506 batches | accuracy    0.695\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  8.12s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   6 |   500/ 1506 batches | accuracy    0.687\n",
      "| epoch   6 |  1000/ 1506 batches | accuracy    0.684\n",
      "| epoch   6 |  1500/ 1506 batches | accuracy    0.675\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  7.54s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   7 |   500/ 1506 batches | accuracy    0.697\n",
      "| epoch   7 |  1000/ 1506 batches | accuracy    0.687\n",
      "| epoch   7 |  1500/ 1506 batches | accuracy    0.686\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  7.68s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   8 |   500/ 1506 batches | accuracy    0.698\n",
      "| epoch   8 |  1000/ 1506 batches | accuracy    0.686\n",
      "| epoch   8 |  1500/ 1506 batches | accuracy    0.694\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  7.61s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch   9 |   500/ 1506 batches | accuracy    0.691\n",
      "| epoch   9 |  1000/ 1506 batches | accuracy    0.702\n",
      "| epoch   9 |  1500/ 1506 batches | accuracy    0.678\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  7.40s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "$$$$$$$$$$$$$$$$$\n",
      "| epoch  10 |   500/ 1506 batches | accuracy    0.691\n",
      "| epoch  10 |  1000/ 1506 batches | accuracy    0.683\n",
      "| epoch  10 |  1500/ 1506 batches | accuracy    0.696\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.52s | valid accuracy    0.722 \n",
      "-----------------------------------------------------------\n",
      "test accuracy    0.722\n"
     ]
    }
   ],
   "source": [
    "def jieba_tokenizer(sentence):\n",
    "    tokens=[]\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for seg in seg_list:\n",
    "        tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "train_dataset = myDataset('train.jsonl')\n",
    "\n",
    "train_iterator = iter(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for item in data_iter:\n",
    "        yield jieba_tokenizer(item['sentence'])\n",
    "\n",
    "count = 0\n",
    "for tokens in yield_tokens(train_iterator): # Use a new iterator\n",
    "    print(tokens)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iterator), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# Print a few random tokens and their corresponding indices\n",
    "for token in ['保', '说', '，', '小']:\n",
    "    print(f\"Token: {token}, Index: {vocab[token]}\")\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab(jieba_tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, token_ids_list, offsets = [], [], [0]\n",
    "    for item in batch:\n",
    "        label_list.append(label_pipeline(item['label'][0]))\n",
    "        token_ids = torch.tensor(text_pipeline('sentence'), dtype=torch.int64)\n",
    "        token_ids_list.append(token_ids)\n",
    "        offsets.append(token_ids.size(0))\n",
    "\n",
    "    labels = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    token_ids = torch.cat(token_ids_list)\n",
    "\n",
    "    return labels.to(device), token_ids.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False,collate_fn=collate_batch)\n",
    "count = 0\n",
    "# Test the dataloader\n",
    "for i, item in enumerate(train_dataloader):\n",
    "    print(i)\n",
    "    print(item)\n",
    "    count += 1\n",
    "    if count > 7:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Test the dataloader\n",
    "for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "    print(f\"batch {i} label: {labels}\")\n",
    "    print(f\"batch {i} text: {token_ids}\")\n",
    "    print(f\"batch {i} offsets: {offsets}\")\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "# What does offsets mean?\n",
    "print('Number of tokens: ', token_ids.size(0))\n",
    "print('Number of examples in one batch: ', labels.size(0))\n",
    "print('Example 1: ', token_ids[offsets[0]:offsets[1]])\n",
    "print('Example 8: ', token_ids[offsets[7]:])\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "# Example usage\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 512\n",
    "num_classes = 2 #0,1\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "# Examine the output\n",
    "print('output size:', output.size())\n",
    "print('output:', output)\n",
    "\n",
    "\n",
    "########################################################\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "# First, obtain some output and labels\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (labels, token_ids, offsets) in enumerate(train_dataloader):\n",
    "        output = model(token_ids, offsets)\n",
    "        # print(f\"batch {i} output: {output}\")\n",
    "        if i == 0:\n",
    "            break\n",
    "\n",
    "loss = criterion(output, labels)\n",
    "print('loss:', loss)\n",
    "\n",
    "criterion2 = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "loss2 = criterion2(output, labels)\n",
    "print('loss non-reduced:', loss2)\n",
    "print('mean of loss non-reduced:', torch.mean(loss2))\n",
    "\n",
    "# Manually calculate the loss\n",
    "probs = torch.exp(output[0,:]) / torch.exp(output[0,:]).sum()\n",
    "loss3 = -torch.log(probs[labels[0]])\n",
    "print('loss manually computed:', loss3)\n",
    "\n",
    "\n",
    "\n",
    "total_accu = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    print('$$$$$$$$$$$$$$$$$')\n",
    "    train(model, train_dataloader, optimizer, criterion, epoch)\n",
    "    accuracy, precision, recall, f1 = evaluate(model, valid_dataloader, criterion)\n",
    "\n",
    "    if total_accu is not None and total_accu > accuracy:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accuracy\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} | precision {:8.3f} | recall {:6.3f} |f1 {:5.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accuracy,precision,recall,f1\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"text_classification_jieba_model.pth\")\n",
    "# accu_test = evaluate(model, valid_dataloader, criterion)\n",
    "# print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
