{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the code from Lab 7\n",
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        self.stack.append(self.buffer.pop())\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        self.deps.add((self.stack[-1], self.stack[-2], label))\n",
    "        self.stack.pop(-2)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        self.deps.add((self.stack[-2], self.stack[-1], label))\n",
    "        self.stack.pop(-1)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)\n",
    "\n",
    "\n",
    "def get_training_instances(dep_tree) -> List[Tuple[State, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0)  # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "\n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "\n",
    "        # Decide transition action\n",
    "        if stack_top2.head == stack_top1.id:  # Left-Arc\n",
    "            seq.append((copy.deepcopy(state), (\"left_arc\", stack_top2.deprel)))\n",
    "            state.left_arc(stack_top2.deprel)\n",
    "            childcount[stack_top1.id] -= 1\n",
    "        elif stack_top1.head == stack_top2.id:  # Right-Arc\n",
    "            if childcount[stack_top1.id] != 0:\n",
    "                seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "                state.shift()\n",
    "            else:\n",
    "                seq.append((copy.deepcopy(state), (\"right_arc\", stack_top1.deprel)))\n",
    "                state.right_arc(stack_top1.deprel)\n",
    "                childcount[stack_top2.id] -= 1\n",
    "        else:  # Shift\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "\n",
    "        # print(seq[-1])\n",
    "\n",
    "    seq.append((copy.deepcopy(state), (\"done\", None)))\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def process(\n",
    "    dep_trees: List[DependencyTree],\n",
    "    word_vocab: dict, word_vectors: dict,\n",
    "    pos_vocab: dict, pos_vectors: dict,\n",
    "    action_vocab: dict) -> torch.Tensor:\n",
    "    tensor_data = []\n",
    "    tensor_truth = []\n",
    "    for tree in dep_trees:\n",
    "        instances = get_training_instances(tree)\n",
    "        for state, action in instances:\n",
    "            if action[0] == \"done\":\n",
    "                continue\n",
    "            # convert to torch tensor and append to tensor_data\n",
    "            # use stack[-3:] and buffer[-3:] to get the top 3 elements of the stack and buffer\n",
    "            # if the stack or buffer has less than 3 elements, use the <NULL> to pad\n",
    "            stack = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "            buffer = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "            # use word_vocab, pos_vocab to convert the word, pos, action to index\n",
    "            stack_idxes = []\n",
    "            stack_pos_idxes = []\n",
    "            buffer_idxes = []\n",
    "            buffer_pos_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif s == 0:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    stack_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(tree.deprels[s].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "                    stack_pos_idxes.extend(\n",
    "                        pos_vectors[\n",
    "                            pos_vocab.get(tree.deprels[s].pos, pos_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif b == 0:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    buffer_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(tree.deprels[b].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "                    buffer_pos_idxes.extend(\n",
    "                        pos_vectors[\n",
    "                            pos_vocab.get(tree.deprels[b].pos, pos_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            # concatenate all index to get word vectors\n",
    "            data_vector = torch.tensor(stack_idxes + buffer_idxes + stack_pos_idxes + buffer_pos_idxes)\n",
    "            tensor_data.append(data_vector)\n",
    "            tensor_truth.append(action_vocab[action])\n",
    "\n",
    "    return (torch.stack(tensor_data), torch.tensor(tensor_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "    def __init__(self, vec_dim, hidden_dim, action_size, dropout=0.2):\n",
    "        super(Parser, self).__init__()\n",
    "        self.input_dim = vec_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = action_size\n",
    "        self.dropout = dropout\n",
    "        self.W_1w = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.W_1t = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.W_2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        # self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_2(self.ReLU(self.W_1w(x[:x.shape[0]//2]) + self.W_1t(x[x.shape[0]//2:])))\n",
    "\n",
    "    def parse_sentence(\n",
    "        self, sentence,\n",
    "        word_vocab: dict, word_vectors: dict,\n",
    "        pos_vocab: dict, pos_vectors: dict,\n",
    "        action_vocab: dict,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        state = State(sentence)\n",
    "        state.stack.append(0)\n",
    "        while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "            stack = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "            buffer = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "            stack_idxes = []\n",
    "            stack_pos_idxes = []\n",
    "            buffer_idxes = []\n",
    "            buffer_pos_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif s == 0:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    stack_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(sentence[s].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "                    stack_pos_idxes.extend(\n",
    "                        pos_vectors[pos_vocab.get(sentence[s].pos, pos_vocab[\"<NULL>\"])]\n",
    "                    )\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif b == 0:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    buffer_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(sentence[b].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "                    buffer_pos_idxes.extend(\n",
    "                        pos_vectors[pos_vocab.get(sentence[b].pos, pos_vocab[\"<NULL>\"])]\n",
    "                    )\n",
    "            data_vector = torch.tensor(\n",
    "                stack_idxes + buffer_idxes + stack_pos_idxes + buffer_pos_idxes\n",
    "            ).to(device)\n",
    "            actions = torch.argsort(self.forward(data_vector))\n",
    "            action_idx = 0\n",
    "            while action_idx < len(actions):\n",
    "                action = action_vocab[actions[action_idx].item()]\n",
    "                if action[0] == \"shift\":\n",
    "                    if len(state.buffer) == 0:\n",
    "                        action_idx += 1\n",
    "                        continue\n",
    "                    state.shift()\n",
    "                    break\n",
    "                elif action[0] == \"left_arc\":\n",
    "                    if len(state.stack) < 2 or state.stack[-2] == 0:\n",
    "                        action_idx += 1\n",
    "                        continue\n",
    "                    state.left_arc(action[1])\n",
    "                    break\n",
    "                elif action[0] == \"right_arc\":\n",
    "                    if len(state.stack) < 2 or (state.stack[-2] == 0 and len(state.buffer) > 0):\n",
    "                        action_idx += 1\n",
    "                        continue\n",
    "                    state.right_arc(action[1])\n",
    "                    break\n",
    "                action_idx += 1\n",
    "        return state.deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39832 trees read.\n",
      "In dev.conll:\n",
      "1700 trees read.\n",
      "In test.conll:\n",
      "2416 trees read.\n",
      "Found 39 unique dependency relations in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"In train.conll:\")\n",
    "with open(\"data/train.conll\") as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f\"{len(train_trees)} trees read.\")\n",
    "\n",
    "print(\"In dev.conll:\")\n",
    "with open(\"data/dev.conll\") as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f\"{len(dev_trees)} trees read.\")\n",
    "\n",
    "print(\"In test.conll:\")\n",
    "with open(\"data/test.conll\") as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f\"{len(test_trees)} trees read.\")\n",
    "\n",
    "rel_counter = Counter()\n",
    "for tree in train_trees:\n",
    "    for item in tree.deprels.values():\n",
    "        rel_counter[item.deprel] += 1\n",
    "\n",
    "print(f\"Found {len(rel_counter)} unique dependency relations in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocab size: 400002\n",
      "POS vocab size: 48\n",
      "Action vocab size: 78\n",
      "[[-0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581], [0.013441, 0.23682, -0.16899, 0.40951, 0.63812, 0.47709, -0.42852, -0.55641, -0.364, -0.23938, 0.13001, -0.063734, -0.39575, -0.48162, 0.23291, 0.090201, -0.13324, 0.078639, -0.41634, -0.15428, 0.10068, 0.48891, 0.31226, -0.1252, -0.037512, -1.5179, 0.12612, -0.02442, -0.042961, -0.28351, 3.5416, -0.11956, -0.014533, -0.1499, 0.21864, -0.33412, -0.13872, 0.31806, 0.70358, 0.44858, -0.080262, 0.63003, 0.32111, -0.46765, 0.22786, 0.36034, -0.37818, -0.56657, 0.044691, 0.30392], [0.15164, 0.30177, -0.16763, 0.17684, 0.31719, 0.33973, -0.43478, -0.31086, -0.44999, -0.29486, 0.16608, 0.11963, -0.41328, -0.42353, 0.59868, 0.28825, -0.11547, -0.041848, -0.67989, -0.25063, 0.18472, 0.086876, 0.46582, 0.015035, 0.043474, -1.4671, -0.30384, -0.023441, 0.30589, -0.21785, 3.746, 0.0042284, -0.18436, -0.46209, 0.098329, -0.11907, 0.23919, 0.1161, 0.41705, 0.056763, -6.3681e-05, 0.068987, 0.087939, -0.10285, -0.13931, 0.22314, -0.080803, -0.35652, 0.016413, 0.10216], [0.70853, 0.57088, -0.4716, 0.18048, 0.54449, 0.72603, 0.18157, -0.52393, 0.10381, -0.17566, 0.078852, -0.36216, -0.11829, -0.83336, 0.11917, -0.16605, 0.061555, -0.012719, -0.56623, 0.013616, 0.22851, -0.14396, -0.067549, -0.38157, -0.23698, -1.7037, -0.86692, -0.26704, -0.2589, 0.1767, 3.8676, -0.1613, -0.13273, -0.68881, 0.18444, 0.0052464, -0.33874, -0.078956, 0.24185, 0.36576, -0.34727, 0.28483, 0.075693, -0.062178, -0.38988, 0.22902, -0.21617, -0.22562, -0.093918, -0.80375], [0.68047, -0.039263, 0.30186, -0.17792, 0.42962, 0.032246, -0.41376, 0.13228, -0.29847, -0.085253, 0.17118, 0.22419, -0.10046, -0.43653, 0.33418, 0.67846, 0.057204, -0.34448, -0.42785, -0.43275, 0.55963, 0.10032, 0.18677, -0.26854, 0.037334, -2.0932, 0.22171, -0.39868, 0.20912, -0.55725, 3.8826, 0.47466, -0.95658, -0.37788, 0.20869, -0.32752, 0.12751, 0.088359, 0.16351, -0.21634, -0.094375, 0.018324, 0.21048, -0.03088, -0.19722, 0.082279, -0.09434, -0.073297, -0.064699, -0.26044], [0.26818, 0.14346, -0.27877, 0.016257, 0.11384, 0.69923, -0.51332, -0.47368, -0.33075, -0.13834, 0.2702, 0.30938, -0.45012, -0.4127, -0.09932, 0.038085, 0.029749, 0.10076, -0.25058, -0.51818, 0.34558, 0.44922, 0.48791, -0.080866, -0.10121, -1.3777, -0.10866, -0.23201, 0.012839, -0.46508, 3.8463, 0.31362, 0.13643, -0.52244, 0.3302, 0.33707, -0.35601, 0.32431, 0.12041, 0.3512, -0.069043, 0.36885, 0.25168, -0.24517, 0.25381, 0.1367, -0.31178, -0.6321, -0.25028, -0.38097], [0.33042, 0.24995, -0.60874, 0.10923, 0.036372, 0.151, -0.55083, -0.074239, -0.092307, -0.32821, 0.09598, -0.82269, -0.36717, -0.67009, 0.42909, 0.016496, -0.23573, 0.12864, -1.0953, 0.43334, 0.57067, -0.1036, 0.20422, 0.078308, -0.42795, -1.7984, -0.27865, 0.11954, -0.12689, 0.031744, 3.8631, -0.17786, -0.082434, -0.62698, 0.26497, -0.057185, -0.073521, 0.46103, 0.30862, 0.12498, -0.48609, -0.0080272, 0.031184, -0.36576, -0.42699, 0.42164, -0.11666, -0.50703, -0.027273, -0.53285], [0.21705, 0.46515, -0.46757, 0.10082, 1.0135, 0.74845, -0.53104, -0.26256, 0.16812, 0.13182, -0.24909, -0.44185, -0.21739, 0.51004, 0.13448, -0.43141, -0.03123, 0.20674, -0.78138, -0.20148, -0.097401, 0.16088, -0.61836, -0.18504, -0.12461, -2.2526, -0.22321, 0.5043, 0.32257, 0.15313, 3.9636, -0.71365, -0.67012, 0.28388, 0.21738, 0.14433, 0.25926, 0.23434, 0.4274, -0.44451, 0.13813, 0.36973, -0.64289, 0.024142, -0.039315, -0.26037, 0.12017, -0.043782, 0.41013, 0.1796]]\n"
     ]
    }
   ],
   "source": [
    "# load word embeddings\n",
    "emb_path = \"./data/glove.6B.50d.txt\"\n",
    "word_vocab = {\"<NULL>\": -1, \"<ROOT>\": 0}\n",
    "word_vectors = [[-0.01] * 50, [0] * 50]\n",
    "with open(emb_path) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = list(map(float, parts[1:]))\n",
    "        word_vocab[word] = len(word_vocab)\n",
    "        word_vectors.append(vector)\n",
    "\n",
    "# word_vocab = {\"<NULL>\": -1, \"<ROOT>\": 0}\n",
    "# random pos embeddings within (−0.01, 0.01)\n",
    "pos_vocab = {\"<NULL>\": -1, \"<NONE>\": 0}\n",
    "pos_vectors = [[-0.01] * 50, [0] * 50]\n",
    "\n",
    "for tree in train_trees:\n",
    "    for pos in tree.pos():\n",
    "        if pos not in pos_vocab:\n",
    "            pos_vocab[pos] = len(pos_vocab)\n",
    "            rand_vec = [random.uniform(-0.01, 0.01) for _ in range(50)]\n",
    "            while rand_vec in pos_vectors:\n",
    "                rand_vec = [random.uniform(-0.01, 0.01) for _ in range(50)]\n",
    "            pos_vectors.append(rand_vec)\n",
    "\n",
    "# build action vocab\n",
    "action_vocab = {}\n",
    "action_rev_vocab = {}\n",
    "action_vocab[(\"right_arc\", \"root\")] = 0\n",
    "action_vocab[(\"shift\", None)] = 1\n",
    "for rel in rel_counter.keys():\n",
    "    if rel == \"root\":\n",
    "        continue\n",
    "    action_vocab[(\"left_arc\", rel)] = len(action_vocab)\n",
    "    action_vocab[(\"right_arc\", rel)] = len(action_vocab)\n",
    "\n",
    "for k, v in action_vocab.items():\n",
    "    action_rev_vocab[v] = k\n",
    "\n",
    "print(f\"Word vocab size: {len(word_vocab)}\")\n",
    "print(f\"POS vocab size: {len(pos_vocab)}\")\n",
    "print(f\"Action vocab size: {len(action_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m train_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m train_truth \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m train_data, train_truth \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_trees\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43maction_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 99\u001B[0m, in \u001B[0;36mprocess\u001B[0;34m(dep_trees, word_vocab, word_vectors, pos_vocab, pos_vectors, action_vocab)\u001B[0m\n\u001B[1;32m     97\u001B[0m tensor_truth \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tree \u001B[38;5;129;01min\u001B[39;00m dep_trees:\n\u001B[0;32m---> 99\u001B[0m     instances \u001B[38;5;241m=\u001B[39m \u001B[43mget_training_instances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtree\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    100\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m state, action \u001B[38;5;129;01min\u001B[39;00m instances:\n\u001B[1;32m    101\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m action[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "Cell \u001B[0;32mIn[11], line 82\u001B[0m, in \u001B[0;36mget_training_instances\u001B[0;34m(dep_tree)\u001B[0m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Shift\u001B[39;00m\n\u001B[1;32m     81\u001B[0m         seq\u001B[38;5;241m.\u001B[39mappend((copy\u001B[38;5;241m.\u001B[39mdeepcopy(state), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshift\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)))\n\u001B[0;32m---> 82\u001B[0m         \u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshift\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;66;03m# print(seq[-1])\u001B[39;00m\n\u001B[1;32m     86\u001B[0m seq\u001B[38;5;241m.\u001B[39mappend((copy\u001B[38;5;241m.\u001B[39mdeepcopy(state), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)))\n",
      "Cell \u001B[0;32mIn[11], line 22\u001B[0m, in \u001B[0;36mState.shift\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshift\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m### START YOUR CODE ###\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_truth = []\n",
    "train_data, train_truth = process(\n",
    "    train_trees,\n",
    "    word_vocab,\n",
    "    word_vectors,\n",
    "    pos_vocab,\n",
    "    pos_vectors,\n",
    "    action_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_truth = []\n",
    "test_data, test_truth = process(\n",
    "    test_trees,\n",
    "    word_vocab,\n",
    "    word_vectors,\n",
    "    pos_vocab,\n",
    "    pos_vectors,\n",
    "    action_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model = Parser(300, 128, 78)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 1\n",
    "display_step = 1000\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "train_data = train_data.to(device)\n",
    "train_truth = train_truth.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(range(0, len(train_data)))\n",
    "    for i in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.Tensor(train_data[i]))\n",
    "        loss = loss_fn(output, train_truth[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % display_step == 0:\n",
    "            pbar.set_description(f\"loss: {total_loss / display_step}\")\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "# example = model.parse_sentence(test_trees[0].deprels, word_vocab, word_vectors, pos_vocab, pos_vectors, action_rev_vocab, device)\n",
    "# print(example)\n",
    "# example_truth = set([(x.head, x.id, x.deprel) for x in test_trees[0].deprels.values()])\n",
    "# print(example_truth)\n",
    "\n",
    "las_correct = 0\n",
    "uas_correct = 0\n",
    "total = 0\n",
    "\n",
    "for tree in test_trees[:10]:\n",
    "    example = model.parse_sentence(tree.deprels, word_vocab, word_vectors, pos_vocab, pos_vectors, action_rev_vocab, device)\n",
    "    example_truth = set([(x.head, x.id, x.deprel) for x in tree.deprels.values()])\n",
    "    for edge in example:\n",
    "        if edge in example_truth:\n",
    "            las_correct += 1\n",
    "        if (edge[0], edge[1]) in [(x.head, x.id) for x in tree.deprels.values()]:\n",
    "            uas_correct += 1\n",
    "        total += 1\n",
    "\n",
    "print(f\"LAS: {las_correct / total}\")\n",
    "print(f\"UAS: {uas_correct / total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
