{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 4. Dependency Parsing\n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "In this assignment, you will train feed-forward neural network-based dependency parser and evaluate its performance on the provided treebank dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read Data and Generate Training Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the code from Lab 7\n",
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "\n",
    "class State(object):\n",
    "    def __init__(self, sentence=[]):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        self.stack.append(self.buffer.pop())\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        self.deps.add((self.stack[-1], self.stack[-2], label))\n",
    "        self.stack.pop(-2)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        self.deps.add((self.stack[-2], self.stack[-1], label))\n",
    "        self.stack.pop(-1)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)\n",
    "\n",
    "\n",
    "def get_training_instances(dep_tree) -> List[Tuple[State, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0)  # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "\n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "\n",
    "        # Decide transition action\n",
    "        if stack_top2.head == stack_top1.id:  # Left-Arc\n",
    "            seq.append((copy.deepcopy(state), (\"left_arc\", stack_top2.deprel)))\n",
    "            state.left_arc(stack_top2.deprel)\n",
    "            childcount[stack_top1.id] -= 1\n",
    "        elif stack_top1.head == stack_top2.id:  # Right-Arc\n",
    "            if childcount[stack_top1.id] != 0:\n",
    "                seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "                state.shift()\n",
    "            else:\n",
    "                seq.append((copy.deepcopy(state), (\"right_arc\", stack_top1.deprel)))\n",
    "                state.right_arc(stack_top1.deprel)\n",
    "                childcount[stack_top2.id] -= 1\n",
    "        else:  # Shift\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "\n",
    "        # print(seq[-1])\n",
    "\n",
    "    seq.append((copy.deepcopy(state), (\"done\", None)))\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def process(\n",
    "    dep_trees: List[DependencyTree],\n",
    "    word_vocab: dict, word_vectors: dict, pos_vocab: dict, pos_vectors: dict, action_vocab: dict) -> torch.Tensor:\n",
    "    tensor_data = []\n",
    "    tensor_truth = []\n",
    "    for tree in dep_trees:\n",
    "        instances = get_training_instances(tree)\n",
    "        for state, action in instances:\n",
    "            if action[0] == \"done\":\n",
    "                continue\n",
    "            # convert to torch tensor and append to tensor_data\n",
    "            # use stack[-3:] and buffer[-3:] to get the top 3 elements of the stack and buffer\n",
    "            # if the stack or buffer has less than 3 elements, use the <NULL> to pad\n",
    "            stack = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "            buffer = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "            # use word_vocab, pos_vocab to convert the word, pos, action to index\n",
    "            stack_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                elif s == 0:\n",
    "                    stack_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                else:\n",
    "                    stack_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(tree.deprels[s].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            buffer_idxes = []\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<NULL>\"]])\n",
    "                elif b == 0:\n",
    "                    buffer_idxes.extend(word_vectors[word_vocab[\"<ROOT>\"]])\n",
    "                else:\n",
    "                    buffer_idxes.extend(\n",
    "                        word_vectors[\n",
    "                            word_vocab.get(tree.deprels[b].word, word_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            # get POS tags\n",
    "            stack_pos_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif s == 0:\n",
    "                    stack_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    stack_pos_idxes.extend(\n",
    "                        pos_vectors[\n",
    "                            pos_vocab.get(tree.deprels[s].pos, pos_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            buffer_pos_idxes = []\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NULL>\"]])\n",
    "                elif b == 0:\n",
    "                    buffer_pos_idxes.extend(pos_vectors[pos_vocab[\"<NONE>\"]])\n",
    "                else:\n",
    "                    buffer_pos_idxes.extend(\n",
    "                        pos_vectors[\n",
    "                            pos_vocab.get(tree.deprels[b].pos, pos_vocab[\"<NULL>\"])\n",
    "                        ]\n",
    "                    )\n",
    "            # concatenate all index to get word vectors\n",
    "            data_vector = torch.tensor(stack_idxes + buffer_idxes + stack_pos_idxes + buffer_pos_idxes)\n",
    "            tensor_data.append(data_vector)\n",
    "            tensor_truth.append(action_vocab[action])\n",
    "\n",
    "    return (torch.stack(tensor_data), torch.tensor(tensor_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser(nn.Module):\n",
    "    def __init__(self, vec_dim, hidden_dim, action_size, dropout=0.2):\n",
    "        super(Parser, self).__init__()\n",
    "        self.input_dim = vec_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = action_size\n",
    "        self.dropout = dropout\n",
    "        self.W_1w = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.W_1t = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.W_2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.W_2(self.ReLU(self.W_1w(x[:x.shape[0]//2]) + self.W_1t(x[x.shape[0]//2:]))))\n",
    "\n",
    "    def parse_sentence(self, sentence, word_vocab, pos_vocab):\n",
    "        state = State(sentence)\n",
    "        state.stack.append(0)\n",
    "        while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "            stack = state.stack[-3:] if len(state.stack) >= 3 else state.stack + [-1] * (3 - len(state.stack))\n",
    "            buffer = state.buffer[-3:] if len(state.buffer) >= 3 else state.buffer + [-1] * (3 - len(state.buffer))\n",
    "            stack_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_idxes.extend(word_vocab[\"<NULL>\"])\n",
    "                elif s == 0:\n",
    "                    stack_idxes.extend(word_vocab[\"<ROOT>\"])\n",
    "                else:\n",
    "                    stack_idxes.extend(\n",
    "                        word_vocab.get(sentence[s].word, word_vocab[\"<NULL>\"])\n",
    "                    )\n",
    "            buffer_idxes = []\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_idxes.extend(word_vocab[\"<NULL>\"])\n",
    "                elif b == 0:\n",
    "                    buffer_idxes.extend(word_vocab[\"<ROOT>\"])\n",
    "                else:\n",
    "                    buffer_idxes.extend(\n",
    "                        word_vocab.get(sentence[b].word, word_vocab[\"<NULL>\"])\n",
    "                    )\n",
    "            stack_pos_idxes = []\n",
    "            for s in stack:\n",
    "                if s == -1:\n",
    "                    stack_pos_idxes.extend(pos_vocab[\"<NULL>\"])\n",
    "                elif s == 0:\n",
    "                    stack_pos_idxes.extend(pos_vocab[\"<NONE>\"])\n",
    "                else:\n",
    "                    stack_pos_idxes.extend(\n",
    "                        pos_vocab.get(sentence[s].pos, pos_vocab[\"<NULL>\"])\n",
    "                    )\n",
    "            buffer_pos_idxes = []\n",
    "            for b in buffer:\n",
    "                if b == -1:\n",
    "                    buffer_pos_idxes.extend(pos_vocab[\"<NULL>\"])\n",
    "                elif b == 0:\n",
    "                    buffer_pos_idxes.extend(pos_vocab[\"<NONE>\"])\n",
    "                else:\n",
    "                    buffer_pos_idxes.extend(\n",
    "                        pos_vocab.get(sentence[b].pos, pos_vocab[\"<NULL>\"])\n",
    "                    )\n",
    "            data_vector = torch.tensor(\n",
    "                stack_idxes + buffer_idxes + stack_pos_idxes + buffer_pos_idxes\n",
    "            )\n",
    "            actions = torch.argsort(self.forward(data_vector))\n",
    "            action_idx = 0\n",
    "            while action_idx < len(actions):\n",
    "                if actions[action_idx] == 0:  # shift\n",
    "                    if len(state.buffer) == 0:\n",
    "                        action_idx += 1\n",
    "                    else:\n",
    "                        state.shift()\n",
    "                        break\n",
    "                if actions[action_idx] == 1:  # left\n",
    "                    if len(state.stack) < 2 or state.stack[-2] == 0:\n",
    "                        action_idx += 1\n",
    "                    else:\n",
    "                        dep_tag = sentence[state.stack[-2]].deprel\n",
    "                        state.left_arc(dep_tag)\n",
    "                        break\n",
    "                if actions[action_idx] == 2:  # right\n",
    "                    if len(state.stack) < 2:\n",
    "                        action_idx += 1\n",
    "                    else:\n",
    "                        dep_tag = sentence[state.stack[-1]].deprel\n",
    "                        state.right_arc(dep_tag)\n",
    "                        break\n",
    "        return state.deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39832 trees read.\n",
      "In dev.conll:\n",
      "1700 trees read.\n",
      "In test.conll:\n",
      "2416 trees read.\n",
      "Found 39 unique dependency relations in the training set.\n"
     ]
    }
   ],
   "source": [
    "print(\"In train.conll:\")\n",
    "with open(\"data/train.conll\") as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f\"{len(train_trees)} trees read.\")\n",
    "\n",
    "print(\"In dev.conll:\")\n",
    "with open(\"data/dev.conll\") as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f\"{len(dev_trees)} trees read.\")\n",
    "\n",
    "print(\"In test.conll:\")\n",
    "with open(\"data/test.conll\") as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f\"{len(test_trees)} trees read.\")\n",
    "\n",
    "rel_counter = Counter()\n",
    "for tree in train_trees:\n",
    "    for item in tree.deprels.values():\n",
    "        rel_counter[item.deprel] += 1\n",
    "\n",
    "print(f\"Found {len(rel_counter)} unique dependency relations in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocab size: 400002\n",
      "POS vocab size: 48\n",
      "Action vocab size: 78\n",
      "{('right_arc', 'root'): 0, ('shift', None): 1, ('left_arc', 'case'): 2, ('right_arc', 'case'): 3, ('left_arc', 'det'): 4, ('right_arc', 'det'): 5, ('left_arc', 'compound'): 6, ('right_arc', 'compound'): 7, ('left_arc', 'nummod'): 8, ('right_arc', 'nummod'): 9, ('left_arc', 'nmod'): 10, ('right_arc', 'nmod'): 11, ('left_arc', 'punct'): 12, ('right_arc', 'punct'): 13, ('left_arc', 'nmod:poss'): 14, ('right_arc', 'nmod:poss'): 15, ('left_arc', 'amod'): 16, ('right_arc', 'amod'): 17, ('left_arc', 'nsubj'): 18, ('right_arc', 'nsubj'): 19, ('left_arc', 'dep'): 20, ('right_arc', 'dep'): 21, ('left_arc', 'dobj'): 22, ('right_arc', 'dobj'): 23, ('left_arc', 'cc'): 24, ('right_arc', 'cc'): 25, ('left_arc', 'conj'): 26, ('right_arc', 'conj'): 27, ('left_arc', 'nsubjpass'): 28, ('right_arc', 'nsubjpass'): 29, ('left_arc', 'acl'): 30, ('right_arc', 'acl'): 31, ('left_arc', 'auxpass'): 32, ('right_arc', 'auxpass'): 33, ('left_arc', 'advmod'): 34, ('right_arc', 'advmod'): 35, ('left_arc', 'ccomp'): 36, ('right_arc', 'ccomp'): 37, ('left_arc', 'mark'): 38, ('right_arc', 'mark'): 39, ('left_arc', 'xcomp'): 40, ('right_arc', 'xcomp'): 41, ('left_arc', 'nmod:tmod'): 42, ('right_arc', 'nmod:tmod'): 43, ('left_arc', 'appos'): 44, ('right_arc', 'appos'): 45, ('left_arc', 'nmod:npmod'): 46, ('right_arc', 'nmod:npmod'): 47, ('left_arc', 'aux'): 48, ('right_arc', 'aux'): 49, ('left_arc', 'cop'): 50, ('right_arc', 'cop'): 51, ('left_arc', 'neg'): 52, ('right_arc', 'neg'): 53, ('left_arc', 'acl:relcl'): 54, ('right_arc', 'acl:relcl'): 55, ('left_arc', 'advcl'): 56, ('right_arc', 'advcl'): 57, ('left_arc', 'mwe'): 58, ('right_arc', 'mwe'): 59, ('left_arc', 'det:predet'): 60, ('right_arc', 'det:predet'): 61, ('left_arc', 'csubj'): 62, ('right_arc', 'csubj'): 63, ('left_arc', 'parataxis'): 64, ('right_arc', 'parataxis'): 65, ('left_arc', 'compound:prt'): 66, ('right_arc', 'compound:prt'): 67, ('left_arc', 'iobj'): 68, ('right_arc', 'iobj'): 69, ('left_arc', 'expl'): 70, ('right_arc', 'expl'): 71, ('left_arc', 'cc:preconj'): 72, ('right_arc', 'cc:preconj'): 73, ('left_arc', 'discourse'): 74, ('right_arc', 'discourse'): 75, ('left_arc', 'csubjpass'): 76, ('right_arc', 'csubjpass'): 77}\n"
     ]
    }
   ],
   "source": [
    "# load word embeddings\n",
    "emb_path = \"./data/glove.6B.50d.txt\"\n",
    "word_vocab = {\"<NULL>\": -1, \"<ROOT>\": 0}\n",
    "word_vectors = [[-0.01] * 50, [0] * 50]\n",
    "with open(emb_path) as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        word = parts[0]\n",
    "        vector = list(map(float, parts[1:]))\n",
    "        word_vocab[word] = len(word_vocab)\n",
    "        word_vectors.append(vector)\n",
    "\n",
    "# word_vocab = {\"<NULL>\": -1, \"<ROOT>\": 0}\n",
    "# random pos embeddings within (âˆ’0.01, 0.01)\n",
    "pos_vocab = {\"<NULL>\": -1, \"<NONE>\": 0}\n",
    "pos_vectors = [[-0.01] * 50, [0] * 50]\n",
    "\n",
    "for tree in train_trees:\n",
    "    # for word in tree.words():\n",
    "    #     if word not in word_vocab:\n",
    "    #         word_vocab[word] = len(word_vocab)\n",
    "    for pos in tree.pos():\n",
    "        if pos not in pos_vocab:\n",
    "            pos_vocab[pos] = len(pos_vocab)\n",
    "            pos_vectors.append([random.uniform(-0.01, 0.01) for _ in range(50)])\n",
    "\n",
    "# build action vocab\n",
    "action_vocab = {}\n",
    "action_vocab[(\"right_arc\", \"root\")] = 0\n",
    "action_vocab[(\"shift\", None)] = 1\n",
    "for rel in rel_counter.keys():\n",
    "    if rel == \"root\":\n",
    "        continue\n",
    "    action_vocab[(\"left_arc\", rel)] = len(action_vocab)\n",
    "    action_vocab[(\"right_arc\", rel)] = len(action_vocab)\n",
    "\n",
    "print(f\"Word vocab size: {len(word_vocab)}\")\n",
    "print(f\"POS vocab size: {len(pos_vocab)}\")\n",
    "print(f\"Action vocab size: {len(action_vocab)}\")\n",
    "print(action_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m train_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m train_truth \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m train_data, train_truth \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_trees\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mword_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpos_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43maction_vocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 97\u001B[0m, in \u001B[0;36mprocess\u001B[0;34m(dep_trees, word_vocab, word_vectors, pos_vocab, pos_vectors, action_vocab)\u001B[0m\n\u001B[1;32m     95\u001B[0m tensor_truth \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m tree \u001B[38;5;129;01min\u001B[39;00m dep_trees:\n\u001B[0;32m---> 97\u001B[0m     instances \u001B[38;5;241m=\u001B[39m \u001B[43mget_training_instances\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtree\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m state, action \u001B[38;5;129;01min\u001B[39;00m instances:\n\u001B[1;32m     99\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m action[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "Cell \u001B[0;32mIn[2], line 82\u001B[0m, in \u001B[0;36mget_training_instances\u001B[0;34m(dep_tree)\u001B[0m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# Shift\u001B[39;00m\n\u001B[1;32m     81\u001B[0m         seq\u001B[38;5;241m.\u001B[39mappend((copy\u001B[38;5;241m.\u001B[39mdeepcopy(state), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshift\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)))\n\u001B[0;32m---> 82\u001B[0m         \u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshift\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m     \u001B[38;5;66;03m# print(seq[-1])\u001B[39;00m\n\u001B[1;32m     86\u001B[0m seq\u001B[38;5;241m.\u001B[39mappend((copy\u001B[38;5;241m.\u001B[39mdeepcopy(state), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdone\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)))\n",
      "Cell \u001B[0;32mIn[2], line 22\u001B[0m, in \u001B[0;36mState.shift\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshift\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m### START YOUR CODE ###\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_truth = []\n",
    "train_data, train_truth = process(\n",
    "    train_trees,\n",
    "    word_vocab,\n",
    "    word_vectors,\n",
    "    pos_vocab,\n",
    "    pos_vectors,\n",
    "    action_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_truth = []\n",
    "test_data, test_truth = process(\n",
    "    test_trees,\n",
    "    word_vocab,\n",
    "    word_vectors,\n",
    "    pos_vocab,\n",
    "    pos_vectors,\n",
    "    action_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model = Parser(300, 128, 78)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 1\n",
    "display_step = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(range(0, len(train_data)))\n",
    "    for i in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(torch.Tensor(train_data[i]))\n",
    "        # print(torch.argmax(output))\n",
    "        loss = loss_fn(output, train_truth[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % display_step == 0:\n",
    "            pbar.set_description(f\"Epoch {epoch + 1} loss: {total_loss / display_step}\")\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(test_data[1000])\n",
    "print(\n",
    "    model.W_2(\n",
    "        model.ReLU(model.W_1w(x[: x.shape[0] // 2]) + model.W_1t(x[x.shape[0] // 2 :]))\n",
    "    )\n",
    ")\n",
    "print(model.forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# evaluate\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m example \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_sentence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_trees\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeprels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_vocab\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_vocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(example)\n\u001B[1;32m      4\u001B[0m example_truth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m([(x\u001B[38;5;241m.\u001B[39mhead, x\u001B[38;5;241m.\u001B[39mid, x\u001B[38;5;241m.\u001B[39mdeprel) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m test_trees[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdeprels\u001B[38;5;241m.\u001B[39mvalues()])\n",
      "Cell \u001B[0;32mIn[8], line 28\u001B[0m, in \u001B[0;36mParser.parse_sentence\u001B[0;34m(self, sentence, word_vocab, pos_vocab)\u001B[0m\n\u001B[1;32m     26\u001B[0m     stack_idxes\u001B[38;5;241m.\u001B[39mextend(word_vocab[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<NULL>\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m s \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 28\u001B[0m     \u001B[43mstack_idxes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_vocab\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<ROOT>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     30\u001B[0m     stack_idxes\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m     31\u001B[0m         word_vocab\u001B[38;5;241m.\u001B[39mget(sentence[s]\u001B[38;5;241m.\u001B[39mword, word_vocab[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<NULL>\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     32\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "example = model.parse_sentence(test_trees[0].deprels, word_vocab, pos_vocab)\n",
    "print(example)\n",
    "example_truth = set([(x.head, x.id, x.deprel) for x in test_trees[0].deprels.values()])\n",
    "print(example_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parse_sentence() missing 2 required positional arguments: 'word_vocab' and 'pos_vocab'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m Parser(\u001B[38;5;241m300\u001B[39m, \u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m78\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_sentence\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mapple\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrees\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrow\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: parse_sentence() missing 2 required positional arguments: 'word_vocab' and 'pos_vocab'"
     ]
    }
   ],
   "source": [
    "model = Parser(300, 128, 78)\n",
    "model.parse_sentence([\"apple\",\"trees\",\"grow\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
