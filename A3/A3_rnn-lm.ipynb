{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 2). Named Entity Recognition with Bi-LSTM\n",
    "\n",
    "**Total points**: 30 + 20 bonus points\n",
    "\n",
    "In this assignment, you will train a bidirectional LSTM model on the CoNLL2003 English named entity recognition task set and evaluate its performance.\n",
    "\n",
    "For the bonus questions, submit them as separate notebook files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def read_ner_data(path_to_file):\n",
    "    words = []\n",
    "    tags = []\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            splitted = line.split()\n",
    "            if len(splitted) == 0:\n",
    "                continue\n",
    "            word = splitted[0]\n",
    "            if word == '-DOCSTART-':\n",
    "                continue\n",
    "            entity = splitted[-1]\n",
    "            words.append(word)\n",
    "            tags.append(entity)\n",
    "        return words, tags\n",
    "\n",
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "\n",
    "\n",
    "train_words, train_tags = read_ner_data(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data(TEST_PATH)\n",
    "\n",
    "\n",
    "# Convert all words to lowercase\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]\n",
    "\n",
    "# Build vocabularies for words and labels\n",
    "word_vocab = set(train_words + dev_words + test_words)\n",
    "label_vocab = set(train_tags + dev_tags + test_tags)\n",
    "\n",
    "print('Word vocabulary size:', len(word_vocab))\n",
    "print('Tag vocabulary size:', len(label_vocab))\n",
    "\n",
    "# Define mappings from words and labels to indices\n",
    "word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "label2idx = {label: idx for idx, label in enumerate(label_vocab)}\n",
    "\n",
    "# Define a data loader that returns batches\n",
    "def collate_fn(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "    max_length = max(sentence_lengths)\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence = [word2idx[word] for word in sentence]\n",
    "        padded_sentence += [0] * (max_length - len(sentence))\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    return torch.LongTensor(padded_sentences), torch.LongTensor(labels), torch.LongTensor(sentence_lengths)\n",
    "\n",
    "# Create data loaders\n",
    "train_data = list(zip(train_words, train_tags))\n",
    "dev_data = list(zip(dev_words, dev_tags))\n",
    "test_data = list(zip(test_words, test_tags))\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "dev_dataset = CustomDataset(dev_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "batch_size = 32  # Set your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_dim = 100  # Set the desired embedding dimension\n",
    "embedding_file = 'glove.6B/glove.6B.100d.txt'  # Path to the pretrained embedding file\n",
    "\n",
    "# Load the pretrained embeddings\n",
    "pretrained_embeddings = {}\n",
    "with open(embedding_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor([float(val) for val in values[1:]])\n",
    "        pretrained_embeddings[word] = vector\n",
    "\n",
    "# Initialize the embedding layer with pretrained embeddings\n",
    "num_embeddings = len(word_vocab)\n",
    "embedding_matrix = torch.zeros(num_embeddings, embedding_dim)\n",
    "for word, idx in word2idx.items():\n",
    "    if word in pretrained_embeddings:\n",
    "        embedding_matrix[idx] = pretrained_embeddings[word]\n",
    "\n",
    "# Now you can use the embedding_matrix to initialize your embedding layer in the model\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_layers, num_classes):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        lstm_output, _ = self.lstm(embedded)\n",
    "        # Use the last hidden states from both directions\n",
    "        last_hidden = torch.cat((lstm_output[:, -1, :hidden_dim], lstm_output[:, 0, hidden_dim:]), dim=1)\n",
    "        logits = self.fc(last_hidden)\n",
    "        return logits\n",
    "\n",
    "# Set hyperparameters\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "num_classes = len(label_vocab)\n",
    "\n",
    "# Instantiate the model\n",
    "model = BiLSTMClassifier(embedding_dim, hidden_dim, num_layers, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels, _ \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     14\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     15\u001B[0m         logits \u001B[38;5;241m=\u001B[39m model(inputs)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[56], line 61\u001B[0m, in \u001B[0;36mcollate_fn\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m     59\u001B[0m     padded_sentence \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m (max_length \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(sentence))\n\u001B[1;32m     60\u001B[0m     padded_sentences\u001B[38;5;241m.\u001B[39mappend(padded_sentence)\n\u001B[0;32m---> 61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mLongTensor(padded_sentences), \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLongTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m, torch\u001B[38;5;241m.\u001B[39mLongTensor(sentence_lengths)\n",
      "\u001B[0;31mValueError\u001B[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set training parameters\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.001  # L2 regularization parameter\n",
    "num_epochs = 10\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, _ in test_loader:\n",
    "        logits = model(inputs)\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "# Print F1 score on test set\n",
    "print(\"F1 Score on Test Set: {:.4f}\".format(f1))\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
