{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 2). Named Entity Recognition with Bi-LSTM\n",
    "\n",
    "**Total points**: 30 + 20 bonus points\n",
    "\n",
    "In this assignment, you will train a bidirectional LSTM model on the CoNLL2003 English named entity recognition task set and evaluate its performance.\n",
    "\n",
    "For the bonus questions, submit them as separate notebook files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext.vocab as vocab\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def read_ner_data(path_to_file):\n",
    "    words = []\n",
    "    tags = []\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            splitted = line.split()\n",
    "            if len(splitted) == 0:\n",
    "                continue\n",
    "            word = splitted[0]\n",
    "            if word == '-DOCSTART-':\n",
    "                continue\n",
    "            entity = splitted[-1]\n",
    "            words.append(word)\n",
    "            tags.append(entity)\n",
    "        return words, tags\n",
    "\n",
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "\n",
    "\n",
    "train_words, train_tags = read_ner_data(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data(TEST_PATH)\n",
    "\n",
    "\n",
    "# Convert all words to lowercase\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]\n",
    "\n",
    "# Build vocabularies for words and labels\n",
    "word_vocab = set(train_words + dev_words + test_words)\n",
    "label_vocab = set(train_tags + dev_tags + test_tags)\n",
    "\n",
    "# Define mappings from words and labels to indices\n",
    "word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "label2idx = {label: idx for idx, label in enumerate(label_vocab)}\n",
    "\n",
    "# Define a data loader that returns batches\n",
    "def collate_fn(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    sentence_lengths = [len(sentence) for sentence in sentences]\n",
    "    max_length = max(sentence_lengths)\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence = [word2idx[word] for word in sentence]\n",
    "        padded_sentence += [0] * (max_length - len(sentence))\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    return torch.LongTensor(padded_sentences), torch.LongTensor(labels), torch.LongTensor(sentence_lengths)\n",
    "\n",
    "# Create data loaders\n",
    "train_data = list(zip(train_words, train_tags))\n",
    "dev_data = list(zip(dev_words, dev_tags))\n",
    "test_data = list(zip(test_words, test_tags))\n",
    "\n",
    "train_dataset = CustomDataset(train_data)\n",
    "dev_dataset = CustomDataset(dev_data)\n",
    "test_dataset = CustomDataset(test_data)\n",
    "\n",
    "batch_size = 32  # Set your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embedding_dim = 100  # Set the desired embedding dimension\n",
    "embedding_file = 'glove.6B/glove.6B.100d.txt'  # Path to the pretrained embedding file\n",
    "\n",
    "# Load the pretrained embeddings\n",
    "pretrained_embeddings = {}\n",
    "with open(embedding_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor([float(val) for val in values[1:]])\n",
    "        pretrained_embeddings[word] = vector\n",
    "\n",
    "# Initialize the embedding layer with pretrained embeddings\n",
    "num_embeddings = len(word_vocab)\n",
    "embedding_matrix = torch.zeros(num_embeddings, embedding_dim)\n",
    "for word, idx in word2idx.items():\n",
    "    if word in pretrained_embeddings:\n",
    "        embedding_matrix[idx] = pretrained_embeddings[word]\n",
    "\n",
    "# Now you can use the embedding_matrix to initialize your embedding layer in the model\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
