{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 2). Named Entity Recognition with Bi-LSTM\n",
    "\n",
    "**Total points**: 30 + 20 bonus points\n",
    "\n",
    "In this assignment, you will train a bidirectional LSTM model on the CoNLL2003 English named entity recognition task set and evaluate its performance.\n",
    "\n",
    "For the bonus questions, submit them as separate notebook files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocabulary size: 26869\n",
      "Tag vocabulary size: 9\n"
     ]
    }
   ],
   "source": [
    "# load train, dev, test data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, words, tags, word2idx, tag2idx):\n",
    "        self.words = words\n",
    "        self.tags = tags\n",
    "        self.word2idx = word2idx\n",
    "        self.tag2idx = tag2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word = self.words[idx]\n",
    "        tag = self.tags[idx]\n",
    "        return self.word2idx[word], self.tag2idx[tag]\n",
    "\n",
    "\n",
    "def read_ner_data(path_to_file):\n",
    "    words = []\n",
    "    tags = []\n",
    "    with open(path_to_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            splitted = line.split()\n",
    "            if len(splitted) == 0:\n",
    "                continue\n",
    "            word = splitted[0]\n",
    "            if word == '-DOCSTART-':\n",
    "                continue\n",
    "            entity = splitted[-1]\n",
    "            words.append(word)\n",
    "            tags.append(entity)\n",
    "        return words, tags\n",
    "\n",
    "\n",
    "TRAIN_PATH = 'data/train.txt'\n",
    "DEV_PATH = 'data/dev.txt'\n",
    "TEST_PATH = 'data/test.txt'\n",
    "\n",
    "train_words, train_tags = read_ner_data(TRAIN_PATH)\n",
    "dev_words, dev_tags = read_ner_data(DEV_PATH)\n",
    "test_words, test_tags = read_ner_data(TEST_PATH)\n",
    "\n",
    "# Convert all words to lowercase\n",
    "train_words = [word.lower() for word in train_words]\n",
    "dev_words = [word.lower() for word in dev_words]\n",
    "test_words = [word.lower() for word in test_words]\n",
    "\n",
    "# Build vocabularies for words and labels\n",
    "word_vocab = set(train_words + dev_words + test_words)\n",
    "label_vocab = set(train_tags + dev_tags + test_tags)\n",
    "\n",
    "print('Word vocabulary size:', len(word_vocab))\n",
    "print('Tag vocabulary size:', len(label_vocab))\n",
    "\n",
    "# Define mappings from words and labels to indices\n",
    "word2idx = {word: idx for idx, word in enumerate(word_vocab)}\n",
    "label2idx = {label: idx for idx, label in enumerate(label_vocab)}\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train_words, train_tags, word2idx, label2idx)\n",
    "dev_dataset = CustomDataset(dev_words, dev_tags, word2idx, label2idx)\n",
    "test_dataset = CustomDataset(test_words, test_tags, word2idx, label2idx)\n",
    "\n",
    "batch_size = 128  # Set your desired batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读入\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained embedding data\n",
    "def read_embedding_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    print('已读入')\n",
    "    word2vec = {}\n",
    "    # 遍历每一行内容\n",
    "    for i, line in enumerate(lines):\n",
    "        # 利用空格分割每一行，获取单词和对应的embedding向量\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        embedding = np.array([float(x) for x in parts[1:]])\n",
    "        word2vec[word] = embedding\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "\n",
    "emb_size = 100  # Set the desired embedding dimension\n",
    "embedding_file = 'glove.6B/glove.6B.100d.txt'  # Path to the pretrained embedding file\n",
    "\n",
    "# Load the pretrained embeddings\n",
    "word2vec = read_embedding_file(embedding_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "\n",
    "# build model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_size, emb_size, hidden_dim, num_layers, pretrained_embeddings=None):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tag_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        tag_scores = torch.nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/3508861102.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).to(device)\n",
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/3508861102.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.5242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/3508861102.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).to(device)\n",
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/3508861102.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Validation Accuracy: 86.46%\n",
      "Epoch [2/10], Train Loss: 0.4343\n",
      "Epoch [2/10], Validation Accuracy: 86.22%\n",
      "Epoch [3/10], Train Loss: 0.4292\n",
      "Epoch [3/10], Validation Accuracy: 86.96%\n",
      "Epoch [4/10], Train Loss: 0.4273\n",
      "Epoch [4/10], Validation Accuracy: 86.77%\n",
      "Epoch [5/10], Train Loss: 0.4264\n",
      "Epoch [5/10], Validation Accuracy: 86.95%\n",
      "Epoch [6/10], Train Loss: 0.4257\n",
      "Epoch [6/10], Validation Accuracy: 86.86%\n",
      "Epoch [7/10], Train Loss: 0.4260\n",
      "Epoch [7/10], Validation Accuracy: 86.83%\n",
      "Epoch [8/10], Train Loss: 0.4259\n",
      "Epoch [8/10], Validation Accuracy: 86.95%\n",
      "Epoch [9/10], Train Loss: 0.4263\n",
      "Epoch [9/10], Validation Accuracy: 86.55%\n",
      "Epoch [10/10], Train Loss: 0.4268\n",
      "Epoch [10/10], Validation Accuracy: 86.66%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = torch.tensor(inputs).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = torch.tensor(inputs).to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    # print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "vocab_size = len(word_vocab)\n",
    "tag_size = len(label_vocab)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate the model\n",
    "model = BiLSTMClassifier(vocab_size, tag_size, emb_size, hidden_dim, num_layers)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on validation set\n",
    "    accuracy = evaluate(model, dev_loader, device)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Save the best model based on validation accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/655073072.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).to(device)\n",
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_98662/655073072.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 score: 0.8198\n"
     ]
    }
   ],
   "source": [
    "# Load the best model and evaluate on the test set\n",
    "best_model = BiLSTMClassifier(vocab_size, tag_size, emb_size, hidden_dim, num_layers)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "best_model.to(device)\n",
    "\n",
    "# accuracy = evaluate(best_model, test_loader, device)\n",
    "# print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = torch.tensor(inputs).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        outputs = best_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute the F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f\"Test F1 score: {f1:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
