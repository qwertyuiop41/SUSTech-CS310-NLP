{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 1). Recurrent Neural Networks for Language Modeling\n",
    "\n",
    "**Total points**: 30\n",
    "\n",
    "In this assignment, you will train a vanilla RNN language model on《论语》and evaluate its perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n",
      "logits: torch.Size([512, 393, 1353])\n",
      "log_probs: torch.Size([512, 393, 1353])\n",
      "loss: torch.Size([201216])\n"
     ]
    }
   ],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "word2id: dict = {}\n",
    "id2word: dict = {}\n",
    "\n",
    "word2id.update({'[PAD]': 0})\n",
    "word2id.update({k: v+1 for k, v in corpus.word2id.items()})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(line.strip())\n",
    "\n",
    "embedding_lunyu = nn.Embedding(len(word2id), 50)\n",
    "rnn_lunyu = nn.RNN(50, 100, batch_first=True)\n",
    "\n",
    "seq_ids = [torch.tensor([word2id.get(w, 0) for w in line], dtype=torch.long) for line in lines]\n",
    "seq_lens = torch.tensor([len(line) for line in seq_ids])\n",
    "seq_ids_padded = nn.utils.rnn.pad_sequence(seq_ids, batch_first=True)\n",
    "\n",
    "seq_embs = embedding_lunyu(seq_ids_padded)\n",
    "seq_embs_packed = nn.utils.rnn.pack_padded_sequence(seq_embs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "out_packed,_= rnn_lunyu(seq_embs_packed)\n",
    "out_unpacked,_= nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "\n",
    "targets_padded = seq_ids_padded.clone()\n",
    "for i in range(len(targets_padded)):\n",
    "    targets_padded[i, :-1] = targets_padded[i, 1:].clone()\n",
    "    targets_padded[i, -1] = word2id.get('[PAD]', 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # super(RNNLM, self).__init__()\n",
    "        # self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "        # self.rnn = nn.RNN(kwargs['emb_size'], kwargs['hidden_size'], batch_first=True)\n",
    "        # self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "\n",
    "        # 多层RNN\n",
    "        num_layers = 3\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = kwargs['emb_size'] if i == 0 else kwargs['hidden_size']\n",
    "            self.rnn_layers.append(nn.RNN(input_size, kwargs['hidden_size'], batch_first=True))\n",
    "\n",
    "        self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "\n",
    "\n",
    "def forward(self, seq, seq_lens):\n",
    "        embedded = self.embedding(seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # 多层RNN的前向传播\n",
    "        rnn_output = packed\n",
    "        for rnn_layer in self.rnn_layers:\n",
    "            rnn_output, _ = rnn_layer(rnn_output)\n",
    "\n",
    "        padded, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
    "        logits = self.fc(padded)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(100, len(word2id))\n",
    "logits = fc(out_unpacked)\n",
    "# log_probs = F.log_softmax(logits, dim=-1)\n",
    "#\n",
    "#\n",
    "# # Test result\n",
    "# print('logits:', logits.size())\n",
    "# print('log_probs:', log_probs.size())\n",
    "#\n",
    "# # Report Compute Perplexity\n",
    "# print('Report Compute Perplexity:')\n",
    "# loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "#\n",
    "#\n",
    "# # Calculate the loss\n",
    "# with torch.no_grad():\n",
    "#     loss = loss_fn(log_probs.view(-1, log_probs.size(-1)), targets_padded.view(-1))\n",
    "#\n",
    "# # Test result\n",
    "# print('loss:', loss.size())\n",
    "\n",
    "\n",
    "\n",
    "# 定义计算困惑度的函数\n",
    "def compute_perplexity(logits, targets):\n",
    "    with torch.no_grad():\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.nll_loss(log_probs.view(-1, log_probs.size(-1)), targets.view(-1), ignore_index=0, reduction='none')\n",
    "        # num_words = targets.ne(0).sum().item()  # 计算非填充词的数量\n",
    "        perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "# 计算训练集的困惑度\n",
    "perplexity = compute_perplexity(logits, targets_padded)\n",
    "print(f\"Perplexity on training set: {perplexity.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "# 词汇表大小\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "# 嵌入维度\n",
    "emb_size = embedding_lunyu.embedding_dim\n",
    "\n",
    "# 隐藏层维度\n",
    "hidden_size = rnn_lunyu.hidden_size\n",
    "\n",
    "# # 输入序列的填充后的张量\n",
    "# seq_ids_padded = seq_ids_padded\n",
    "#\n",
    "# # 输入序列的长度\n",
    "# seq_lens = seq_lens\n",
    "#\n",
    "# # 目标序列的填充后的张量\n",
    "# targets_padded = targets_padded\n",
    "\n",
    "model = RNNLM(vocab_size=vocab_size, emb_size=emb_size, hidden_size=hidden_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练过程\n",
    "model.train()\n",
    "num_epochs = 5  # 迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(seq_ids_padded, seq_lens)\n",
    "    loss = criterion(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# 评估过程\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     logits = model(seq_ids_padded, seq_lens)\n",
    "#     loss = criterion(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
    "#     perplexity = torch.exp(loss)\n",
    "#\n",
    "#     print(f\"Perplexity: {perplexity.item()}\")\n",
    "\n",
    "\n",
    "def generate_sentence(model, start_tokens, max_length=20):\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_tokens = start_tokens[:]  # 初始化当前的标记序列为起始标记序列\n",
    "        hidden = None  # 初始化隐藏状态为None，因为我们只生成一个句子\n",
    "        for _ in range(max_length):  # 迭代生成最大长度的句子\n",
    "            # 将当前标记序列转换为对应的ID序列，并转换为张量\n",
    "            input_tensor = torch.tensor([[word2id.get(token, 0) for token in current_tokens]], dtype=torch.long)\n",
    "            embeddings = model.embedding(input_tensor)  # 将ID序列转换为嵌入向量\n",
    "            output, hidden = model.rnn(embeddings, hidden)  # 将嵌入向量输入RNN模型得到输出和隐藏状态\n",
    "            logits = model.fc(output)  # 使用线性层将RNN输出转换为词汇表大小的logits\n",
    "            probabilities = F.softmax(logits, dim=-1)  # 使用softmax函数将logits转换为概率分布\n",
    "            current_word = torch.argmax(probabilities[:, -1, :], dim=-1).item()  # 选择概率最高的词作为当前词\n",
    "            current_token = id2word.get(current_word, \"[UNK]\")  # 将当前词的ID转换为对应的标记\n",
    "            current_tokens.append(current_token)  # 将当前标记追加到标记序列中\n",
    "            if current_token == \"。\":  # 如果生成了句号，停止生成\n",
    "                break\n",
    "        return current_tokens  # 返回生成的标记序列\n",
    "\n",
    "# 生成句子\n",
    "num_sentences = 5\n",
    "start_tokens = [\"子\", \"曰\"]\n",
    "for _ in range(num_sentences):\n",
    "    sentence = generate_sentence(model, start_tokens)\n",
    "    print(\"Generated Sentence:\", \"\".join(sentence))  # 将标记序列连接为字符串并打印出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(f'embeddings_{emb_size}_{k}_{window_size}.txt')\n",
    "\n",
    "\n",
    "\n",
    "vacob_size = len(corpus.id2word)\n",
    "# model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "# Get embeddings as numpy array\n",
    "embeddings = model.embedding.weight.data.numpy()\n",
    "\n",
    "# # Truncated SVD\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# embeddings_2d = svd.fit_transform(embeddings)\n",
    "\n",
    "# 加载预训练嵌入\n",
    "pretrained_embeddings = torch.load('pretrained_embeddings.pth')  # 根据预训练的嵌入文件的路径进行调整\n",
    "\n",
    "# 应用预训练嵌入到模型的嵌入层\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# 计算使用预训练嵌入的困惑度\n",
    "logits_with_pretrained = model(seq_ids_padded, seq_lens)\n",
    "perplexity_with_pretrained = compute_perplexity(logits_with_pretrained, targets_padded)\n",
    "print(f\"Perplexity with pretrained embeddings: {perplexity_with_pretrained.item()}\")\n",
    "\n",
    "# 重新随机初始化模型的嵌入层\n",
    "model.embedding.reset_parameters()\n",
    "\n",
    "# 计算使用随机初始化嵌入的困惑度\n",
    "logits_random = model(seq_ids_padded, seq_lens)\n",
    "perplexity_random = compute_perplexity(logits_random, targets_padded)\n",
    "print(f\"Perplexity with randomly initialized embeddings: {perplexity_random.item()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
