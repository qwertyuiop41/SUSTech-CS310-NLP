{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 1). Recurrent Neural Networks for Language Modeling\n",
    "\n",
    "**Total points**: 30\n",
    "\n",
    "In this assignment, you will train a vanilla RNN language model on《论语》and evaluate its perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n",
      "2\n",
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "word2id: dict = {}\n",
    "id2word: dict = {}\n",
    "\n",
    "word2id.update({'[PAD]': 0})\n",
    "word2id.update({k: v+1 for k, v in corpus.word2id.items()})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "print(word2id['子'])\n",
    "print(word2id['曰'])\n",
    "print(word2id['。'])\n",
    "\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(line.strip())\n",
    "\n",
    "embedding_lunyu = nn.Embedding(len(word2id), 50)\n",
    "rnn_lunyu = nn.RNN(50, 100, batch_first=True)\n",
    "\n",
    "seq_ids = [torch.tensor([word2id.get(w, 0) for w in line], dtype=torch.long) for line in lines]\n",
    "seq_lens = torch.tensor([len(line) for line in seq_ids])\n",
    "seq_ids_padded = nn.utils.rnn.pad_sequence(seq_ids, batch_first=True)\n",
    "\n",
    "seq_embs = embedding_lunyu(seq_ids_padded)\n",
    "seq_embs_packed = nn.utils.rnn.pack_padded_sequence(seq_embs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "out_packed,_= rnn_lunyu(seq_embs_packed)\n",
    "out_unpacked,_= nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "\n",
    "targets_padded = seq_ids_padded.clone()\n",
    "for i in range(len(targets_padded)):\n",
    "    targets_padded[i, :-1] = targets_padded[i, 1:].clone()\n",
    "    targets_padded[i, -1] = word2id.get('[PAD]', 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # super(RNNLM, self).__init__()\n",
    "        # self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "        # self.rnn = nn.RNN(kwargs['emb_size'], kwargs['hidden_size'], batch_first=True)\n",
    "        # self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "\n",
    "        # 多层RNN\n",
    "        num_layers = 3\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = kwargs['emb_size'] if i == 0 else kwargs['hidden_size']\n",
    "            self.rnn_layers.append(nn.RNN(input_size, kwargs['hidden_size'], batch_first=True))\n",
    "\n",
    "        self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "            embedded = self.embedding(seq)\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            # 多层RNN的前向传播\n",
    "            rnn_output = packed\n",
    "            for rnn_layer in self.rnn_layers:\n",
    "                rnn_output, _ = rnn_layer(rnn_output)\n",
    "\n",
    "            padded, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
    "            logits = self.fc(padded)\n",
    "            return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on training set: 2.00504469871521\n",
      "Epoch [1/5], Loss: 7.1803975105285645\n",
      "Epoch [2/5], Loss: 7.098686695098877\n",
      "Epoch [3/5], Loss: 7.016990661621094\n",
      "Epoch [4/5], Loss: 6.935307502746582\n",
      "Epoch [5/5], Loss: 6.853636264801025\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(100, len(word2id))\n",
    "logits = fc(out_unpacked)\n",
    "# log_probs = F.log_softmax(logits, dim=-1)\n",
    "#\n",
    "#\n",
    "# # Test result\n",
    "# print('logits:', logits.size())\n",
    "# print('log_probs:', log_probs.size())\n",
    "#\n",
    "# # Report Compute Perplexity\n",
    "# print('Report Compute Perplexity:')\n",
    "# loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "#\n",
    "#\n",
    "# # Calculate the loss\n",
    "# with torch.no_grad():\n",
    "#     loss = loss_fn(log_probs.view(-1, log_probs.size(-1)), targets_padded.view(-1))\n",
    "#\n",
    "# # Test result\n",
    "# print('loss:', loss.size())\n",
    "\n",
    "\n",
    "\n",
    "# 定义计算困惑度的函数\n",
    "def compute_perplexity(logits, targets):\n",
    "    with torch.no_grad():\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.nll_loss(log_probs.view(-1, log_probs.size(-1)), targets.view(-1), ignore_index=0, reduction='none')\n",
    "        # num_words = targets.ne(0).sum().item()  # 计算非填充词的数量\n",
    "        perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "perplexity = compute_perplexity(logits, targets_padded)\n",
    "print(f\"Perplexity on training set: {perplexity.item()}\")\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "emb_size = embedding_lunyu.embedding_dim\n",
    "\n",
    "hidden_size = rnn_lunyu.hidden_size\n",
    "\n",
    "# # 输入序列的填充后的张量\n",
    "# seq_ids_padded = seq_ids_padded\n",
    "# # 输入序列的长度\n",
    "# seq_lens = seq_lens\n",
    "# # 目标序列的填充后的张量\n",
    "# targets_padded = targets_padded\n",
    "\n",
    "model = RNNLM(vocab_size=vocab_size, emb_size=emb_size, hidden_size=hidden_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练过程\n",
    "model.train()\n",
    "num_epochs = 5  # 迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(seq_ids_padded, seq_lens)\n",
    "    loss = criterion(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0013, 0.0009, 0.0008,  ..., 0.0007, 0.0009, 0.0006]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[99], line 37\u001B[0m\n\u001B[1;32m     35\u001B[0m num_sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m  \u001B[38;5;66;03m# 要生成的句子数量\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_sentences):\n\u001B[0;32m---> 37\u001B[0m     sentence \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_sentence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_token\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     sentence_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(sentence)  \u001B[38;5;66;03m# 将生成的标记组合成句子\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated Sentence:\u001B[39m\u001B[38;5;124m\"\u001B[39m, sentence_text)\n",
      "Cell \u001B[0;32mIn[99], line 18\u001B[0m, in \u001B[0;36mgenerate_sentence\u001B[0;34m(model, start_tokens, end_token, max_length)\u001B[0m\n\u001B[1;32m     15\u001B[0m sorted_probs, sorted_indices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msort(probabilities[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], descending\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Check if the second highest probability is the end token\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m predicted_token \u001B[38;5;241m=\u001B[39m \u001B[43msorted_indices\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m id2word\u001B[38;5;241m.\u001B[39mget(predicted_token) \u001B[38;5;241m==\u001B[39m end_token:\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, start_tokens, end_token, max_length=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_tokens = [word2id.get(token, 0) for token in start_tokens]\n",
    "        generated_sentence = start_tokens.copy()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            current_input = torch.tensor([current_tokens], dtype=torch.long)  # 添加批次维度\n",
    "            seq_lens = torch.tensor([len(current_tokens)], dtype=torch.long)  # 添加句子长度\n",
    "            logits = model(current_input, seq_lens)  # 调用模型进行前向传播\n",
    "            probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            print(probabilities)\n",
    "            # _, predicted_token = torch.max(probabilities, dim=-1)\n",
    "            # predicted_token = predicted_token.item()\n",
    "            sorted_probs, sorted_indices = torch.sort(probabilities[:, -1], descending=True)\n",
    "\n",
    "            # Check if the second highest probability is the end token\n",
    "            predicted_token = sorted_indices[0, 1].item()\n",
    "\n",
    "            if id2word.get(predicted_token) == end_token:\n",
    "                break\n",
    "\n",
    "            predicted_word = id2word.get(predicted_token, \"\")\n",
    "            if id2word.get(predicted_token) != \"[PAD]\":\n",
    "                generated_sentence.append(predicted_word)\n",
    "                current_tokens.append(predicted_token)\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "start_tokens = [\"子\", \"曰\"]  # 开始标记组成的列表\n",
    "end_token = \"。\"  # 结束标记的值\n",
    "max_length = 20  # 生成句子的最大长度\n",
    "\n",
    "# 生成句子\n",
    "num_sentences = 5  # 要生成的句子数量\n",
    "for _ in range(num_sentences):\n",
    "    sentence = generate_sentence(model, start_tokens, end_token, max_length)\n",
    "    sentence_text = \"\".join(sentence)  # 将生成的标记组合成句子\n",
    "    print(\"Generated Sentence:\", sentence_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "skipGram_model = gensim.models.KeyedVectors.load_word2vec_format(f'100_5_15.txt')\n",
    "\n",
    "# Get embeddings as numpy array\n",
    "pretrained_embeddings = skipGram_model.emb_v.cpu().weight.data.numpy()\n",
    "\n",
    "# # Truncated SVD\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# embeddings_2d = svd.fit_transform(embeddings)\n",
    "\n",
    "# 应用预训练嵌入到模型的嵌入层\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# 计算使用预训练嵌入的困惑度\n",
    "logits_with_pretrained = model(seq_ids_padded, seq_lens)\n",
    "perplexity_with_pretrained = compute_perplexity(logits_with_pretrained, targets_padded)\n",
    "print(f\"Perplexity with pretrained embeddings: {perplexity_with_pretrained.item()}\")\n",
    "\n",
    "# 重新随机初始化模型的嵌入层\n",
    "model.embedding.reset_parameters()\n",
    "\n",
    "# 计算使用随机初始化嵌入的困惑度\n",
    "logits_random = model(seq_ids_padded, seq_lens)\n",
    "perplexity_random = compute_perplexity(logits_random, targets_padded)\n",
    "print(f\"Perplexity with randomly initialized embeddings: {perplexity_random.item()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
