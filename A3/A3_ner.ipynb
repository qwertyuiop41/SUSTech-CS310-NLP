{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 1). Recurrent Neural Networks for Language Modeling\n",
    "\n",
    "**Total points**: 30\n",
    "\n",
    "In this assignment, you will train a vanilla RNN language model on《论语》and evaluate its perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "word2id: dict = {}\n",
    "id2word: dict = {}\n",
    "\n",
    "word2id.update({'[PAD]': 0})\n",
    "word2id.update({k: v+1 for k, v in corpus.word2id.items()})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "\n",
    "lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(line.strip())\n",
    "\n",
    "embedding_lunyu = nn.Embedding(len(word2id), 50)\n",
    "rnn_lunyu = nn.RNN(50, 100, batch_first=True)\n",
    "\n",
    "seq_ids = [torch.tensor([word2id.get(w, 0) for w in line], dtype=torch.long) for line in lines]\n",
    "seq_lens = torch.tensor([len(line) for line in seq_ids])\n",
    "seq_ids_padded = nn.utils.rnn.pad_sequence(seq_ids, batch_first=True)\n",
    "\n",
    "seq_embs = embedding_lunyu(seq_ids_padded)\n",
    "seq_embs_packed = nn.utils.rnn.pack_padded_sequence(seq_embs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "out_packed,_= rnn_lunyu(seq_embs_packed)\n",
    "out_unpacked,_= nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "\n",
    "targets_padded = seq_ids_padded.clone()\n",
    "for i in range(len(targets_padded)):\n",
    "    targets_padded[i, :-1] = targets_padded[i, 1:].clone()\n",
    "    targets_padded[i, -1] = word2id.get('[PAD]', 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # super(RNNLM, self).__init__()\n",
    "        # self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "        # self.rnn = nn.RNN(kwargs['emb_size'], kwargs['hidden_size'], batch_first=True)\n",
    "        # self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "\n",
    "        # 多层RNN\n",
    "        num_layers = 3\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_size = kwargs['emb_size'] if i == 0 else kwargs['hidden_size']\n",
    "            self.rnn_layers.append(nn.RNN(input_size, kwargs['hidden_size'], batch_first=True))\n",
    "\n",
    "        self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "            embedded = self.embedding(seq)\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "            # 多层RNN的前向传播\n",
    "            rnn_output = packed\n",
    "            for rnn_layer in self.rnn_layers:\n",
    "                rnn_output, _ = rnn_layer(rnn_output)\n",
    "\n",
    "            padded, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
    "            logits = self.fc(padded)\n",
    "            return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on training set: 2.005084276199341\n",
      "Epoch [1/5], Loss: 7.145854949951172\n",
      "Epoch [2/5], Loss: 7.0641632080078125\n",
      "Epoch [3/5], Loss: 6.982481002807617\n",
      "Epoch [4/5], Loss: 6.900819301605225\n",
      "Epoch [5/5], Loss: 6.8191680908203125\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(100, len(word2id))\n",
    "logits = fc(out_unpacked)\n",
    "# log_probs = F.log_softmax(logits, dim=-1)\n",
    "#\n",
    "#\n",
    "# # Test result\n",
    "# print('logits:', logits.size())\n",
    "# print('log_probs:', log_probs.size())\n",
    "#\n",
    "# # Report Compute Perplexity\n",
    "# print('Report Compute Perplexity:')\n",
    "# loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "#\n",
    "#\n",
    "# # Calculate the loss\n",
    "# with torch.no_grad():\n",
    "#     loss = loss_fn(log_probs.view(-1, log_probs.size(-1)), targets_padded.view(-1))\n",
    "#\n",
    "# # Test result\n",
    "# print('loss:', loss.size())\n",
    "\n",
    "\n",
    "\n",
    "# 定义计算困惑度的函数\n",
    "def compute_perplexity(logits, targets):\n",
    "    with torch.no_grad():\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.nll_loss(log_probs.view(-1, log_probs.size(-1)), targets.view(-1), ignore_index=0, reduction='none')\n",
    "        # num_words = targets.ne(0).sum().item()  # 计算非填充词的数量\n",
    "        perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n",
    "\n",
    "perplexity = compute_perplexity(logits, targets_padded)\n",
    "print(f\"Perplexity on training set: {perplexity.item()}\")\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "\n",
    "emb_size = embedding_lunyu.embedding_dim\n",
    "\n",
    "hidden_size = rnn_lunyu.hidden_size\n",
    "\n",
    "# # 输入序列的填充后的张量\n",
    "# seq_ids_padded = seq_ids_padded\n",
    "# # 输入序列的长度\n",
    "# seq_lens = seq_lens\n",
    "# # 目标序列的填充后的张量\n",
    "# targets_padded = targets_padded\n",
    "\n",
    "model = RNNLM(vocab_size=vocab_size, emb_size=emb_size, hidden_size=hidden_size)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 训练过程\n",
    "model.train()\n",
    "num_epochs = 5  # 迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(seq_ids_padded, seq_lens)\n",
    "    loss = criterion(logits.view(-1, logits.shape[-1]), targets_padded.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on training set: 2.0097408294677734\n",
      "Epoch [1/5], Loss: 7.250707149505615\n",
      "Epoch [2/5], Loss: 7.168994903564453\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model, start_tokens, max_length=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_token = torch.tensor(start_tokens).unsqueeze(0)  # 添加批次维度\n",
    "        hidden_state = None\n",
    "\n",
    "        generated_sentence = start_tokens.copy()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            logits, hidden_state = model(current_token, hidden_state)\n",
    "            probabilities = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            predicted_token = torch.multinomial(probabilities, num_samples=1).squeeze(1)\n",
    "            generated_sentence.append(predicted_token.item())\n",
    "\n",
    "            if predicted_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "            current_token = predicted_token.unsqueeze(0)\n",
    "\n",
    "    return generated_sentence\n",
    "\n",
    "# 假设你有一个已经训练好的模型实例model和开始标记start_tokens和结束标记end_token\n",
    "start_tokens = [\"子\", \"曰\"]  # 开始标记组成的列表\n",
    "end_token = \"。\"  # 结束标记的值\n",
    "\n",
    "print(type(generate_sentence(model, start_tokens)))\n",
    "print(generate_sentence(model, start_tokens))\n",
    "\n",
    "# 生成句子\n",
    "# num_sentences = 5  # 要生成的句子数量\n",
    "# for _ in range(num_sentences):\n",
    "#     sentence = generate_sentence(model, start_tokens)\n",
    "#     print(\"Generated Sentence:\", sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "skipGram_model = gensim.models.KeyedVectors.load_word2vec_format(f'100_5_15.txt')\n",
    "\n",
    "# Get embeddings as numpy array\n",
    "pretrained_embeddings = skipGram_model.emb_v.cpu().weight.data.numpy()\n",
    "\n",
    "# # Truncated SVD\n",
    "# svd = TruncatedSVD(n_components=2)\n",
    "# embeddings_2d = svd.fit_transform(embeddings)\n",
    "\n",
    "# 应用预训练嵌入到模型的嵌入层\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# 计算使用预训练嵌入的困惑度\n",
    "logits_with_pretrained = model(seq_ids_padded, seq_lens)\n",
    "perplexity_with_pretrained = compute_perplexity(logits_with_pretrained, targets_padded)\n",
    "print(f\"Perplexity with pretrained embeddings: {perplexity_with_pretrained.item()}\")\n",
    "\n",
    "# 重新随机初始化模型的嵌入层\n",
    "model.embedding.reset_parameters()\n",
    "\n",
    "# 计算使用随机初始化嵌入的困惑度\n",
    "logits_random = model(seq_ids_padded, seq_lens)\n",
    "perplexity_random = compute_perplexity(logits_random, targets_padded)\n",
    "print(f\"Perplexity with randomly initialized embeddings: {perplexity_random.item()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
