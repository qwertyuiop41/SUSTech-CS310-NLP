{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 1). Recurrent Neural Networks for Language Modeling\n",
    "\n",
    "**Total points**: 30\n",
    "\n",
    "In this assignment, you will train a vanilla RNN language model on《论语》and evaluate its perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "# You can use the code from previous lab or rewrite it\n",
    "# Hint: you can comment out the `self.initTableNegatives()` in `__init__` method\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Modify word2id to make 0 as the padding token '[PAD]', and increase the index of all other words by 1\n",
    "# Modify the id2word list to make the first word '[PAD]' as well\n",
    "# Hint: Both word2id and id2word in utils.CorpusReader are dict objects\n",
    "\n",
    "# Hint: Both word2id and id2word in utils.CorpusReader are dict objects\n",
    "word2id: dict = {}\n",
    "id2word: dict = {}\n",
    "\n",
    "word2id.update({'[PAD]': 0})\n",
    "word2id.update({k: v+1 for k, v in corpus.word2id.items()})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('id2word:', sorted(list(id2word.items()), key=lambda x: x[0])[:5])\n",
    "print('word2id:', sorted(list(word2id.items()), key=lambda x: x[1])[:5])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "\n",
    "lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(line.strip())\n",
    "        if i == 15:\n",
    "            break\n",
    "\n",
    "embedding_lunyu = nn.Embedding(len(word2id), 50)\n",
    "rnn_lunyu = nn.RNN(50, 100, batch_first=True)\n",
    "\n",
    "seq_ids = [torch.tensor([word2id.get(w, 0) for w in line], dtype=torch.long) for line in lines]\n",
    "seq_lens = torch.tensor([len(line) for line in seq_ids])\n",
    "seq_ids_padded = nn.utils.rnn.pad_sequence(seq_ids, batch_first=True)\n",
    "\n",
    "seq_embs = embedding_lunyu(seq_ids_padded)\n",
    "seq_embs_packed = nn.utils.rnn.pack_padded_sequence(seq_embs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "out_packed,_= rnn_lunyu(seq_embs_packed)\n",
    "out_unpacked,_= nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "\n",
    "# Test result\n",
    "print('seq_ids_padded:', seq_ids_padded.size())\n",
    "print('seq_embs:', seq_embs.size())\n",
    "print('out_unpacked:', out_unpacked.size())\n",
    "\n",
    "# You should expect to see:\n",
    "# seq_ids_padded: torch.Size([16, 85])\n",
    "# seq_embs: torch.Size([16, 85, 50])\n",
    "# out_unpacked: torch.Size([16, 85, 100])\n",
    "\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "fc = nn.Linear(100, len(word2id))\n",
    "logits = fc(out_unpacked)\n",
    "log_probs = F.log_softmax(logits, dim=-1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('logits:', logits.size())\n",
    "print('log_probs:', log_probs.size())\n",
    "\n",
    "# You should expect to see:\n",
    "# logits: torch.Size([16, 85, 1353])\n",
    "\n",
    "\n",
    "### START YOUR CODE ###\n",
    "targets_padded = seq_ids_padded.clone()\n",
    "for i in range(len(targets_padded)):\n",
    "    targets_padded[i, :-1] = targets_padded[i, 1:].clone()\n",
    "    targets_padded[i, -1] = word2id.get('[PAD]', 0)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('targets_padded:', targets_padded.size())\n",
    "print('last column of targets_padded:', targets_padded[:, -1])\n",
    "\n",
    "# You should expect to see:\n",
    "# targets_padded: torch.Size([16, 85])\n",
    "# last column of targets_padded: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "        self.rnn = nn.RNN(kwargs['emb_size'], kwargs['hidden_size'], batch_first=True)\n",
    "        self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "        embedded = self.embedding(seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.rnn(packed)\n",
    "        padded, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        logits = self.fc(padded)\n",
    "        return logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
