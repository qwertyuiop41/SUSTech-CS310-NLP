{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 3 (part 1). Recurrent Neural Networks for Language Modeling\n",
    "\n",
    "**Total points**: 30\n",
    "\n",
    "In this assignment, you will train a vanilla RNN language model on《论语》and evaluate its perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n",
      "2\n",
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "word2id: dict = {}\n",
    "id2word: dict = {}\n",
    "\n",
    "word2id.update({'[PAD]': 0})\n",
    "word2id.update({k: v+1 for k, v in corpus.word2id.items()})\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "print(word2id['子'])\n",
    "print(word2id['曰'])\n",
    "print(word2id['。'])\n",
    "\n",
    "lines = []\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        lines.append(list(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self,vocab_size,emb_size,hidden_size ):\n",
    "        # super(RNNLM, self).__init__()\n",
    "        # self.embedding = nn.Embedding(kwargs['vocab_size'], kwargs['emb_size'])\n",
    "        # self.rnn = nn.RNN(kwargs['emb_size'], kwargs['hidden_size'], batch_first=True)\n",
    "        # self.fc = nn.Linear(kwargs['hidden_size'], kwargs['vocab_size'])\n",
    "\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.rnn=nn.RNN(emb_size, hidden_size, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "            embedded = self.embedding(seq)\n",
    "            packed = nn.utils.rnn.pack_padded_sequence(embedded, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "\n",
    "            out_packed,_ = self.rnn(packed)\n",
    "\n",
    "            out_unpacked,_ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "            logits = self.fc(out_unpacked)\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            return log_probs\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "def compute_perplexity(logits, targets):\n",
    "    loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "    with torch.no_grad():\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        loss = F.nll_loss(log_probs.view(-1, log_probs.size(-1)), targets.view(-1), ignore_index=0, reduction='none')\n",
    "        perplexity = torch.exp(loss.mean())\n",
    "    return perplexity\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], loss: 0.6954891681671143, perplexity:2.0046894550323486\n",
      "Epoch [2/2], loss: 0.6339095830917358, perplexity:1.8849656581878662\n"
     ]
    }
   ],
   "source": [
    "# randomly initialized embedding + report perplexity\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emb_size=50\n",
    "vocab_size=len(word2id)\n",
    "hidden_size=256\n",
    "model = RNNLM(vocab_size=vocab_size, emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(model_train,criterion,optimizer):\n",
    "    # 训练过程\n",
    "\n",
    "    num_epochs = 2  # 迭代次数\n",
    "    for epoch in range(num_epochs):\n",
    "        model_train.train()\n",
    "        optimizer.zero_grad()\n",
    "        seq_ids = [torch.tensor([word2id.get(w, 0) for w in line], dtype=torch.long).to(device) for line in lines]\n",
    "        seq_lens = torch.tensor([len(line) for line in seq_ids])\n",
    "        seq_ids_padded = nn.utils.rnn.pad_sequence(seq_ids, batch_first=True).to(device)\n",
    "\n",
    "        targets_padded = seq_ids_padded.clone()\n",
    "        for i in range(len(targets_padded)):\n",
    "            targets_padded[i, :-1] = targets_padded[i, 1:].clone()\n",
    "            targets_padded[i, -1] = word2id.get('[PAD]', 0)\n",
    "\n",
    "        log_probs = model_train(seq_ids_padded, seq_lens)\n",
    "        loss = criterion(log_probs.view(-1, log_probs.shape[-1]), targets_padded.view(-1))\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        perplexity = torch.exp(loss.mean())\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], loss: {loss.mean()}, perplexity:{perplexity}\")\n",
    "\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction=\"none\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "train(model,criterion,optimizer)\n",
    "torch.save(model, \"model_part1.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: 子曰也，吾，必以进怡优瞽，其身忠言？仞欲域忧\n"
     ]
    }
   ],
   "source": [
    "# generate sentences\n",
    "def generate_sentence(model, start_tokens, end_token, max_length=20):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_ids = torch.tensor([word2id.get(w, 0) for w in start_tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        current_ids=start_ids\n",
    "        generated_sentence = start_tokens.copy()\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            log_probs=model(current_ids,[len(current_ids[0])])\n",
    "            last_word_log_probs = log_probs[:, -1, :]\n",
    "            predicted_id = torch.multinomial(torch.exp(last_word_log_probs.squeeze()), 1).item()\n",
    "            predicted_word = id2word.get(predicted_id, \"\")\n",
    "            generated_sentence.append(predicted_word)\n",
    "            if predicted_word == end_token:\n",
    "                break\n",
    "            current_ids = torch.tensor([[predicted_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    return \"\".join(generated_sentence)\n",
    "\n",
    "start_tokens = [\"子\", \"曰\"]  # 开始标记组成的列表\n",
    "end_token = \"。\"  # 结束标记的值\n",
    "max_length = 20  # 生成句子的最大长度\n",
    "sentence = generate_sentence(model, start_tokens, end_token, max_length)\n",
    "print(\"Generated Sentence:\", sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已读入\n",
      "Epoch [1/2], loss: 0.0, perplexity:1.0\n",
      "Epoch [2/2], loss: 0.0, perplexity:1.0\n"
     ]
    }
   ],
   "source": [
    "# pretrained embeddings + perplexity\n",
    "embeddings = np.random.rand(len(word2id), 50)\n",
    "\n",
    "# 读取txt文件内容\n",
    "with open('50_1_5.txt', 'r',encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "print('已读入')\n",
    "# 遍历每一行内容\n",
    "for i,line in enumerate(lines):\n",
    "    if i!=0:\n",
    "        # 利用空格分割每一行，获取单词和对应的embedding向量\n",
    "        parts = line.split()\n",
    "        word = parts[0]\n",
    "        embedding = np.array([float(x) for x in parts[1:]])\n",
    "        if word in word2id:\n",
    "            # 将word和embedding添加到字典中\n",
    "            embeddings[word2id[word]] = embedding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pretrained_embeddings = torch.from_numpy(embeddings).float()\n",
    "\n",
    "model_pretrained=RNNLM(vocab_size=vocab_size, emb_size=emb_size, hidden_size=hidden_size).to(device)\n",
    "model_pretrained.embedding.from_pretrained(pretrained_embeddings)\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction=\"none\")\n",
    "optimizer = optim.Adam(model_pretrained.parameters(), lr=0.1)\n",
    "train(model_pretrained,criterion,optimizer)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
