{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 4 (part 1): Training a *n*-gram language model\n",
    "\n",
    "Install the `nltk` library:\n",
    "```bash\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the corpus\n",
    "\n",
    "This time we use the full text of 《论语》."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "['子', '曰', '：', '学', '而', '时', '习', '之', '，', '不', '亦', '说', '乎', '？', '有', '朋', '自', '远', '方', '来', '，', '不', '亦', '乐', '乎', '？', '人', '不', '知', '而', '不', '愠', '，', '不', '亦', '君', '子', '乎', '？']\n",
      "['孔', '子', '曰', '：', '不', '知', '命', '，', '无', '以', '为', '君', '子', '也', '；', '不', '知', '礼', '，', '无', '以', '立', '也', '；', '不', '知', '言', '，', '无', '以', '知', '人', '也', '。']\n"
     ]
    }
   ],
   "source": [
    "# A cleaned corpus is provided\n",
    "\n",
    "text: List[List[str]] = [] \n",
    "with open(f\"lunyu_20chapters.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        text.append(list(line))\n",
    "\n",
    "# Test result\n",
    "print(len(text))\n",
    "print(text[0])\n",
    "print(text[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Implement bigrams\n",
    "Try implement the customized `my_bigrams()` function to collect all bigrams from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: ['子', '曰', '：', '学', '而', '时', '习', '之']\n",
      "bigrams: [('子', '曰'), ('曰', '：'), ('：', '学'), ('学', '而'), ('而', '时'), ('时', '习'), ('习', '之')]\n"
     ]
    }
   ],
   "source": [
    "def my_bigrams(text: List[str]) -> List[Tuple[str, str]]:\n",
    "    bigrams = []\n",
    "    ### START CODE HERE ###\n",
    "    for i in range(len(text) - 1):\n",
    "        bigrams.append((text[i], text[i+1]))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return bigrams\n",
    "\n",
    "# Test result\n",
    "print('unigrams:', text[0][:8])\n",
    "print('bigrams:', my_bigrams(text[0][:8]))\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# unigrams: ['子', '曰', '：', '学', '而', '时', '习', '之']\n",
    "# bigrams: [('子', '曰'), ('曰', '：'), ('：', '学'), ('学', '而'), ('而', '时'), ('时', '习'), ('习', '之')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `nltk.util.bigrams`, you should see the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams from nltk: [('子', '曰'), ('曰', '：'), ('：', '学'), ('学', '而'), ('而', '时'), ('时', '习'), ('习', '之')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "\n",
    "print('bigrams from nltk:', list(bigrams(text[0][:8])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Count the frequency of bigrams\n",
    "\n",
    "Using the just defined `my_bigrams()` function, count the frequency of all bigrams throughout 《论语》. Print the top 10 most frequent ones.\n",
    "\n",
    "*Hint*: `collections.Counter` is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('曰', '：'), 739),\n",
      " (('子', '曰'), 452),\n",
      " (('也', '。'), 187),\n",
      " (('也', '，'), 132),\n",
      " (('君', '子'), 107),\n",
      " (('矣', '。'), 105),\n",
      " (('，', '不'), 104),\n",
      " (('之', '，'), 86),\n",
      " (('，', '子'), 86),\n",
      " (('？', '子'), 81)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "### START CODE HERE ###\n",
    "all_bigrams = []\n",
    "# bigrams_freq = None\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    all_bigrams.append(my_bigrams(line))\n",
    "\n",
    "\n",
    "flattened_bigrams = [item for sublist in all_bigrams for item in sublist]\n",
    "\n",
    "bigrams_freq = Counter(flattened_bigrams)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "pprint(bigrams_freq.most_common(10))\n",
    "\n",
    "### You are expected to get the following output:\n",
    "# [(('曰', '：'), 739),\n",
    "#  (('子', '曰'), 452),\n",
    "#  (('也', '。'), 187),\n",
    "#  (('也', '，'), 132),\n",
    "#  (('君', '子'), 107),\n",
    "#  (('矣', '。'), 105),\n",
    "#  (('，', '不'), 104),\n",
    "#  (('之', '，'), 86),\n",
    "#  (('，', '子'), 86),\n",
    "#  (('？', '子'), 81)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Preprocessing Steps\n",
    "\n",
    "`nltk` provides very useful functions for padding. The default padding token is `\"<s\"` for the start of sentence and `\"</s>\"` for the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded with n=2: ['<s>', '子', '曰', '：', '学', '而', '时', '习', '之', '</s>']\n",
      "padded with n=3: ['<s>', '<s>', '子', '曰', '：', '学', '而', '时', '习', '之', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "print('padded with n=2:', list(pad_both_ends(text[0][:8], n=2))) # n=2 for bigrams\n",
    "print('padded with n=3:', list(pad_both_ends(text[0][:8], n=3))) # n=3 for trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the make more robust we could also train it on a mixture of unigrams, bigrams, and higher order terms. `nltk` once again helpfully provides a function called `everygrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams + bigrams:\n",
      "[('<s>',),\n",
      " ('<s>', '子'),\n",
      " ('子',),\n",
      " ('子', '曰'),\n",
      " ('曰',),\n",
      " ('曰', '：'),\n",
      " ('：',),\n",
      " ('：', '学'),\n",
      " ('学',),\n",
      " ('学', '而'),\n",
      " ('而',),\n",
      " ('而', '时'),\n",
      " ('时',),\n",
      " ('时', '习'),\n",
      " ('习',),\n",
      " ('习', '之'),\n",
      " ('之',),\n",
      " ('之', '</s>'),\n",
      " ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "padded_bigrams = list(pad_both_ends(text[0][:8], n=2))\n",
    "\n",
    "print('unigrams + bigrams:')\n",
    "pprint(list(everygrams(padded_bigrams, max_len=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two steps can be combined into the `padded_everygrams_pipeline` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "train, vocab = padded_everygram_pipeline(2, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the `train` and `vocab` returned are lazy iterators, which are only evaluated on demand at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'itertools.chain'>\n"
     ]
    }
   ],
   "source": [
    "# Test train and vocab\n",
    "print(type(train))\n",
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Having the data prepared, we are ready to start training an n-gram model. \n",
    "\n",
    "We can simply use the `MLE` class from `nltk`, and specify the highest order n-gram to instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "lm = MLE(2) # 2 for a bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is straight-forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the two iterators are not used yet; if yes, re-create them\n",
    "\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's done! Now check out some attributes of `lm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 1355 items>\n",
      "<NgramCounter with 2 ngram orders and 41284 ngrams>\n",
      "('子', '曰', '：', '学', '而', '时', '习', '之')\n",
      "('<UNK>', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "print(lm.vocab) # A summary of the vocabulary\n",
    "print(lm.counts) # A summary of the ngrams and their counts\n",
    "\n",
    "# Check if words exist in vocabulary\n",
    "print(lm.vocab.lookup(text[0][:8])) \n",
    "print(lm.vocab.lookup([\"some\", \"sample\", \"words\"])) # Unknow words are mapped to <UNK>\n",
    "\n",
    "### You should expect to see the following output:\n",
    "# <Vocabulary with cutoff=1 unk_label='<UNK>' and 1355 items>\n",
    "# <NgramCounter with 2 ngram orders and 41284 ngrams>\n",
    "# ('子', '曰', '：', '学', '而', '时', '习', '之')\n",
    "# ('<UNK>', '<UNK>', '<UNK>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained model\n",
    "\n",
    "It provides convenient interface to access counts for unigrams and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972\n",
      "452\n"
     ]
    }
   ],
   "source": [
    "print(lm.counts['子'])\n",
    "print(lm.counts[['子']]['曰']) # Count of the bigram ('子', '曰')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MLE.score()` returns the probability of a word given its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046511627906976744\n",
      "0.6796875\n",
      "0.46502057613168724\n"
     ]
    }
   ],
   "source": [
    "print(lm.score('子'))\n",
    "print(lm.score('子', context=['<s>'])) # P('子' | '<s>')\n",
    "print(lm.score('曰', context=['子'])) # P('曰' | '子')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MLE.perplexity()` returns the perplexity of a given sequence of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.479500591963152\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "likely_senquence = [('<s>',), ('<s>', '子'),('子',), ('子', '曰'), ('曰',)] # “<s> 子 曰”\n",
    "unlikly_sequence = [('<s>',), ('<s>', '曰'),('曰',), ('曰', '子'), ('子',)] # “<s> 曰 子” -- never seen in the training data\n",
    "\n",
    "print(lm.perplexity(likely_senquence))\n",
    "print(lm.perplexity(unlikly_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Find the most probable/likelihood word given a context\n",
    "\n",
    "Go through all the words in vocabulary, and find the 10 most probable word given the context \"乐\" (delightness/joy).\n",
    "\n",
    "*Hint*: `MLE.vocab` provides the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('，', 0.25),\n",
      " ('。', 0.10416666666666667),\n",
      " ('不', 0.0625),\n",
      " ('之', 0.041666666666666664),\n",
      " ('乎', 0.041666666666666664),\n",
      " ('、', 0.041666666666666664),\n",
      " ('云', 0.041666666666666664),\n",
      " ('征', 0.041666666666666664),\n",
      " ('而', 0.020833333333333332),\n",
      " ('亦', 0.020833333333333332)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "probs = {}\n",
    "### START CODE HERE ###\n",
    "\n",
    "\n",
    "# Context word\n",
    "context = '乐'\n",
    "\n",
    "for word in lm.vocab:\n",
    "    probs[word] = lm.score(word,context=[context])\n",
    "\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "pprint(sorted(probs.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# [('，', 0.25),\n",
    "#  ('。', 0.10416666666666667),\n",
    "#  ('不', 0.0625),\n",
    "#  ('之', 0.041666666666666664),\n",
    "#  ('乎', 0.041666666666666664),\n",
    "#  ('、', 0.041666666666666664),\n",
    "#  ('云', 0.041666666666666664),\n",
    "#  ('征', 0.041666666666666664),\n",
    "#  ('而', 0.020833333333333332),\n",
    "#  ('亦', 0.020833333333333332)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a sentence\n",
    "\n",
    "Use the `MLE.generate()` function to generate a sentence. Try with any context words you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['而', '不', '食', '。', '</s>', '子', '曰', '：', '上', '好']"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate(10, text_seed=['子'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
