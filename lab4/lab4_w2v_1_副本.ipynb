{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 4 (part 2): Data preparation for implementing word2vec\n",
    "\n",
    "skipgram architecture and negative sampling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pprint import pprint\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "# We set min_count=1 to include all words in the corpus\n",
    "\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "子\n",
      "1352\n"
     ]
    }
   ],
   "source": [
    "print(corpus.word2id[\"子\"])\n",
    "print(corpus.id2word[1])\n",
    "print(len(corpus.id2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient way for negative sampling\n",
    "\n",
    "In `utils.CorpusReader` class, we have implemented a method `initTableNegatives`. It creates a list of words (`self.negatives`) with a size of 1e8. This size is set a large value so that it scales up to very large corpus. \n",
    "\n",
    "The list contains the index of each word in the vocabulary, whose probability is proportional to the power of 0.75 of the word's original frequency count. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simulation of how initTableNegatives works\n",
    "# The impl. in utils.py is a bit different, but the idea is the same\n",
    "word_frequency = {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4}\n",
    "\n",
    "# the scaled sum of frequencies Z = 1**0.75 + 2**0.75 + 3**0.75 + 4**0.75 = 7.7897270\n",
    "# then the scaled probability of a = 1**0.75 / Z = 0.12837420128374202\n",
    "# the scaled probability of b = 2**0.75 / Z = 0.21589881215898812\n",
    "# the scaled probability of c = 3**0.75 / Z = 0.29262990292629903\n",
    "# the scaled probability of d = 4**0.75 / Z = 0.3630970836309708\n",
    "\n",
    "def initTableNegatives():\n",
    "    pow_frequency = np.array(list(word_frequency.values())) ** 0.75\n",
    "    words_pow = sum(pow_frequency)\n",
    "    ratio = pow_frequency / words_pow\n",
    "    count = np.round(ratio * CorpusReader.NEGATIVE_TABLE_SIZE)\n",
    "    negatives = []\n",
    "    for wid, c in enumerate(count):\n",
    "        negatives += [wid] * int(c)\n",
    "    negatives = np.array(negatives)\n",
    "    np.random.shuffle(negatives)\n",
    "    return negatives\n",
    "\n",
    "negatives = initTableNegatives()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999999\n",
      "{0, 1, 2, 3}\n",
      "0.12837420128374202\n",
      "0.21589881215898812\n",
      "0.29262990292629903\n",
      "0.3630970836309708\n"
     ]
    }
   ],
   "source": [
    "print(len(negatives))\n",
    "print(set(negatives)) # the word indices: a -> 0, b -> 1, c -> 2, d -> 3\n",
    "print(np.sum(negatives == 0) / len(negatives)) # should be the scaled probability of a\n",
    "print(np.sum(negatives == 1) / len(negatives)) # should be the scaled probability of b\n",
    "print(np.sum(negatives == 2) / len(negatives)) # should be the scaled probability of c\n",
    "print(np.sum(negatives == 3) / len(negatives)) # should be the scaled probability of d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `getNegatives` method returns the negative samples for a target word. The idea is to chop off a segment of given `size` from the `negatives` list. \n",
    "\n",
    "If the segment contains the target word, it is discarded and a new segment is taken. This is done to avoid the target word itself to be sampled as a negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 45, 210,  93,  27, 218])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test some examples\n",
    "\n",
    "corpus.getNegatives(target=1, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for training\n",
    "\n",
    "Now we are going to implement the sliding window to generate center, outside, and negative words for each position in a sentence.\n",
    "\n",
    "- It takes a list of words as input and go through each word as a center word.\n",
    "- For each center word, both the left and right `window_size` words are considered as outside words. This number is smaller near the two ends of the sentence.\n",
    "- Call `corpus.getNegatives` to get negative samples for each center word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    word_ids = [] # convert the list of words to a list of word ids\n",
    "    # Use for loop and yield\n",
    "    word_ids = [corpus.word2id[word] for word in words]  # Convert the list of words to a list of word ids\n",
    "\n",
    "    for center_index, center_id in enumerate(word_ids):\n",
    "        context_indices = None\n",
    "        # Iterate over the left context words\n",
    "        for i in range(max(0, center_index - window_size), center_index):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "        # Iterate over the right context words\n",
    "        for i in range(center_index + 1, min(center_index + window_size + 1, len(word_ids))):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "    ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['学', '而', '时', '习', '之']\n",
      "word ids: [46, 8, 224, 544, 5]\n",
      "\n",
      "When window size is 3, for center word 学 -> 46\n",
      "the outside words are: \n",
      "而 -> 8\n",
      "时 -> 224\n",
      "习 -> 544\n",
      "\n",
      "output from generate_data:\n",
      "[(46, 8, array([294,  29, 665, 326, 510])),\n",
      " (46, 224, array([ 94, 712,  73, 374, 276])),\n",
      " (46, 544, array([ 33, 842,   0, 619, 123]))]\n"
     ]
    }
   ],
   "source": [
    "# Test generate_data\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "print('words:', words)\n",
    "print('word ids:', [corpus.word2id[word] for word in words])\n",
    "\n",
    "# first center word is 学\n",
    "print()\n",
    "print(f'When window size is 3, for center word 学 -> {corpus.word2id[\"学\"]}')\n",
    "print(f'the outside words are: ')\n",
    "print(f'而 -> {corpus.word2id[\"而\"]}')\n",
    "print(f'时 -> {corpus.word2id[\"时\"]}')\n",
    "print(f'习 -> {corpus.word2id[\"习\"]}')\n",
    "\n",
    "print()\n",
    "print('output from generate_data:')\n",
    "data = list(generate_data(list(text), window_size=3, k=5, corpus=corpus))\n",
    "print(data[:3])\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# words: ['学', '而', '时', '习', '之']\n",
    "# word ids: [46, 8, 224, 544, 5]\n",
    "\n",
    "# When window size is 3, for center word 学 -> 46\n",
    "# the outside words are: \n",
    "# 而 -> 8\n",
    "# 时 -> 224\n",
    "# 习 -> 544\n",
    "\n",
    "# output from generate_data:\n",
    "# [(46, 8, array([354,   3, 831, 570,  27])),\n",
    "#  (46, 224, array([1077, 1095,   89,  340,   92])),\n",
    "#  (46, 544, array([ 49, 488,   4, 269,  30]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the above data are not in batch. We want all center words are batched into a tensor of dimension `batch_size`; same for the outside words and negative samples.\n",
    "\n",
    "For example, in \"学而时习之\", if `batch_size` is 4, then the returned batch[0] will contain three tensors. \n",
    "- The first tensor contains center words, i.e., 3 \"学\" plus 1 \"而\" => [46, 46, 46, 8]\n",
    "- The second tensor contains the correponding outside words, i.e., \"而\", \"时\", and \"习\" for \"学\"; \"学\" for \"而\" => [8, 224, 544,  46]\n",
    "- The third tensor contains the negative samples, whose dimension is `batch_size` $\\times$ `k`\n",
    "  \n",
    "The data type of the tensors is `torch.long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        center_batch = []\n",
    "        outside_batch = []\n",
    "        negative_batch = []\n",
    "\n",
    "        for center, outside, negative in batch:\n",
    "            center_batch.append(center)\n",
    "            outside_batch.append(outside)\n",
    "            negative_batch.append(negative)\n",
    "\n",
    "        center_tensor = torch.tensor(center_batch, dtype=torch.long)\n",
    "        outside_tensor = torch.tensor(outside_batch, dtype=torch.long)\n",
    "        negative_tensor = torch.tensor(negative_batch, dtype=torch.long)\n",
    "\n",
    "        yield center_tensor, outside_tensor, negative_tensor\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# (tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[  85,    3,   72,   26,   35],\n",
    "#         [   7,    1,  487,   20,    4],\n",
    "#         [  12,  227,    2,   25,  639],\n",
    "#         [ 582,  148,   15, 1203,   85]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([46, 46, 46,  8]), tensor([  8, 224, 544,  46]), tensor([[  89,  181,   19,   40, 1219],\n",
      "        [ 505,    1,   70,  228,   35],\n",
      "        [ 485,  382,   56,   78,    1],\n",
      "        [   1,   51,  396,  208,  521]]))\n"
     ]
    }
   ],
   "source": [
    "# Test batchify\n",
    "\n",
    "text = \"学而时习之\"\n",
    "words = list(text)\n",
    "data = list(generate_data(words, window_size=3, k=5, corpus=corpus))\n",
    "\n",
    "batches = list(batchify(data, batch_size=4))\n",
    "print(batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the SkipGram class\n",
    "\n",
    "`SkipGram` is a subclass of `nn.Module`. The two key components are:\n",
    "- `__init__`: initialize the embeddings\n",
    "  - Two `nn.Embedding` objects are created: `self.emb_v` for center words; `self.emb_u` for outside words and negative samples.\n",
    "  - Each `nn.Embedding` is created with `vocab_size` and `emb_dim` as input arguments. \n",
    "  - `self.emb_v` is initialized with uniform distribution; `self.emb_u` is initialized with zeros.\n",
    "- `forward`: given input tensors, return the loss of the model\n",
    "  - Takes three tensors as input: center words, outside words, and negative samples. They are the output from the previously defined `batchify` function.\n",
    "  - Compute the loss using the formula: $-\\log\\sigma(v_c \\cdot u_o) - \\sum_{k=1}^K \\log\\sigma(-v_c \\cdot u_k)$\n",
    "\n",
    "*Hint*:\n",
    "- For the $\\log\\sigma$ function, you can use `F.logsigmoid` in PyTorch. See the imported module: `import torch.nn.functional as F`\n",
    "- If the input to `F.logsigmoid` is too large, it will return 0, which is not good for training. You can use `torch.clamp` to limit the input to a certain range. For example, `torch.clamp(x, min=-10, max=10)` will limit the input to be in the range of $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        #\n",
    "        # ### YOUR CODE HERE ###\n",
    "        # Positive sample score\n",
    "        pos_score = torch.sum(torch.mul(v_c, u_o), dim=1)  # (B,)\n",
    "        pos_loss = F.logsigmoid(torch.clamp(pos_score, min=-10, max=10))  # (B,)\n",
    "\n",
    "        # Negative sample scores\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2)  # (B, k)\n",
    "        neg_loss = F.logsigmoid(torch.clamp(-neg_score, min=-10, max=10))  # (B, k)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = -torch.sum(pos_loss + torch.sum(neg_loss, dim=1))  # Scalar\n",
    "\n",
    "        # ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(143.8297)\n",
      "tensor(92.2233)\n",
      "tensor(180.2658)\n",
      "tensor(158.9765)\n",
      "tensor(137.2470)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "vacob_size = len(corpus.id2word)\n",
    "emb_size = 32\n",
    "model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "weight = torch.empty(vacob_size, emb_size)\n",
    "start_value = 0.01\n",
    "for i in range(vacob_size):\n",
    "    weight[i] = start_value + i * 0.01\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.emb_v.weight.copy_(weight)\n",
    "    model.emb_u.weight.copy_(weight)\n",
    "\n",
    "    for batch in batches:\n",
    "        loss = model(batch[0], batch[1], batch[2])\n",
    "        print(loss)\n",
    "\n",
    "### You are expected to see the following output:\n",
    "### Note that the negative samples are random, so you may see different numbers\n",
    "# tensor([25.4124, 36.0555, 31.7922, 22.9003])\n",
    "# tensor([30.0502, 14.1560, 26.8572, 45.7634])\n",
    "# tensor([45.7649, 50.0003, 50.0134, 50.0003])\n",
    "# tensor([45.2376, 40.6255, 41.9053,  9.2085])\n",
    "# tensor([34.3999, 26.6441, 25.4124, 36.0555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.330986499786377\n"
     ]
    }
   ],
   "source": [
    "# Test the loss\n",
    "v_c = torch.FloatTensor([[0.1, 0.2, 0.3, 0.4, 0.5]])  \n",
    "u_o = torch.FloatTensor([[0.5, 0.4, 0.3, 0.2, 0.1]])  \n",
    "u_n = torch.FloatTensor([[[0.2, 0.2, 0.2, 0.2, 0.2], \n",
    "                          [0.3, 0.3, 0.3, 0.3, 0.3]]])\n",
    "\n",
    "### YOUR CODE HERE ###score_o = torch.sum(torch.mul(v_c, u_o), dim=1)\n",
    "# loss = None\n",
    "\n",
    "# Positive sample score\n",
    "pos_score = torch.sum(torch.mul(v_c, u_o), dim=1)  # (B,)\n",
    "pos_loss = F.logsigmoid(torch.clamp(pos_score, min=-10, max=10))  # (B,)\n",
    "\n",
    "# Negative sample scores\n",
    "neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2)  # (B, k)\n",
    "neg_loss = F.logsigmoid(torch.clamp(-neg_score, min=-10, max=10))  # (B, k)\n",
    "\n",
    "# Combine losses\n",
    "loss = -torch.sum(pos_loss + torch.sum(neg_loss, dim=1))  # Scalar\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "### You are expected to see the following output:\n",
    "# Loss: 2.330986261367798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
