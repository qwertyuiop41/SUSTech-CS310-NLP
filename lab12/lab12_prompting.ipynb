{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 12: Play with Prompting and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Play with zero-shot and few-shot prompting with ChatGLM-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1)** Download the ChatGLM-3 model from ModelScope: https://modelscope.cn/models/ZhipuAI/chatglm3-6b/files\n",
    " - `model.safetensors.index.json`, `config.json`, `configuration.json`\n",
    " - `model-00001-of-00007.safetensors` to `model-00007-of-00007.safetensors`\n",
    " - `tokenizer_config.json`, `tokenizer.model`\n",
    "Put all the files in a folder such as `./chatglm3-6b`. Or, you can directly download the zip file from the course website and unzip it.\n",
    "\n",
    "**Step 2)** Download and build the tool `chatglm.cpp` (https://github.com/li-plus/chatglm.cpp), which allows you to run most Chinese LLMs locally on your laptop computer. \n",
    " - Follow the instruction in the repository's README, and test it with the ChatGLM-3 model downloaded at Step 1.\n",
    "\n",
    "**Step 3)** Interact with ChatGLM-3 in the command line, and try to solve the following problems.\n",
    " - Use zero-shot and few-shot prompting to solve the problems.\n",
    " - Add Chain-of-Thought prompt if needed.\n",
    "\n",
    "Try solving these problems with prompting:\n",
    "1. Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there? A: \n",
    "2. 鸡和兔在一个笼子里，共有35个头，94只脚，那么鸡有多少只，兔有多少只？\n",
    "3. Q: 242342 + 423443 = ? A: \n",
    "4. 一个人花8块钱买了一只鸡，9块钱卖掉了，然后他觉得不划算，花10块钱又买回来了，11块卖给另外一个人。问他赚了多少?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Implement LoRA (Basics)\n",
    "\n",
    "Low rank adaptation (LoRA) applies to the query and value matrcies of the attentation layer, i.e., $W^Q$ and $W^V$. \n",
    "\n",
    "$W^Q$ and $W^V$ are usually implemented as `nn.Linear` layers in PyTorch, so here we implement `LoRALinear` as a subclass of `nn.Linear`.\n",
    "\n",
    "There are two places you need to implement:\n",
    "1. In the `__init__` function, implement the `A` and `B` matrices as instances of `nn.Parameter`.\n",
    "   - `A` is in shape of `(lora_rank, in_features)`; `B` is in shape of `(out_features, lora_rank)`.\n",
    "   - Initialize them with `torch.empty`; `reset_parameters` already takes care of later initialization.\n",
    "2. In the `forward` function, implement the LoRA equation of computing the hidden state `h`:\n",
    "   - `h = W(x) + B(A(lora_dropout(x))) * scaling`, where `W(x)` is already implemented for you; `lora_dropout` is defined for you. \n",
    "   - The parentheses `()` calls need be implemented using `torch.nn.functional.linear` (https://pytorch.org/docs/stable/nn.functional.html#linear-functions).\n",
    "   - `scaling` is the class attribute `self.lora_scaling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting the methods of nn.Linear:\n",
    "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
    "class LoRALinear(nn.Linear):\n",
    "\n",
    "    def __init__(self,\n",
    "                 # nn.Linear parameters\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None,\n",
    "                 # LoRA parameters\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_alpha: float = 0.0,\n",
    "                 lora_dropout: float = 0.0,\n",
    "                ) -> None:\n",
    "        nn.Linear.__init__(\n",
    "            self,\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        # LoRA stuff\n",
    "        self.has_weights_merged = False\n",
    "        if lora_rank > 0:\n",
    "            self.lora_dropout = nn.Dropout(lora_dropout)\n",
    "\n",
    "            self.lora_scaling = lora_alpha / lora_rank\n",
    "            ### START YOUR CODE ###\n",
    "            # self.lora_A = None\n",
    "            # self.lora_B = None\n",
    "            self.lora_A = nn.Parameter(torch.empty(lora_rank, in_features))\n",
    "            self.lora_B = nn.Parameter(torch.empty(out_features, lora_rank))\n",
    "            ### END YOUR CODE ###\n",
    "\n",
    "            self.lora_A.requires_grad = False\n",
    "            self.lora_B.requires_grad = False\n",
    "\n",
    "            self.reset_parameters()\n",
    "\n",
    "    def is_lora(self) -> bool:\n",
    "        return hasattr(self, 'lora_A')\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if self.is_lora():\n",
    "            torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5)) # Same as nn.Linear\n",
    "            torch.nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = nn.Linear.forward(self, input) # This is W(x)\n",
    "        if not self.has_weights_merged and self.is_lora():\n",
    "            ### START YOUR CODE ###\n",
    "            # h = W(x) + B(A(lora_dropout(x))) * scaling\n",
    "            # h = None\n",
    "            h = x + self.lora_B @ torch.nn.functional.linear(self.lora_dropout(x), self.lora_A) * self.lora_scaling\n",
    "        else:\n",
    "            h=x\n",
    "            ### END YOUR CODE ###\n",
    "        return h\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        if self.is_lora():\n",
    "            out += f', lora_rank={self.lora_A.shape[0]}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout.p}'\n",
    "        return out\n",
    "\n",
    "    def train(self, mode: bool = True) -> \"LoRALinear\":\n",
    "        nn.Linear.train(self, mode)\n",
    "        if self.has_weights_merged and self.is_lora():\n",
    "            # de-merge weights, i.e., remove BA from W = W + BA\n",
    "            self.weight.data -= self.lora_scaling * self.lora_B @ self.lora_A\n",
    "            self.has_weights_merged = False\n",
    "        return self\n",
    "\n",
    "    def eval(self) -> \"LoRALinear\":\n",
    "        nn.Linear.eval(self)\n",
    "        if not self.has_weights_merged and self.is_lora():\n",
    "            # merge weights, i.e., add BA to W\n",
    "            self.weight.data += self.lora_scaling * self.lora_B @ self.lora_A\n",
    "            self.has_weights_merged = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 768])\n",
      "torch.Size([1, 1024, 2304])\n",
      "tensor([-0.7818,  0.0917,  0.1308, -0.3660,  1.2284])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from dataclasses import dataclass\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 \n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True \n",
    "    # LoRA parameters\n",
    "    lora_rank: int = 0\n",
    "    lora_alpha: float = 0.0\n",
    "    lora_dropout: float = 0.0\n",
    "\n",
    "config = GPTConfig()\n",
    "attn = LoRALinear(\n",
    "            in_features=config.n_embd,\n",
    "            out_features=3 * config.n_embd,\n",
    "            bias=config.bias,\n",
    "            lora_rank=config.lora_rank,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            lora_dropout=config.lora_dropout\n",
    "        )\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = torch.randn(1, config.block_size, config.n_embd)\n",
    "    x2 = attn(x)\n",
    "    print(x.shape)\n",
    "    print(x2.shape)\n",
    "    print(x2[0, 0, :5])\n",
    "\n",
    "# Expected output:\n",
    "# torch.Size([1, 1024, 768])\n",
    "# torch.Size([1, 1024, 2304])\n",
    "# tensor([-0.7818,  0.0917,  0.1308, -0.3660,  1.2284])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
