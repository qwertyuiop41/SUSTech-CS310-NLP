{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 2. Word2vec Implementation \n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "Train a word2vec model using the **skip-gram** architecture and **negative sampling**.\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from *Lab 4 (part 2): Data preparation for implementing word2vec*. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "import logging\n",
    "import gensim\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing\n",
    "\n",
    "The corpus data is in `lunyu_20chapters.txt`. Use the `CorpusReader` class in `utils.py` to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary: 1352\n"
     ]
    }
   ],
   "source": [
    "# Read raw data in lunyu_20chapters.txt\n",
    "# 配置日志记录器\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# 配置日志处理程序\n",
    "log_file = 'A2.log'\n",
    "if os.path.exists(log_file):\n",
    "    open(log_file, 'w').close()\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# 配置日志格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# 配置日志处理程序 - 控制台处理程序\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# 将控制台处理程序添加到日志记录器\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "# 将处理程序添加到日志记录器\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)\n",
    "with open('lunyu_20chapters.txt', 'r', encoding='utf-8') as file:\n",
    "    raw_data = file.read()\n",
    "raw_data=raw_data.replace('\\n','')\n",
    "# print(raw_data[:10])\n",
    "\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_data\n",
      "[(1, 4, array([ 63,  20,  32,  66, 630])), (1, 3, array([209, 217,  18, 125, 213])), (1, 46, array([ 455,   33,  109, 1175,  716])), (4, 1, array([109, 718, 284, 807, 827])), (4, 3, array([ 87,  10, 439,  53,  68]))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2s/8cp0j_3x6ts54xdw2_dpwhrh0000gn/T/ipykernel_67740/2865262044.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  negative_tensor = torch.tensor(negative_batch, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchify\n",
      "(tensor([1, 1, 1, 4]), tensor([ 4,  3, 46,  1]), tensor([[  63,   20,   32,   66,  630],\n",
      "        [ 209,  217,   18,  125,  213],\n",
      "        [ 455,   33,  109, 1175,  716],\n",
      "        [ 109,  718,  284,  807,  827]]))\n"
     ]
    }
   ],
   "source": [
    "# Re-use the code from lab with necessary modifications\n",
    "\n",
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    word_ids = [corpus.word2id[word] for word in words]  # Convert the list of words to a list of word ids\n",
    "\n",
    "    for center_index, center_id in enumerate(word_ids):\n",
    "        context_indices = None\n",
    "        # Iterate over the left context words\n",
    "        for i in range(max(0, center_index - window_size), center_index):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "        # Iterate over the right context words\n",
    "        for i in range(center_index + 1, min(center_index + window_size + 1, len(word_ids))):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        center_batch = []\n",
    "        outside_batch = []\n",
    "        negative_batch = []\n",
    "\n",
    "        for center, outside, negative in batch:\n",
    "            center_batch.append(center)\n",
    "            outside_batch.append(outside)\n",
    "            negative_batch.append(negative)\n",
    "\n",
    "        center_tensor = torch.tensor(center_batch, dtype=torch.long)\n",
    "        outside_tensor = torch.tensor(outside_batch, dtype=torch.long)\n",
    "        negative_tensor = torch.tensor(negative_batch, dtype=torch.long)\n",
    "\n",
    "        yield center_tensor, outside_tensor, negative_tensor\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "data = list(generate_data(list(raw_data), window_size=3, k=5, corpus=corpus))\n",
    "print(\"generate_data\")\n",
    "print(data[:5])\n",
    "\n",
    "batches = list(batchify(data, batch_size=4))\n",
    "print(\"batchify\")\n",
    "print(batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        #\n",
    "        # ### YOUR CODE HERE ###\n",
    "        # Positive sample score\n",
    "        pos_score = torch.sum(torch.mul(v_c, u_o), dim=1)  # (B,)\n",
    "        pos_loss = F.logsigmoid(torch.clamp(pos_score, min=-10, max=10))  # (B,)\n",
    "\n",
    "        # Negative sample scores\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2)  # (B, k)\n",
    "        neg_loss = F.logsigmoid(torch.clamp(-neg_score, min=-10, max=10))  # (B, k)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = -torch.sum(pos_loss + torch.sum(neg_loss, dim=1))  # Scalar\n",
    "\n",
    "        # Hint: torch.clamp the input to F.logsigmoid to avoid numerical underflow/overflow\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-25 19:32:54,919 - INFO - Config 1: \n",
      "2024-03-25 19:32:54,921 - INFO - Hyper-parameters:emb_size=50,window_size=1,k=5\n",
      "2024-03-25 19:32:54,922 - INFO - Saved embedding file name:embeddings/50_1_5.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:33:18,403 - INFO - Config 2: \n",
      "2024-03-25 19:33:18,405 - INFO - Hyper-parameters:emb_size=50,window_size=1,k=10\n",
      "2024-03-25 19:33:18,406 - INFO - Saved embedding file name:embeddings/50_1_10.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:33:42,965 - INFO - Config 3: \n",
      "2024-03-25 19:33:42,968 - INFO - Hyper-parameters:emb_size=50,window_size=1,k=15\n",
      "2024-03-25 19:33:42,969 - INFO - Saved embedding file name:embeddings/50_1_15.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:34:13,489 - INFO - Config 4: \n",
      "2024-03-25 19:34:13,490 - INFO - Hyper-parameters:emb_size=50,window_size=3,k=5\n",
      "2024-03-25 19:34:13,491 - INFO - Saved embedding file name:embeddings/50_3_5.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:34:43,669 - INFO - Config 5: \n",
      "2024-03-25 19:34:43,670 - INFO - Hyper-parameters:emb_size=50,window_size=3,k=10\n",
      "2024-03-25 19:34:43,671 - INFO - Saved embedding file name:embeddings/50_3_10.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:35:15,353 - INFO - Config 6: \n",
      "2024-03-25 19:35:15,355 - INFO - Hyper-parameters:emb_size=50,window_size=3,k=15\n",
      "2024-03-25 19:35:15,356 - INFO - Saved embedding file name:embeddings/50_3_15.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:35:45,460 - INFO - Config 7: \n",
      "2024-03-25 19:35:45,460 - INFO - Hyper-parameters:emb_size=50,window_size=5,k=5\n",
      "2024-03-25 19:35:45,461 - INFO - Saved embedding file name:embeddings/50_5_5.txt\n",
      "Total vocabulary: 1352\n",
      "2024-03-25 19:45:25,788 - INFO - Config 8: \n",
      "2024-03-25 19:45:25,791 - INFO - Hyper-parameters:emb_size=50,window_size=5,k=10\n",
      "2024-03-25 19:45:25,792 - INFO - Saved embedding file name:embeddings/50_5_10.txt\n",
      "Total vocabulary: 1352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 71\u001B[0m\n\u001B[1;32m     69\u001B[0m dataloader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(batchify(generated_data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Initialize the corpus and model\u001B[39;00m\n\u001B[0;32m---> 71\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[43mCorpusReader\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlunyu_20chapters.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_count\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m model \u001B[38;5;241m=\u001B[39m SkipGram(vacob_size, emb_size)\n\u001B[1;32m     74\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mSparseAdam(model\u001B[38;5;241m.\u001B[39mparameters(),lr\u001B[38;5;241m=\u001B[39minitial_lr) \u001B[38;5;66;03m# or torch.optim.SparseAdam() or torch.optim.Adam()\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/wsy/NLP/A2/utils.py:21\u001B[0m, in \u001B[0;36mCorpusReader.__init__\u001B[0;34m(self, inputFileName, min_count, lang)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputFileName \u001B[38;5;241m=\u001B[39m inputFileName\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_words(min_count)\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitTableNegatives\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minitTableDiscards()\n",
      "File \u001B[0;32m~/Desktop/wsy/NLP/A2/utils.py:60\u001B[0m, in \u001B[0;36mCorpusReader.initTableNegatives\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m wid, c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(count):\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnegatives \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [wid] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mint\u001B[39m(c)\n\u001B[0;32m---> 60\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnegatives \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnegatives\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mshuffle(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnegatives)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, scheduler, epochs):\n",
    "    # Write your own code for this train function\n",
    "    # You don't need exactly the same arguments\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Determine the device (GPU or CPU)\n",
    "    model.to(device)  # Move the model to the appropriate device\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    print_interval = 1000  # Print loss every 1000 iterations\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        iterations = 0\n",
    "\n",
    "        for i, (center_word, context_word, negative_words) in enumerate(dataloader):\n",
    "            center_word = center_word.to(device)\n",
    "            context_word = context_word.to(device)\n",
    "            negative_words = negative_words.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model(center_word, context_word, negative_words)  # Forward pass\n",
    "\n",
    "            loss.backward()  # Backward pass\n",
    "\n",
    "            dense_params = [param.clone().detach().to(device) for param in model.parameters() if param.requires_grad]\n",
    "            clip_grad_norm_(dense_params, max_norm=5.0)\n",
    "\n",
    "            # clip_grad_norm_(model.parameters(), max_norm=5.0)  # Clip gradients to avoid exploding gradients\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            iterations += 1\n",
    "\n",
    "            if (i + 1) % print_interval == 0:\n",
    "                avg_loss = total_loss / iterations\n",
    "                logger.info(f\"Epoch [{epoch+1}/{epochs}], Iteration [{i+1}/{len(dataloader)}], Loss: {avg_loss}\")\n",
    "\n",
    "        scheduler.step()  # Update learning rate scheduler\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Suggested hyperparameters\n",
    "initial_lr = 0.025\n",
    "batch_size = 16\n",
    "emb_sizes = [50,100]\n",
    "window_sizes = [1,3,5]\n",
    "ks = [5,10,15] # the number of negative samples, change with your own choice for better embedding performance\n",
    "min_count = 1 # because our data is small. If min_count > 1, you should filter out those unknown words from the data in train() function\n",
    "\n",
    "\n",
    "epochs=0\n",
    "vacob_size = len(corpus.id2word)\n",
    "\n",
    "count=0\n",
    "for emb_size in emb_sizes:\n",
    "    for window_size in window_sizes:\n",
    "        for k in ks:\n",
    "            count+=1\n",
    "            logger.info(f\"Config {count}: \")\n",
    "            logger.info(f\"Hyper-parameters:emb_size={emb_size},window_size={window_size},k={k}\")\n",
    "            embedding_file_name=f\"embeddings/{emb_size}_{window_size}_{k}.txt\"\n",
    "            # model_name=f\"{emb_size}_{window_size}_{k}.pth\"\n",
    "            logger.info(f\"Saved embedding file name:embeddings/{emb_size}_{window_size}_{k}.txt\")\n",
    "            generated_data= list(generate_data(list(raw_data), window_size=window_size, k=k, corpus=corpus))\n",
    "            dataloader = list(batchify(generated_data, batch_size=4))\n",
    "            # Initialize the corpus and model\n",
    "            corpus = CorpusReader('lunyu_20chapters.txt', min_count)\n",
    "            model = SkipGram(vacob_size, emb_size)\n",
    "\n",
    "            optimizer = torch.optim.SparseAdam(model.parameters(),lr=initial_lr) # or torch.optim.SparseAdam() or torch.optim.Adam()\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(dataloader)*epochs)  # or torch.optim.lr_scheduler.StepLR()\n",
    "            # scheduler=torch.optim.lr_scheduler.StepLR()\n",
    "\n",
    "            train(model, dataloader, optimizer, scheduler,epochs)\n",
    "\n",
    "            # torch.save(model.state_dict(), model_name)\n",
    "            model.save_embedding(corpus.id2word,file_name=embedding_file_name)\n",
    "\n",
    "\n",
    "\n",
    "### Hints: ###\n",
    "# - If you have cuda-supported GPUs, you can run the training faster by\n",
    "#   `device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")`\n",
    "#   `model.cuda()`\n",
    "#   You also need to move all tensor data to the same device\n",
    "# - If you find Inf or NaN in the loss, you can try to clip the gradient usning `torch.nn.utils.clip_grad_norm_`\n",
    "# - Remember to save the embeddings when training is done\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot and Compare Embeddings\n",
    "\n",
    "Use `sklearn.decomposition.TruncatedSVD` to reduce the dimensionality of the obtained embeddings to 2 and plot the selected words in 2D space.\n",
    "\n",
    "*Hint*:\n",
    "- Obtain the embeddings into a numpy array by `model.emb_v.cpu().data.numpy()`\n",
    "- The word2id dictionary is in `model.word2id`\n",
    "- If you are trying to load from a saved embedding file, you can use the APIs from `gensim`.\n",
    "  - For exmaple, `model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')`\n",
    "  - Check out the documentation for more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings/50_1_15.txt\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'KeyedVectors' object has no attribute 'emb_v'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(file_path)\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mKeyedVectors\u001B[38;5;241m.\u001B[39mload_word2vec_format(file_path)\n\u001B[0;32m---> 12\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43memb_v\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m### END YOUR CODE ###\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Truncated SVD\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m### YOUR CODE HERE ###\u001B[39;00m\n\u001B[1;32m     16\u001B[0m svd \u001B[38;5;241m=\u001B[39m TruncatedSVD(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KeyedVectors' object has no attribute 'emb_v'"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "### YOUR CODE HERE ###\n",
    "folder_path = 'embeddings'  # 指定文件夹路径\n",
    "\n",
    "# 使用 glob 模块匹配文件夹下所有以 .txt 结尾的文件\n",
    "txt_files = glob.glob(os.path.join(folder_path, '*.txt'))\n",
    "\n",
    "# 遍历匹配到的文件列表\n",
    "for file_path in txt_files:\n",
    "    print(file_path)\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(file_path)\n",
    "    embeddings = model.emb_v.cpu().data.numpy()\n",
    "    ### END YOUR CODE ###\n",
    "    # Truncated SVD\n",
    "    ### YOUR CODE HERE ###\n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    embeddings_2d = svd.fit_transform(embeddings)\n",
    "    ### END YOUR CODE ###\n",
    "    # Plot the following words or other words you are interested in\n",
    "    # You better pick those words that look different in the 2D space compared with the LSA vectors\n",
    "    words = ['学', '习', '曰', '子', '人', '仁']\n",
    "    words_pinyin = ['xue', 'xi', 'yue', 'zi', 'ren1', 'ren2']\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        x, y = embeddings_2d[model.word2id[word]]\n",
    "        plt.scatter(x, y, label=word)\n",
    "        plt.annotate(words_pinyin[i], (x, y), textcoords=\"offset points\", xytext=(-5,5), ha='right')\n",
    "\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Word Embeddings - Truncated SVD')\n",
    "    plt.legend()\n",
    "    # Save the figure\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    plt.savefig(f'embeddings/{file_name}.png')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    ### END YOUR CODE ###"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
