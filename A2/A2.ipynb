{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 2. Word2vec Implementation \n",
    "\n",
    "**Total points**: 50\n",
    "\n",
    "Train a word2vec model using the **skip-gram** architecture and **negative sampling**.\n",
    "\n",
    "You should roughtly follow the structure of the notebook. Add additional cells if you feel needed. \n",
    "\n",
    "You can (and you should) re-use the code from *Lab 4 (part 2): Data preparation for implementing word2vec*. \n",
    "\n",
    "Make sure your code is readable and well-structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from utils import CorpusReader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Processing\n",
    "\n",
    "The corpus data is in `lunyu_20chapters.txt`. Use the `CorpusReader` class in `utils.py` to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data in lunyu_20chapters.txt\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "corpus = CorpusReader(inputFileName=\"lunyu_20chapters.txt\", min_count=1)\n",
    "\n",
    "print(corpus.word2id[\"Â≠ê\"])\n",
    "print(corpus.id2word[1])\n",
    "print(len(corpus.id2word))\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the code from lab with necessary modifications\n",
    "\n",
    "def generate_data(words: List[str], window_size: int, k: int, corpus: CorpusReader):\n",
    "    \"\"\" Generate the training data for word2vec skip-gram model\n",
    "    Args:\n",
    "        text: the input text\n",
    "        window_size: the size of the context window\n",
    "        k: the number of negative samples\n",
    "        corpus: the corpus object, providing utilities such as word2id, getNegatives, etc.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    word_ids = [corpus.word2id[word] for word in words]  # Convert the list of words to a list of word ids\n",
    "\n",
    "    for center_index, center_id in enumerate(word_ids):\n",
    "        context_indices = None\n",
    "        # Iterate over the left context words\n",
    "        for i in range(max(0, center_index - window_size), center_index):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "        # Iterate over the right context words\n",
    "        for i in range(center_index + 1, min(center_index + window_size + 1, len(word_ids))):\n",
    "            context_indices=word_ids[i]\n",
    "            negative_samples = corpus.getNegatives(center_id, k)\n",
    "            yield center_id, context_indices, negative_samples\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "def batchify(data: List, batch_size: int):\n",
    "    \"\"\" Group a stream into batches and yield them as torch tensors.\n",
    "    Args:\n",
    "        data: a list of tuples\n",
    "        batch_size: the batch size \n",
    "    Yields:\n",
    "        a tuple of three torch tensors: center, outside, negative\n",
    "    \"\"\"\n",
    "    assert batch_size < len(data) # data should be long enough\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        if i > len(data) - batch_size: # if the last batch is smaller than batch_size, pad it with the first few data\n",
    "            batch = batch + data[:i + batch_size - len(data)]\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        center_batch = []\n",
    "        outside_batch = []\n",
    "        negative_batch = []\n",
    "\n",
    "        for center, outside, negative in batch:\n",
    "            center_batch.append(center)\n",
    "            outside_batch.append(outside)\n",
    "            negative_batch.append(negative)\n",
    "\n",
    "        center_tensor = torch.tensor(center_batch, dtype=torch.long)\n",
    "        outside_tensor = torch.tensor(outside_batch, dtype=torch.long)\n",
    "        negative_tensor = torch.tensor(negative_batch, dtype=torch.long)\n",
    "\n",
    "        yield center_tensor, outside_tensor, negative_tensor\n",
    "\n",
    "        ### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.emb_v = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "        self.emb_u = nn.Embedding(vocab_size, emb_size, sparse=True)\n",
    "\n",
    "        initrange = 1.0 / self.emb_size # some experience passed down from generation to generation\n",
    "        nn.init.uniform_(self.emb_v.weight.data, -initrange, initrange) # same outcome as self.emb_v.weight.data.uniform_(-initrange, initrange)\n",
    "        nn.init.constant_(self.emb_u.weight.data, 0) # same outcome as self.emb_u.weight.data.zero_()\n",
    "\n",
    "    def forward(self, center, outside, negative):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center: the center word indices (B, )\n",
    "            outside: the outside word indices (B, )\n",
    "            negative: the negative word indices (B, k)\n",
    "        \"\"\"\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        \n",
    "        ### YOUR CODE HERE ###\n",
    "        v_c = self.emb_v(center)\n",
    "        u_o = self.emb_u(outside)\n",
    "        u_n = self.emb_u(negative)\n",
    "        #\n",
    "        # ### YOUR CODE HERE ###\n",
    "        # Positive sample score\n",
    "        pos_score = torch.sum(torch.mul(v_c, u_o), dim=1)  # (B,)\n",
    "        pos_loss = F.logsigmoid(torch.clamp(pos_score, min=-10, max=10))  # (B,)\n",
    "\n",
    "        # Negative sample scores\n",
    "        neg_score = torch.bmm(u_n, v_c.unsqueeze(2)).squeeze(2)  # (B, k)\n",
    "        neg_loss = F.logsigmoid(torch.clamp(-neg_score, min=-10, max=10))  # (B, k)\n",
    "\n",
    "        # Combine losses\n",
    "        loss = -torch.sum(pos_loss + torch.sum(neg_loss, dim=1))  # Scalar\n",
    "\n",
    "        # Hint: torch.clamp the input to F.logsigmoid to avoid numerical underflow/overflow\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def save_embedding(self, id2word, file_name):\n",
    "        embedding = self.emb_v.weight.cpu().data.numpy()\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write('%d %d\\n' % (len(id2word), self.emb_size))\n",
    "            for wid, w in id2word.items():\n",
    "                e = ' '.join(map(lambda x: str(x), embedding[wid]))\n",
    "                f.write('%s %s\\n' % (w, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, epochs):\n",
    "    # Write your own code for this train function\n",
    "    # You don't need exactly the same arguments\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    pass\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Suggested hyperparameters\n",
    "initial_lr = 0.025\n",
    "batch_size = 16\n",
    "emb_size = 50\n",
    "window_size = 5\n",
    "k = 10 # the number of negative samples, change with your own choice for better embedding performance\n",
    "min_count = 1 # because our data is small. If min_count > 1, you should filter out those unknown words from the data in train() function\n",
    "optimizer = torch.optim.Adam() # or torch.optim.SparseAdam()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR() # or torch.optim.lr_scheduler.StepLR()\n",
    "\n",
    "# Initialize the corpus and model\n",
    "corpus = CorpusReader('lunyu_20chapters.txt', min_count)\n",
    "model = SkipGram(corpus.vocab_size, emb_size)\n",
    "\n",
    "\n",
    "### Hints: ###\n",
    "# - If you have cuda-supported GPUs, you can run the training faster by\n",
    "#   `device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")`\n",
    "#   `model.cuda()`\n",
    "#   You also need to move all tensor data to the same device\n",
    "# - If you find Inf or NaN in the loss, you can try to clip the gradient usning `torch.nn.utils.clip_grad_norm_`\n",
    "# - Remember to save the embeddings when training is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plot and Compare Embeddings\n",
    "\n",
    "Use `sklearn.decomposition.TruncatedSVD` to reduce the dimensionality of the obtained embeddings to 2 and plot the selected words in 2D space.\n",
    "\n",
    "*Hint*:\n",
    "- Obtain the embeddings into a numpy array by `model.emb_v.cpu().data.numpy()`\n",
    "- The word2id dictionary is in `model.word2id`\n",
    "- If you are trying to load from a saved embedding file, you can use the APIs from `gensim`.\n",
    "  - For exmaple, `model = gensim.models.KeyedVectors.load_word2vec_format('path/to/file')`\n",
    "  - Check out the documentation for more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the following words or other words you are interested in\n",
    "# You better pick those words that look different in the 2D space compared with the LSA vectors\n",
    "words = ['Â≠¶', '‰π†', 'Êõ∞', 'Â≠ê', '‰∫∫', '‰ªÅ']\n",
    "words_pinyin = ['xue', 'xi', 'yue', 'zi', 'ren1', 'ren2']\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "pass\n",
    "### END YOUR CODE ###"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
