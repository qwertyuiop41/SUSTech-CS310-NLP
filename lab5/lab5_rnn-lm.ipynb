{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 5 (part 1): Data preparation for implementing an RNN Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process input sequences of variable lengths\n",
    "\n",
    "When training RNN (LSTM or vanilla-RNN), it is difficult to batch the variable length sequences. \n",
    "\n",
    "For example: if the length of sequences in a size 8 batch is `[4,6,8,5,4,3,7,8]`, you will pad all the sequences and that will result in 8 sequences of length 8. You would end up doing 64 computations (8x8), but you needed to do only 45 computations. \n",
    "\n",
    "PyTorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps (see example below) and other contains the size of each sequence the batch size at each step. \n",
    "\n",
    "This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step. \n",
    "\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = [torch.tensor([1,2,3]), torch.tensor([3,4])] # Sequences\n",
    "seq_lens = torch.tensor([3,2]) # Actual lengths of sequences\n",
    "\n",
    "# First, pad the sequences to the same length\n",
    "padded_seqs = nn.utils.rnn.pad_sequence(seqs, batch_first=True)\n",
    "\n",
    "# Then pack them all before passing to the RNN\n",
    "packed_seqs = nn.utils.rnn.pack_padded_sequence(padded_seqs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "# Print intermediate results\n",
    "print('original sequences:', seqs)\n",
    "print('padded sequences:', padded_seqs)\n",
    "print('packed sequences:', packed_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "- Default padding ID is 0\n",
    "- The padded sequence is of shape `batch_size x max_length`. Assuming it is word ids, then after it is embedded, it will be of shape `batch_size x max_length x embedding_size`. \n",
    "- Here, `max_length` is the length of the longest sequence in the batch. \n",
    "- We set `enforce_sorted` to `False` in `pack_padded_sequence` because we are not sorting the sequences by length. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will first embed the padded sequence (integer word ids) and then pack the embedded sequence. \n",
    "\n",
    "It is the packed embedded sequence that we pass to RNN. It will internally unpack the sequences and compute only the necessary time steps. \n",
    "\n",
    "To examine the output, you need to unpack it, which is a reverse process of packing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(5, 10)\n",
    "rnn = nn.RNN(10, 20, batch_first=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    padded_embs = embedding(padded_seqs)\n",
    "    packed_embs = nn.utils.rnn.pack_padded_sequence(padded_embs, seq_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "    out_packed, _ = rnn(packed_embs)\n",
    "    out_unpacked, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "\n",
    "print('padded emb dim:', padded_embs.size())\n",
    "print('packed output dim:', out_packed.data.size())\n",
    "print('unpacked output', out_unpacked.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "- `pad_packed_sequence` does the reverse of `pack_padded_sequence`.\n",
    "- the unpacked output is of shape `batch_size x max_length x hidden_size`, in which the first dimensions match the shape of the padded input sequences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Practice Padding and Packing\n",
    "\n",
    "First, Read all text data and build the vocabulary. \n",
    "\n",
    "Note that this time the ids for actual words will start from 1, as 0 will be used for padding, i.e., the special token '[PAD]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'lunyu_20chapters.txt'\n",
    "\n",
    "# You can use the code from previous lab or rewrite it\n",
    "# Hint: you can comment out the `self.initTableNegatives()` in `__init__` method\n",
    "from utils import CorpusReader\n",
    "corpus = CorpusReader(inputFileName=input_file, min_count=1)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Modify word2id to make 0 as the padding token '[PAD]', and increase the index of all other words by 1\n",
    "# Modify the id2word list to make the first word '[PAD]' as well\n",
    "# Hint: Both word2id and id2word in utils.CorpusReader are dict objects\n",
    "word2id: dict = None\n",
    "id2word: dict = None\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('id2word:', sorted(list(id2word.items()), key=lambda x: x[0])[:5])\n",
    "print('word2id:', sorted(list(word2id.items()), key=lambda x: x[1])[:5])\n",
    "\n",
    "# You should expect to see:\n",
    "# id2word: [(0, '[PAD]'), (1, '，'), (2, '子'), (3, '。'), (4, '：')]\n",
    "# word2id: [('[PAD]', 0), ('，', 1), ('子', 2), ('。', 3), ('：', 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the first 16 lines of text, and convert them into integer sequences (`torch.Long`) of variable lengths. \n",
    "\n",
    "Then, follow the steps of `pad -> embed -> pack` to obtain the packed embedded sequence. \n",
    "\n",
    "Pass it to the RNN and then unpack the output.\n",
    "\n",
    "*Hint*:\n",
    "- You need to define the `embedding_lunyu` as an `nn.Embedding` object, with the correct vocabulary size and **embedding size of 50**.\n",
    "- Create the `rnn_lunyu` as an `nn.RNN` object, with the correct input size and **hidden size of 100**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "embedding_lunyu = None\n",
    "rnn_lunyu = None\n",
    "\n",
    "seq_ids = None\n",
    "seq_lens = None\n",
    "seq_ids_padded = None\n",
    "\n",
    "seq_embs = None\n",
    "seq_embs_packed = None\n",
    "\n",
    "out_packed, _ = None\n",
    "out_unpacked, _ = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('seq_ids_padded:', seq_ids_padded.size())\n",
    "print('seq_embs:', seq_embs.size())\n",
    "print('out_unpacked:', out_unpacked.size())\n",
    "\n",
    "# You should expect to see:\n",
    "# seq_ids_padded: torch.Size([16, 85])\n",
    "# seq_embs: torch.Size([16, 85, 50])\n",
    "# out_unpacked: torch.Size([16, 85, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, map the output of the RNN to the vocabulary size.\n",
    "\n",
    "*Hint*:\n",
    "- Define a linear layer `fc` with the correct input (hidden size of RNN) and output size (vocabulary size).\n",
    "- The output of `fc` will be of shape `batch_size x max_length x vocab_size`, which we call `logits`.\n",
    "- `logits` are not normalized, so you need to apply `F.log_softmax` to get the log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "fc = None\n",
    "logits = None\n",
    "log_probs = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('logits:', logits.size())\n",
    "print('log_probs:', log_probs.size())\n",
    "\n",
    "# You should expect to see:\n",
    "# logits: torch.Size([16, 85, 1353])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Prepare Target Labels\n",
    "\n",
    "Prepare the target labels for the RNN. The target labels are the same as the input sequences, but shifted by one time step.\n",
    "\n",
    "For example, if the input sequences is `[[1, 2, 3], [3, 4, 0]]`, the target labels should be `[[2, 3, 0], [4, 0, 0]]`, where 0 is the padding ID.\n",
    "\n",
    "In this practice, you need to prepare the target labels for first 16 lines, i.e., `seq_ids_padded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "targets_padded = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('targets_padded:', targets_padded.size())\n",
    "print('last column of targets_padded:', targets_padded[:, -1])\n",
    "\n",
    "# You should expect to see:\n",
    "# targets_padded: torch.Size([16, 85])\n",
    "# last column of targets_padded: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Compute Perplexity\n",
    "\n",
    "In order to compute the perplexity, we first need to compute the negative log probabilities.\n",
    "\n",
    "This can be accomplished by using the `nn.NLLLoss` function, which takes the `log_probs` and the `target_labels` as input, and the negative log probability (cross-entropy) loss, averaged over all the non-padding tokens: $-\\sum \\log(p)$\n",
    "\n",
    "However, the default output of `nn.NLLLoss` is reduced to the average over all the tokens, including the padding tokens. We need to exclude the padding token by setting the `ignore_index` argument to the padding ID, i.e., 0. Also, set the `reduction` argument to `'none'` to get the loss for each non-padding token.\n",
    "\n",
    "Finally, compute the perplexity by exponentiating the average loss per sequence.\n",
    "\n",
    "See the documentation here: https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss(ignore_index=0, reduction='none')\n",
    "\n",
    "### START YOUR CODE ###\n",
    "# Calculate the loss\n",
    "with torch.no_grad():\n",
    "    loss = None\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test result\n",
    "print('loss:', loss.size())\n",
    "\n",
    "# You shoul expect to see:\n",
    "# loss: torch.Size([1360])\n",
    "# Here, 1360 = 16 * 85, i.e., the total number of tokens in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "In `__init__` method, initialize `word_embeddings` with a pretrained embedding weight matrix loaded. For example, the one obtained from previous assignment (saved word2vec file). \n",
    "\n",
    "`nn.Embedding` has a method `from_pretrained` that takes the pretrained weight matrix (a `numpy.ndarray` object) to initialize its weight.\n",
    "\n",
    "`forward` method takes the word id sequences and sequence lengths as inputs, and return the logits or log probabilities from RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embedding = None\n",
    "        self.rnn = None\n",
    "        self.fc = None\n",
    "        pass\n",
    "\n",
    "    def forward(self, seq, seq_lens):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Generation\n",
    "\n",
    "After training the RNN, we can use it to generate sentences. \n",
    "\n",
    "The process is as follows:\n",
    "- Start with a special token or a sequence of tokens, e.g., [\"子\", \"曰\"]\n",
    "- Pass the sequence to the RNN, and sample the next word from the output probability distribution of the last time step. We use greedy search here, i.e., select the word with the highest probability.\n",
    "- Append the sampled word to the sequence, and repeat the process until a special token, e.g., \"。\", is sampled; Or until it reaches the maximum length of generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
