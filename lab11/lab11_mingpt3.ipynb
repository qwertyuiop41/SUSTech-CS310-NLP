{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 11: Pretraining MinGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task today is to work on top of the Andrej Karpathy’s `minGPT` project (original repo URL: https://github.com/karpathy/minGPT), and define the dataset and training code for pretraining a **character-level** language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.utils import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Define the `CharDataset` class\n",
    "\n",
    "`CharDataset` is a subclass of `torch.utils.data.Dataset` that reads a long string of text and returns an iterable sequence of character ID chunks.  \n",
    "\n",
    "The length of the sequence is determined in `__len__` method. The `__getitem__` method takes an integer `idx` as input and returns the `idx`-th **chunk** of character IDs. \n",
    "\n",
    "The size of this chunk is determined by the `block_size` parameter, i.e., the maximum context length for language modeling. The returned `x` and `y` are the character IDs in one chunk, but `y` is shifted by one character.\n",
    "\n",
    "For example, if the input text is \"hello, world!\", and `block_size=4`, then the first chunk will be `x=\"hell\"` and `y=\"ello\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ### START YOUR CODE ###\n",
    "        # # grab a chunk of (block_size + 1) characters from the data\n",
    "        # chunk = None\n",
    "        # # encode every character to an integer\n",
    "        # ids = None\n",
    "        #\n",
    "        # # Convert to tensor\n",
    "        # x = None\n",
    "        # y = None\n",
    "        # ### END YOUR CODE ###\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "\n",
    "        # encode every character to an integer\n",
    "        ids = [self.stoi[ch] for ch in chunk]\n",
    "\n",
    "        # Convert to tensor\n",
    "        x = torch.tensor(ids[:-1])\n",
    "        y = torch.tensor(ids[1:])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 12 characters, 9 unique.\n",
      "chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sample_data = 'hello world!'\n",
    "sample_dataset = CharDataset(sample_data, block_size=4)\n",
    "print('chunk 0:', sample_dataset[0])\n",
    "\n",
    "# You should see the expected output as follows:\n",
    "# data has 12 characters, 9 unique.\n",
    "# chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Use the provided `trainer`\n",
    "\n",
    "Firstly, load some more serious data, such as some sampled text from Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "text = open('input.txt', 'r').read()\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, initialize the `GPT` model, with proper hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(\n",
    "    train_dataset.vocab_size, \n",
    "    train_dataset.block_size,\n",
    "    n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialie a `Trainer` and start training! It may take a long time to finish one epoch, but you can stop it at any time.\n",
    "\n",
    "Notes:\n",
    "- The `Trainer` class supports training in multiple processes, but in order to make it work in Jupyter notebook, we set `num_workers=0` and run in a single process.\n",
    "- `ckpt_path` specifies the path to save the model. By default, it saves the model every epoch. Set it to `None` if you don't want to save the model.\n",
    "- No test data is specified, so the thrid argument of `Trainer` is set to `None`.\n",
    "- Explore other parameters as you like in `trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 17426: train loss 1.71350. lr 3.000169e-04: 100%|██████████| 17427/17427 [39:26:01<00:00,  8.15s/it]   \n",
      "epoch 2 iter 17426: train loss 1.55733. lr 6.000000e-05: 100%|██████████| 17427/17427 [39:20:22<00:00,  8.13s/it]   \n"
     ]
    }
   ],
   "source": [
    "trainer_config = TrainerConfig(max_epochs=2, batch_size=64, \n",
    "                      learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      ckpt_path='mingpt_ckpt.pth', num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, None, trainer_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually save the model by calling `trainer.save_checkpoint()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39msave_checkpoint()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model saved to `mingpt_ckpt.pth`, though it is not fully trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Sample from the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minGPT` provides a `sample` method to generate completions based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO God, O God!\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([train_dataset\u001B[38;5;241m.\u001B[39mstoi[s] \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m prompt], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)[\u001B[38;5;28;01mNone\u001B[39;00m,\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]\u001B[38;5;241m.\u001B[39mto(\u001B[43mtrainer\u001B[49m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m      4\u001B[0m y \u001B[38;5;241m=\u001B[39m sample(model, x, \u001B[38;5;241m100\u001B[39m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m, sample\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, top_k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      6\u001B[0m completion \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([train_dataset\u001B[38;5;241m.\u001B[39mitos[\u001B[38;5;28mint\u001B[39m(i)] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m y])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it does not read like Shakespeare at all because your model is not trained enough. \n",
    "\n",
    "What you can do is to load the model trained by somebody else. Download `mingpt_model.pth` from the course website, and load the model weight by `torch.load`. \n",
    "\n",
    "Note that the provided model was trained on GPU, so you need to specify `map_location=torch.device('cpu')` loading it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# pretrained_weight = None\n",
    "# Load the pretrained weights\n",
    "pretrained_weight = torch.load('mingpt_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print(type(pretrained_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loaded `pretrained_weight` is merely a dictionary of parameters and not a `GPT` model instance yet. \n",
    "\n",
    "So next, you need to instantiate a new `GPT` model, and load the weights using the `model.load_state_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### START YOUR CODE ###\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# model_pretrained = None\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Instantiate a new GPT model and load the weights\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m model_pretrained \u001B[38;5;241m=\u001B[39m \u001B[43mGPT\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m model_pretrained\u001B[38;5;241m.\u001B[39mload_state_dict(pretrained_weight)\n",
      "\u001B[0;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'vocab_size'"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# model_pretrained = None\n",
    "# Instantiate a new GPT model and load the weights\n",
    "model_pretrained = GPT(model_config)\n",
    "model_pretrained.load_state_dict(pretrained_weight)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the generation code to see if there is any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model_pretrained, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text should read more \"Shakespearean\" than before.\n",
    "\n",
    "Congratulations! You have successfully completed the lab. There are several things you can checkout further:\n",
    "- `minGPT` is no longer actively maintained, but its successor `nanoGPT` is there! Check it out at: https://github.com/karpathy/nanoGPT\n",
    "- As the author claims, `nanoGPT` \"prioritizes teeth over education\", which means you can train your own version of GPT-2 level models, given data and GPU cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
