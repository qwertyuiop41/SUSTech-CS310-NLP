{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 11: Pretraining MinGPT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task today is to work on top of the Andrej Karpathyâ€™s `minGPT` project (original repo URL: https://github.com/karpathy/minGPT), and define the dataset and training code for pretraining a **character-level** language model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from mingpt.model import GPT, GPTConfig\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.utils import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Define the `CharDataset` class\n",
    "\n",
    "`CharDataset` is a subclass of `torch.utils.data.Dataset` that reads a long string of text and returns an iterable sequence of character ID chunks.  \n",
    "\n",
    "The length of the sequence is determined in `__len__` method. The `__getitem__` method takes an integer `idx` as input and returns the `idx`-th **chunk** of character IDs. \n",
    "\n",
    "The size of this chunk is determined by the `block_size` parameter, i.e., the maximum context length for language modeling. The returned `x` and `y` are the character IDs in one chunk, but `y` is shifted by one character.\n",
    "\n",
    "For example, if the input text is \"hello, world!\", and `block_size=4`, then the first chunk will be `x=\"hell\"` and `y=\"ello\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # ### START YOUR CODE ###\n",
    "        # # grab a chunk of (block_size + 1) characters from the data\n",
    "        # chunk = None\n",
    "        # # encode every character to an integer\n",
    "        # ids = None\n",
    "        #\n",
    "        # # Convert to tensor\n",
    "        # x = None\n",
    "        # y = None\n",
    "        # ### END YOUR CODE ###\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "\n",
    "        # encode every character to an integer\n",
    "        ids = [self.stoi[ch] for ch in chunk]\n",
    "\n",
    "        # Convert to tensor\n",
    "        x = torch.tensor(ids[:-1])\n",
    "        y = torch.tensor(ids[1:])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 12 characters, 9 unique.\n",
      "chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sample_data = 'hello world!'\n",
    "sample_dataset = CharDataset(sample_data, block_size=4)\n",
    "print('chunk 0:', sample_dataset[0])\n",
    "\n",
    "# You should see the expected output as follows:\n",
    "# data has 12 characters, 9 unique.\n",
    "# chunk 0: (tensor([4, 3, 5, 5]), tensor([3, 5, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Use the provided `trainer`\n",
    "\n",
    "Firstly, load some more serious data, such as some sampled text from Shakespeare's works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "block_size = 128\n",
    "text = open('input.txt', 'r').read()\n",
    "train_dataset = CharDataset(text, block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, initialize the `GPT` model, with proper hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = GPTConfig(\n",
    "    train_dataset.vocab_size, \n",
    "    train_dataset.block_size,\n",
    "    n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's initialie a `Trainer` and start training! It may take a long time to finish one epoch, but you can stop it at any time.\n",
    "\n",
    "Notes:\n",
    "- The `Trainer` class supports training in multiple processes, but in order to make it work in Jupyter notebook, we set `num_workers=0` and run in a single process.\n",
    "- `ckpt_path` specifies the path to save the model. By default, it saves the model every epoch. Set it to `None` if you don't want to save the model.\n",
    "- No test data is specified, so the thrid argument of `Trainer` is set to `None`.\n",
    "- Explore other parameters as you like in `trainer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 54: train loss 2.85674. lr 5.999965e-04:   0%|          | 55/17427 [09:22<49:22:14, 10.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer_config \u001B[38;5;241m=\u001B[39m TrainerConfig(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m, \n\u001B[1;32m      2\u001B[0m                       learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6e-4\u001B[39m, lr_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[1;32m      3\u001B[0m                       warmup_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m20\u001B[39m, final_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlen\u001B[39m(train_dataset)\u001B[38;5;241m*\u001B[39mblock_size,\n\u001B[1;32m      4\u001B[0m                       ckpt_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmingpt_ckpt2.pth\u001B[39m\u001B[38;5;124m'\u001B[39m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      5\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(model, train_dataset, \u001B[38;5;28;01mNone\u001B[39;00m, trainer_config)\n\u001B[0;32m----> 6\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/wsy/NLP/lab11/mingpt/trainer.py:127\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;66;03m# counter used for learning rate decay\u001B[39;00m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mmax_epochs):\n\u001B[0;32m--> 127\u001B[0m     \u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m         run_epoch(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/wsy/NLP/lab11/mingpt/trainer.py:98\u001B[0m, in \u001B[0;36mTrainer.train.<locals>.run_epoch\u001B[0;34m(split)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_train:\n\u001B[1;32m     95\u001B[0m \n\u001B[1;32m     96\u001B[0m     \u001B[38;5;66;03m# backprop and update the parameters\u001B[39;00m\n\u001B[1;32m     97\u001B[0m     model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 98\u001B[0m     \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     99\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), config\u001B[38;5;241m.\u001B[39mgrad_norm_clip)\n\u001B[1;32m    100\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "trainer_config = TrainerConfig(max_epochs=2, batch_size=64, \n",
    "                      learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      ckpt_path='mingpt_ckpt.pth', num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, None, trainer_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also manually save the model by calling `trainer.save_checkpoint()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model saved to `mingpt_ckpt.pth`, though it is not fully trained yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Sample from the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minGPT` provides a `sample` method to generate completions based on a given prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! sadetasemars a sodedot ce aredoreu a te er merosso radeson arorarss,\n",
      "roerrerhe s edheso thi isrt ir\n"
     ]
    }
   ],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course it does not read like Shakespeare at all because your model is not trained enough. \n",
    "\n",
    "What you can do is to load the model trained by somebody else. Download `mingpt_model.pth` from the course website, and load the model weight by `torch.load`. \n",
    "\n",
    "Note that the provided model was trained on GPU, so you need to specify `map_location=torch.device('cpu')` loading it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# pretrained_weight = None\n",
    "# Load the pretrained weights\n",
    "pretrained_weight = torch.load('mingpt_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print(type(pretrained_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loaded `pretrained_weight` is merely a dictionary of parameters and not a `GPT` model instance yet. \n",
    "\n",
    "So next, you need to instantiate a new `GPT` model, and load the weights using the `model.load_state_dict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# model_pretrained = None\n",
    "# Instantiate a new GPT model and load the weights\n",
    "model_pretrained = GPT(model_config)\n",
    "model_pretrained.load_state_dict(pretrained_weight)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, re-run the generation code to see if there is any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! think that\n",
      "And time make will stall thee on, o' madam'd make failst.\n",
      "\n",
      "PLARIANSTIAN:\n",
      "Has this cowas \n"
     ]
    }
   ],
   "source": [
    "prompt = \"O God, O God!\"\n",
    "\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in prompt], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model_pretrained, x, 100, temperature=1.0, sample=True, top_k=10)[0]\n",
    "\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text should read more \"Shakespearean\" than before.\n",
    "\n",
    "Congratulations! You have successfully completed the lab. There are several things you can checkout further:\n",
    "- `minGPT` is no longer actively maintained, but its successor `nanoGPT` is there! Check it out at: https://github.com/karpathy/nanoGPT\n",
    "- As the author claims, `nanoGPT` \"prioritizes teeth over education\", which means you can train your own version of GPT-2 level models, given data and GPU cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
