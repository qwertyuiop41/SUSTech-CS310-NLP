{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 7: Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dep_utils import conll_reader, DependencyTree\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Dependency-Annotated Data\n",
    "\n",
    "The data used in this lab are listed as follows:\n",
    "- data/train.conll - Training data.  ~40k sentences\n",
    "- data/dev.conll - Development data.  ~1.7k sentences. Used for observing loss and accuracy during training, and for tuning hyperparameters.\n",
    "- data/test.conll - Test data.  ~2.4k sentences. Used for evaluating the final model.\n",
    "\n",
    "The data are from a split of the WSJ part of the Penn Treebank.\n",
    "\n",
    "The data are in CoNLL-X  format (CoNLL: Conference on Computational Natural Language learning) Each sentences corresponds to a number of lines, one per word. Sentences are separated with a blank line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train.conll:\n",
      "39832 trees read.\n",
      "In dev.conll:\n",
      "1700 trees read.\n",
      "In test.conll:\n",
      "2416 trees read.\n"
     ]
    }
   ],
   "source": [
    "print('In train.conll:')\n",
    "with open('data/train.conll') as f:\n",
    "    train_trees = list(conll_reader(f))\n",
    "print(f'{len(train_trees)} trees read.')\n",
    "\n",
    "print('In dev.conll:')\n",
    "with open('data/dev.conll') as f:\n",
    "    dev_trees = list(conll_reader(f))\n",
    "print(f'{len(dev_trees)} trees read.')\n",
    "\n",
    "print('In test.conll:')\n",
    "with open('data/test.conll') as f:\n",
    "    test_trees = list(conll_reader(f))\n",
    "print(f'{len(test_trees)} trees read.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some processed sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tThe\t_\t_\tDT\t_\t2\tdet\t_\t_\n",
      "2\tbill\t_\t_\tNN\t_\t3\tnsubj\t_\t_\n",
      "3\tintends\t_\t_\tVBZ\t_\t0\troot\t_\t_\n",
      "4\tto\t_\t_\tTO\t_\t5\tmark\t_\t_\n",
      "5\trestrict\t_\t_\tVB\t_\t3\txcomp\t_\t_\n",
      "6\tthe\t_\t_\tDT\t_\t7\tdet\t_\t_\n",
      "7\tRTC\t_\t_\tNNP\t_\t5\tdobj\t_\t_\n",
      "8\tto\t_\t_\tTO\t_\t10\tcase\t_\t_\n",
      "9\tTreasury\t_\t_\tNNP\t_\t10\tcompound\t_\t_\n",
      "10\tborrowings\t_\t_\tNNS\t_\t5\tnmod\t_\t_\n",
      "11\tonly\t_\t_\tRB\t_\t10\tadvmod\t_\t_\n",
      "12\t,\t_\t_\t,\t_\t3\tpunct\t_\t_\n",
      "13\tunless\t_\t_\tIN\t_\t16\tmark\t_\t_\n",
      "14\tthe\t_\t_\tDT\t_\t15\tdet\t_\t_\n",
      "15\tagency\t_\t_\tNN\t_\t16\tnsubj\t_\t_\n",
      "16\treceives\t_\t_\tVBZ\t_\t3\tadvcl\t_\t_\n",
      "17\tspecific\t_\t_\tJJ\t_\t19\tamod\t_\t_\n",
      "18\tcongressional\t_\t_\tJJ\t_\t19\tamod\t_\t_\n",
      "19\tauthorization\t_\t_\tNN\t_\t16\tdobj\t_\t_\n",
      "20\t.\t_\t_\t.\t_\t3\tpunct\t_\t_\n"
     ]
    }
   ],
   "source": [
    "tree = dev_trees[2]\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell prints the 3rd sentence in dev.conll, which consists of 20 words (including punctuations) in 20 lines.\n",
    "\n",
    "Each line contains fields, seperated by a single tab symbol, as follows:\n",
    "\n",
    "- Word ID: Word index, integer starting from 1 for each new sentence.\n",
    "- Word form: The word itself.\n",
    "- Lemma: Unused (represented as an underscore \"_\").\n",
    "- Universal POS: Unused (represented as an underscore \"_\").\n",
    "- POS: Part of speech tag.\n",
    "- Features: Unused (represented as an underscore \"_\").\n",
    "- Word ID of the **head** of the current word.\n",
    "- Dependency relation: The dependency relation between the head and the current word.\n",
    "- deps: Unused (represented as an underscore \"_\").\n",
    "- misc: Unused (represented as an underscore \"_\").\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Statistics of Dependency Relations\n",
    "\n",
    "Study the code of `DependencyTree` and `DependencyEdge` classes in  `utils.py`, and then count the number of unique dependency relations in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number fo unique relations: 39\n",
      "{'det:predet', 'discourse', 'advmod', 'compound', 'cop', 'punct', 'xcomp', 'nummod', 'aux', 'det', 'root', 'nmod', 'cc:preconj', 'case', 'mark', 'parataxis', 'dep', 'mwe', 'csubjpass', 'iobj', 'amod', 'expl', 'auxpass', 'csubj', 'acl:relcl', 'advcl', 'dobj', 'nsubjpass', 'compound:prt', 'nsubj', 'cc', 'appos', 'neg', 'nmod:npmod', 'acl', 'nmod:poss', 'ccomp', 'nmod:tmod', 'conj'}\n",
      "Number of occurrences of ROOT: 43948\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "rel_counter = Counter()\n",
    "\n",
    "### START YOUR CODE ###\n",
    "for tree in train_trees:\n",
    "    for deprel in tree.deprels.values():\n",
    "        rel_counter[deprel.deprel] += 1\n",
    "for tree in dev_trees:\n",
    "    for deprel in tree.deprels.values():\n",
    "        rel_counter[deprel.deprel] += 1\n",
    "for tree in test_trees:\n",
    "    for deprel in tree.deprels.values():\n",
    "        rel_counter[deprel.deprel] += 1\n",
    "\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test results\n",
    "print('Total number fo unique relations:', len(rel_counter))\n",
    "print(set(rel_counter.keys()))\n",
    "print('Number of occurrences of ROOT:', rel_counter['root'])\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# Total number fo unique relations: 39\n",
    "# {'nummod', 'root', 'nmod:tmod', 'nmod', 'punct', 'expl', 'auxpass', 'neg', 'nsubjpass', 'appos' ...\n",
    "# Number of occurrences of ROOT: 43948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Generate Training Data\n",
    "\n",
    "Study the code of the following `State` class\n",
    "\n",
    "**Note**:\n",
    "- The `State` class consists of `stack`, `buffer`, and `deps` as its members\n",
    "- `stack` and `buffer` are lists of word IDs (integers)\n",
    "  - The top of stack is `stack[-1]`\n",
    "  - The front of buffer is `buffer[-1]`\n",
    "- `deps` represents the currently found dependencies\n",
    "  - It is a list of `(parent, child, relation)` triples, where `parent` and `child` are integer IDs and `relation` is a string (the dependency label).\n",
    "- The `shift` methods moves the front of the buffer to the top of the stack\n",
    "- The `left_arc` method adds a head-dependent relation between the top two words on stack: $s_1 \\rightarrow s_2$. Here $s_1$ is `stack[-1]`.\n",
    "- The `right_arc` method adds a head-dependent relation between the top two words on stack: $s_2 \\rightarrow s_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(object):\n",
    "    def __init__(self, sentence):\n",
    "        self.stack = []\n",
    "        self.buffer = []\n",
    "        if sentence:\n",
    "            self.buffer = list(reversed(sentence))\n",
    "        self.deps = set()\n",
    "\n",
    "    def shift(self):\n",
    "        ### START YOUR CODE ###\n",
    "        # print(\"shift\")\n",
    "        if self.buffer:\n",
    "            buffer_out=self.buffer.pop(-1)\n",
    "            self.stack.append(buffer_out)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def left_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop(-2)\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def right_arc(self, label: str):\n",
    "        assert len(self.stack) >= 2\n",
    "        ### START YOUR CODE ###\n",
    "        dependent = self.stack.pop()\n",
    "        head = self.stack[-1]\n",
    "        self.deps.add((head, dependent, label))\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"({},{},{})\".format(self.stack, self.buffer, self.deps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2 shifts: ([0, 1],[4, 3, 2],set())\n",
      "before right-arc: ([0, 1, 2],[4, 3],set())\n",
      "after right-arc: ([0, 1],[4, 3],{(1, 2, 'label1')})\n",
      "before left-arc: ([0, 1, 3, 4],[],{(1, 2, 'label1')})\n",
      "after left-arc: ([0, 1, 4],[],{(4, 3, 'label2'), (1, 2, 'label1')})\n"
     ]
    }
   ],
   "source": [
    "# Test results\n",
    "state = State([0,1,2,3,4])\n",
    "state.shift()\n",
    "state.shift()\n",
    "print('after 2 shifts:', state)\n",
    "\n",
    "state.shift()\n",
    "print('before right-arc:', state)\n",
    "state.right_arc('label1')\n",
    "print('after right-arc:', state)\n",
    "\n",
    "state.shift()\n",
    "state.shift()\n",
    "print('before left-arc:', state)\n",
    "state.left_arc('label2')\n",
    "print('after left-arc:', state)\n",
    "\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# after 2 shifts: ([0, 1],[4, 3, 2],set())\n",
    "# before right-arc: ([0, 1, 2],[4, 3],set())\n",
    "# after right-arc: ([0, 1],[4, 3],{(1, 2, 'label1')})\n",
    "# before left-arc: ([0, 1, 3, 4],[],{(1, 2, 'label1')})\n",
    "# after left-arc: ([0, 1, 4],[],{(1, 2, 'label1'), (4, 3, 'label2')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the folliwing `get_training_instances` function, so that it can generate the training data instances from a given dependency tree.\n",
    "\n",
    "The return type of this function is a list of two-elements tuples:\n",
    "- Tuple[0] is a `State` object, deepcopied from the initial state\n",
    "- Tuple[1] is a a tuple of `(action, relation)` where `action` is from {\"shift\", \"left_arc\", \"right_arc\"} and `relation` is the specific dependency relation.\n",
    "\n",
    "The transition action is decided in the `if ... elif ... else` block, which corresponds to the following three cases:\n",
    "- If $s_1 \\rightarrow s_2$ exists in `deprels`, then `left_arc` is performed.\n",
    "- If $s_2 \\rightarrow s_1$ exists in `deprels`, **AND** all rules with $s_1$ as the head have already been assigned, then `right_arc` is performed.\n",
    "- Perform `shift` otherwise.\n",
    "\n",
    "Note that we use the dictionary `childcount` to count the number of relations with each word as the head. Each time after a `left_arc` or `right_arc` is performed, the corresponding count is decreased by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootDummy(object):\n",
    "    def __init__(self):\n",
    "        self.head = None\n",
    "        self.id = 0\n",
    "        self.deprel = None\n",
    "    def __repr__(self):\n",
    "        return \"<ROOT>\"\n",
    "\n",
    "\n",
    "def get_training_instances(dep_tree) -> List[Tuple[State, Tuple[str, str]]]:\n",
    "    deprels = dep_tree.deprels\n",
    "\n",
    "    word_ids = list(deprels.keys())\n",
    "\n",
    "    state = State(word_ids)\n",
    "    state.stack.append(0) # ROOT\n",
    "\n",
    "    childcount = defaultdict(int)\n",
    "    for _, rel in deprels.items():\n",
    "        childcount[rel.head] += 1\n",
    "\n",
    "    seq = []\n",
    "    while len(state.buffer) > 0 or len(state.stack) > 1:\n",
    "\n",
    "        if state.stack[-1] == 0:\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "            continue\n",
    "        \n",
    "        stack_top1 = deprels[state.stack[-1]]\n",
    "        if state.stack[-2] == 0:\n",
    "            stack_top2 = RootDummy()\n",
    "        else:\n",
    "            stack_top2 = deprels[state.stack[-2]]\n",
    "\n",
    "\n",
    "\n",
    "        # Decide transition action\n",
    "        ### START YOUR CODE ###\n",
    "        # if None: # Left-Arc\n",
    "        #     pass\n",
    "        # elif None: # Right-Arc\n",
    "        #     pass\n",
    "        # else: # Shift\n",
    "        #     pass\n",
    "        if stack_top2.head == stack_top1.id and childcount[stack_top2.id]==0:\n",
    "            # Left-Arc\n",
    "            relation = stack_top2.deprel\n",
    "            seq.append((copy.deepcopy(state), (\"left_arc\", relation)))\n",
    "            state.left_arc(relation)\n",
    "            childcount[stack_top2.id] -= 1\n",
    "            childcount[stack_top1.id] -= 1\n",
    "        elif stack_top1.head == stack_top2.id and childcount[stack_top1.id]==0:\n",
    "            # Right-Arc\n",
    "            relation = stack_top1.deprel\n",
    "            seq.append((copy.deepcopy(state), (\"right_arc\", relation)))\n",
    "            state.right_arc(relation)\n",
    "            childcount[stack_top1.id] -= 1\n",
    "            childcount[stack_top2.id] -= 1\n",
    "        else:\n",
    "            # Shift\n",
    "            seq.append((copy.deepcopy(state), (\"shift\", None)))\n",
    "            state.shift()\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    seq.append((copy.deepcopy(state), (\"done\", None)))\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with a toy tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tBook\t_\t_\tVERB\t_\t0\troot\t_\t_\n",
      "2\tthe\t_\t_\tDET\t_\t3\tdet\t_\t_\n",
      "3\tflight\t_\t_\tNOUN\t_\t1\tobj\t_\t_\n",
      "4\tthrough\t_\t_\tADP\t_\t5\tcase\t_\t_\n",
      "5\tHouston\t_\t_\tPROPN\t_\t3\tnmod\t_\t_\n"
     ]
    }
   ],
   "source": [
    "toy_tree_str = \"\"\"\n",
    "1\\tBook\\t_\\t_\\tVERB\\t_\\t0\\troot\\t_\\t_\n",
    "2\\tthe\\t_\\t_\\tDET\\t_\\t3\\tdet\\t_\\t_\n",
    "3\\tflight\\t_\\t_\\tNOUN\\t_\\t1\\tobj\\t_\\t_\n",
    "4\\tthrough\\t_\\t_\\tADP\\t_\\t5\\tcase\\t_\\t_\n",
    "5\\tHouston\\t_\\t_\\tPROPN\\t_\\t3\\tnmod\\t_\\t_\n",
    "\"\"\"\n",
    "toy_tree = DependencyTree.from_string(toy_tree_str)\n",
    "print(toy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy_data length: 11\n",
      "[(([0],[5, 4, 3, 2, 1],set()), ('shift', None)),\n",
      " (([0, 1],[5, 4, 3, 2],set()), ('shift', None)),\n",
      " (([0, 1, 2],[5, 4, 3],set()), ('shift', None)),\n",
      " (([0, 1, 2, 3],[5, 4],set()), ('left_arc', 'det')),\n",
      " (([0, 1, 3],[5, 4],{(3, 2, 'det')}), ('shift', None)),\n",
      " (([0, 1, 3, 4],[5],{(3, 2, 'det')}), ('shift', None)),\n",
      " (([0, 1, 3, 4, 5],[],{(3, 2, 'det')}), ('left_arc', 'case')),\n",
      " (([0, 1, 3, 5],[],{(5, 4, 'case'), (3, 2, 'det')}), ('right_arc', 'nmod')),\n",
      " (([0, 1, 3],[],{(5, 4, 'case'), (3, 2, 'det'), (3, 5, 'nmod')}),\n",
      "  ('right_arc', 'obj')),\n",
      " (([0, 1],[],{(1, 3, 'obj'), (5, 4, 'case'), (3, 2, 'det'), (3, 5, 'nmod')}),\n",
      "  ('right_arc', 'root')),\n",
      " (([0],[],{(1, 3, 'obj'), (3, 5, 'nmod'), (0, 1, 'root'), (5, 4, 'case'), (3, 2, 'det')}),\n",
      "  ('done', None))]\n"
     ]
    }
   ],
   "source": [
    "toy_data = get_training_instances(toy_tree)\n",
    "print('toy_data length:', len(toy_data))\n",
    "pprint(toy_data)\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# toy_data length: 11\n",
    "# [(([0],[5, 4, 3, 2, 1],set()), ('shift', None)),\n",
    "#  (([0, 1],[5, 4, 3, 2],set()), ('shift', None)),\n",
    "#  (([0, 1, 2],[5, 4, 3],set()), ('shift', None)),\n",
    "#  (([0, 1, 2, 3],[5, 4],set()), ('left_arc', 'det')),\n",
    "#  (([0, 1, 3],[5, 4],{(3, 2, 'det')}), ('shift', None)),\n",
    "#  (([0, 1, 3, 4],[5],{(3, 2, 'det')}), ('shift', None)),\n",
    "#  (([0, 1, 3, 4, 5],[],{(3, 2, 'det')}), ('left_arc', 'case')),\n",
    "#  (([0, 1, 3, 5],[],{(3, 2, 'det'), (5, 4, 'case')}), ('right_arc', 'nmod')),\n",
    "#  (([0, 1, 3],[],{(3, 5, 'nmod'), (3, 2, 'det'), (5, 4, 'case')}),\n",
    "#   ('right_arc', 'obj')),\n",
    "#  (([0, 1],[],{(3, 5, 'nmod'), (3, 2, 'det'), (5, 4, 'case'), (1, 3, 'obj')}),\n",
    "#   ('right_arc', 'root')),\n",
    "#  (([0],[],{(0, 1, 'root'), (3, 2, 'det'), (1, 3, 'obj'), (3, 5, 'nmod'), (5, 4, 'case')}),\n",
    "#   ('done', None))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with some sentence in dev.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data instances: 41\n",
      "[(([0],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],set()),\n",
      "  ('shift', None)),\n",
      " (([0, 1],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],set()),\n",
      "  ('shift', None)),\n",
      " (([0, 1, 2],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3],set()),\n",
      "  ('left_arc', 'det'))]\n"
     ]
    }
   ],
   "source": [
    "# Test results\n",
    "data = get_training_instances(dev_trees[2])\n",
    "print('Number of data instances:', len(data))\n",
    "pprint(data[:3])\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# Number of data instances: 41\n",
    "# [(([0],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1],set()),\n",
    "#   ('shift', None)),\n",
    "#  (([0, 1],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],set()),\n",
    "#   ('shift', None)),\n",
    "#  (([0, 1, 2],[20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3],set()),\n",
    "#   ('left_arc', 'det'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note**:\n",
    "- The last element in the returned list of instances is a pseudo instance with the label `\"done\"`. This is for demenstration purpose, and should not be used for training.\n",
    "- For actual training step, you need to post-process the data to convert each relation tuple to an integer index. \n",
    "- We have 39 unique dependency relations in the data, including `ROOT`. Considering `ROOT` only appears as the head in a `right_arc` action, we have $(39-1)\\times 2 + 1 = 77$ possible actions in total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example for converting the generated data instances into a a more *model-friendly* format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def process(dep_trees: List[DependencyTree], word_vocab: dict, pos_vocab: dict, action_vocab) -> torch.Tensor:\n",
    "    tensor_data = []\n",
    "    for tree in dep_trees:\n",
    "        instances = get_training_instances(tree)\n",
    "        for state, action in instances:\n",
    "            # TODO\n",
    "            # convert to torch tensor and append to tensor_data\n",
    "            for state, action in instances:\n",
    "                state_tensor = torch.tensor([word_vocab[word] for word in state.buffer],\n",
    "                                            dtype=torch.long)\n",
    "                stack_tensor = torch.tensor([word_vocab[word] for word in state.stack],\n",
    "                                            dtype=torch.long)\n",
    "                pos_tensor = torch.tensor([pos_vocab.get(pos, pos_vocab['<UNK>']) for pos in state.pos],\n",
    "                                          dtype=torch.long)\n",
    "                action_tensor = torch.tensor(action_vocab[action], dtype=torch.long)\n",
    "\n",
    "                tensor_data.append((state_tensor, stack_tensor, pos_tensor, action_tensor))\n",
    "\n",
    "\n",
    "    return torch.stack(tensor_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
