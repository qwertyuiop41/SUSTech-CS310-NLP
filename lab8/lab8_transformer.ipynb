{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 8: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Implement Self-Attention for a Single Head\n",
    "\n",
    "First, prepare the input data in shape $N\\times d$\n",
    "\n",
    "*Hint*: \n",
    "- Use `torch.randn` to generate a torch tensor in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.size(): torch.Size([3, 512])\n",
      "X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "d = 512\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### START YOUR CODE ###\n",
    "X = torch.randn(N, d)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test \n",
    "assert isinstance(X, torch.Tensor)\n",
    "print('X.size():', X.size())\n",
    "print('X[:,0]:', X[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# X.shape: (3, 512)\n",
    "# X[:,0]: [-1.1258398  -0.54607284 -1.0840825 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, initialize weight matrices $W^Q$, $W^K$, and $W^V$. We assume they are for a single head, so $d_k=d_v=d$\n",
    "\n",
    "Using $W^Q$ as an example\n",
    "- First initialize an empty tensor `W_q` in the dimension of $d\\times d_k$, using the `torch.empty()` function. Then initialize it with `nn.init.xavier_uniform_()`.\n",
    "- After `W_q` is initialized, obtain the query matrix `Q` with a multiplication between `X` and `W_q`, using `torch.matmul()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q.size(): torch.Size([3, 512])\n",
      "Q[:,0]: [-0.93220276 -0.10625853  0.0931127 ]\n",
      "K.size(): torch.Size([3, 512])\n",
      "K[:,0]: [ 0.40440953 -1.9039854   0.40878323]\n",
      "V.size(): torch.Size([3, 512])\n",
      "V[:,0]: [-0.51619816 -0.9439035   0.94877446]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0) # Do not remove this line\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "d_k = d // n_heads # Compute d_k\n",
    "\n",
    "# W_q = None\n",
    "# W_k = None\n",
    "# W_v = None\n",
    "W_q = nn.init.xavier_uniform_(torch.empty(d, d_k))\n",
    "W_k = nn.init.xavier_uniform_(torch.empty(d, d_k))\n",
    "W_v = nn.init.xavier_uniform_(torch.empty(d, d_k))\n",
    "\n",
    "\n",
    "\n",
    "# Compute Q, K, V\n",
    "# Q = None\n",
    "# K = None\n",
    "# V = None\n",
    "Q = torch.matmul(X, W_q)\n",
    "K = torch.matmul(X, W_k)\n",
    "V = torch.matmul(X, W_v)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert Q.size() == (N, d_k)\n",
    "assert K.size() == (N, d_k)\n",
    "assert V.size() == (N, d_k)\n",
    "\n",
    "print('Q.size():', Q.size())\n",
    "print('Q[:,0]:', Q[:,0].data.numpy())\n",
    "print('K.size():', K.size())\n",
    "print('K[:,0]:', K[:,0].data.numpy())\n",
    "print('V.size():', V.size())\n",
    "print('V[:,0]:', V[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# Q.size(): torch.Size([3, 512])\n",
    "# Q[:,0]: [-0.45352045 -0.40904033  0.18985942]\n",
    "# K.size(): torch.Size([3, 512])\n",
    "# K[:,0]: [ 1.509987   -0.5503683   0.44788954]\n",
    "# V.size(): torch.Size([3, 512])\n",
    "# V[:,0]: [ 0.43034226  0.00162293 -0.1317436 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, compute the attention scores $\\alpha$ and the weighted output\n",
    "\n",
    "Following the equation:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "*Hint*:\n",
    "- $\\alpha = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d_k}})$, where you can use `torch.nn.functional.softmax()` to compute the softmax. Pay attention to the `dim` parameter.\n",
    "- The weighted output is the multiplication between $\\alpha$ and $V$. Pay attention to their dimensions: $\\alpha$ is of shape $N\\times N$, and $\\alpha_{ij}$ is the attention score from the $i$-th to the $j$-th word. \n",
    "- The weighted output is of shape $N\\times d_v$, and here we assume $d_k=d_v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha.size(): torch.Size([3, 3])\n",
      "alpha: [[0.7866041  0.07920031 0.13419567]\n",
      " [0.18956533 0.5898989  0.22053581]\n",
      " [0.01180242 0.02406028 0.9641373 ]]\n",
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [-0.35347962 -0.44542217  0.88594586]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# alpha = None\n",
    "alpha = F.softmax(torch.matmul(Q, K.transpose(0, 1)) / torch.sqrt(torch.tensor(d_k)), dim=1)\n",
    "# output = None\n",
    "output = torch.matmul(alpha, V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "assert alpha.size() == (N, N)\n",
    "assert output.size() == (N, d_k)\n",
    "\n",
    "print('alpha.size():', alpha.size())\n",
    "print('alpha:', alpha.data.numpy())\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# alpha.size(): torch.Size([3, 3])\n",
    "# alpha: [[0.78344566 0.14102352 0.07553086]\n",
    "#  [0.25583813 0.18030964 0.5638523 ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.32742795  0.03610666 -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Mask Future Tokens\n",
    "\n",
    "First, create a binary mask tensor of size $N\\times N$, which is lower triangular, with the diagonal and upper triangle set to 0.\n",
    "\n",
    "*Hint*: Use `torch.tril` and `torch.ones`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: [[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# mask = None\n",
    "mask = torch.tril(torch.ones(N, N))\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('mask:', mask.data.numpy())\n",
    "\n",
    "# You should expect to see the following output:\n",
    "# mask: [[1. 0. 0.]\n",
    "#  [1. 1. 0.]\n",
    "#  [1. 1. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the mask to fill the corresponding future cells in $QK^\\top$ with $-\\infty$ (`-np.inf`), and then pass it to softmax to compute the new attention scores.\n",
    "\n",
    "*Hint*: Use `torch.Tensor.masked_fill` function to selectively fill the upper triangle area of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_alpha: [[1.         0.         0.        ]\n",
      " [0.40123224 0.5987678  0.        ]\n",
      " [0.21719831 0.21987708 0.5629246 ]]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# new_alpha = None\n",
    "masked_alpha = alpha.masked_fill(mask == 0, -np.inf)\n",
    "new_alpha = F.softmax(masked_alpha, dim=1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "\n",
    "# Test\n",
    "print('new_alpha:', new_alpha.data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_alpha: [[1.         0.         0.        ]\n",
    "#  [0.5865858  0.41341412 0.        ]\n",
    "#  [0.09271843 0.2767209  0.63056064]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the output should also be updated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_output.size(): torch.Size([3, 512])\n",
      "new_output[:,0]: [-0.51619816 -0.7722944   0.21442837]\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "# new_output = None\n",
    "\n",
    "new_output = torch.matmul(new_alpha, V)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print('new_output.size():', new_output.size())\n",
    "print('new_output[:,0]:', new_output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# new_output.size(): torch.Size([3, 512])\n",
    "# new_output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T3. Integrate Multiple Heads\n",
    "\n",
    "Finally, integrate the above implemented functions into the `MultiHeadAttention` class.\n",
    "\n",
    "**Note**:\n",
    "\n",
    "- In this class, the weight matrices `W_q`, `W_k`, and `W_v` are defined as tensors of size $d\\times d$. Thus, the output $Q=XW^Q$ is of size $N\\times d$.\n",
    "\n",
    "- Then we reshape $Q$ (and $K$, $V$ as well) into the tensor `Q_` of shape $N\\times h\\times d_k$, where $h$ is the number of heads (`n_heads`) and $d_k = d // h$. Similar operations are applied to $K$ and $V$. \n",
    "\n",
    "- The multiplication $QK^\\top$ is now between two tensors of shape $N\\times h\\times d_k$, `Q_` and `K_`, and the output is of size $h\\times N \\times N$. Thus, you need to use `torch.matmul` and `torch.permute` properly to make the dimensions of `Q_`, `K_`, and `V_` be in the correct order.\n",
    "\n",
    "- Also, remember to apply the future mask to each attention head's output. You can use `torch.repeat` to replicate the mask for `n_heads` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_k = nn.Parameter(torch.empty(d_model, d_model))\n",
    "        self.W_v = nn.Parameter(torch.empty(d_model, d_model))\n",
    "\n",
    "        nn.init.xavier_normal_(self.W_q)\n",
    "        nn.init.xavier_normal_(self.W_k)\n",
    "        nn.init.xavier_normal_(self.W_v)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        N = X.size(0)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # Q = None\n",
    "        # K = None\n",
    "        # V = None\n",
    "        # Q_ = None\n",
    "        # K_ = None\n",
    "        # V_ = None\n",
    "        #\n",
    "        # # Raw attention scores\n",
    "        # alpha = None\n",
    "        # # Apply the mask\n",
    "        # mask = None\n",
    "        # alpha = None\n",
    "        # # Softmax\n",
    "        # alpha = None\n",
    "        #\n",
    "        # output = None\n",
    "        # Linear transformations\n",
    "        Q = torch.matmul(X, self.W_q)\n",
    "        K = torch.matmul(X, self.W_k)\n",
    "        V = torch.matmul(X, self.W_v)\n",
    "\n",
    "        # Reshape Q, K, and V\n",
    "        Q_ = Q.view(N, self.n_heads, self.d_k).transpose(0, 1)\n",
    "        K_ = K.view(N, self.n_heads, self.d_k).transpose(0, 1)\n",
    "        V_ = V.view(N, self.n_heads, self.d_k).transpose(0, 1)\n",
    "\n",
    "        # Compute raw attention scores\n",
    "        alpha = torch.matmul(Q_, K_.transpose(1, 2))\n",
    "\n",
    "        # Apply the mask\n",
    "        alpha = torch.matmul(Q_, K_.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        mask = torch.tril(torch.ones((N, N), device=X.device)).repeat(self.n_heads, 1, 1)\n",
    "        alpha = alpha.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax\n",
    "        alpha = F.softmax(alpha, dim=-1)\n",
    "\n",
    "        # Compute output\n",
    "        output = torch.matmul(alpha, V_)\n",
    "\n",
    "        output = output.transpose(0, 1).contiguous().view(N, -1)\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.size(): torch.Size([3, 512])\n",
      "output[:,0]: [ 0.4303421   0.2531036  -0.04272253]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# N = 3\n",
    "# d = 512\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# X = torch.randn(N, d)\n",
    "\n",
    "multi_head_attn = MultiHeadAttention(d, n_heads=1)\n",
    "output = multi_head_attn(X)\n",
    "\n",
    "assert output.size() == (N, d)\n",
    "print('output.size():', output.size())\n",
    "print('output[:,0]:', output[:,0].data.numpy())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# output.size(): torch.Size([3, 512])\n",
    "# output[:,0]: [ 0.43034226  0.2531036  -0.04272257]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that the above output size and values should be the same as the previous one, as we used `n_heads=1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
