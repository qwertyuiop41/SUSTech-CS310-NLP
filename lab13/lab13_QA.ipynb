{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Lab 13: Explore Question-Answering Models and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will practice with running pretrained models on question-answering tasks. The we demonstrate with is `distilbert-base-uncased`, which is a smaller version of BERT.\n",
    "\n",
    "We will use the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) datast provided in the [Datasets](https://github.com/huggingface/datasets) library. Make sure to install the library:\n",
    "\n",
    "```bash\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1. Explore the SQuAD dataset\n",
    "\n",
    "First, let's load the SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/siyiwang/.cache/huggingface/datasets/downloads\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric,config\n",
    "print(config.DEFAULT_DOWNLOADED_DATASETS_PATH)\n",
    "\n",
    "squad_dataset = load_dataset('./squad')\n",
    "\n",
    "# # 指定本地 SQuAD 数据集的文件夹路径\n",
    "# data_dir = '/Users/siyiwang/Desktop/wsy/NLP/lab13/squad'\n",
    "#\n",
    "# # 加载本地 SQuAD 数据集\n",
    "# squad_dataset = load_dataset('squad', data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `squad_dataset` object is a `DefaultDict` that contains keys for the train and validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access a data instance, you can specify the split and index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '5733be284776f41900661182',\n 'title': 'University_of_Notre_Dame',\n 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that teh answer is indicated by its span start index (at character `515`) in the passage text. \n",
    "\n",
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>572797ce5951b619008f8e49</td>\n      <td>Nonprofit_organization</td>\n      <td>In the traditional domain noted in RFC 1591, .org is for \"organizations that didn't fit anywhere else\" in the naming system, which implies that it is the proper category for non-commercial organizations if they are not governmental, educational, or one of the other types with a specific TLD. It is not designated specifically for charitable organizations or any specific organizational or tax-law status, however; it encompasses anything that is not classifiable as another category. Currently, no restrictions are enforced on registration of .com or .org, so one can find organizations of all sorts in either of these domains, as well as other top-level domains including newer, more specific ones which may apply to particular sorts of organizations such as .museum for museums or .coop for cooperatives. Organizations might also register by the appropriate country code top-level domain for their country.</td>\n      <td>What is included in the list of organizations allowed to use .org?</td>\n      <td>{'text': ['encompasses anything that is not classifiable as another category'], 'answer_start': [418]}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>56dddfe89a695914005b9633</td>\n      <td>Dutch_Republic</td>\n      <td>During the Dutch Golden Age in the late 16th century onward, the Dutch Republic dominated world trade in the 17th century, conquering a vast colonial empire and operating the largest fleet of merchantmen of any nation. The County of Holland was the wealthiest and most urbanized region in the world.</td>\n      <td>What was the wealthiest and most urbanized region in the world during the 17th century?</td>\n      <td>{'text': ['The County of Holland'], 'answer_start': [219]}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>572f79b4a23a5019007fc66c</td>\n      <td>Han_dynasty</td>\n      <td>The most common staple crops consumed during Han were wheat, barley, foxtail millet, proso millet, rice, and beans. Commonly eaten fruits and vegetables included chestnuts, pears, plums, peaches, melons, apricots, strawberries, red bayberries, jujubes, calabash, bamboo shoots, mustard plant and taro. Domesticated animals that were also eaten included chickens, Mandarin ducks, geese, cows, sheep, pigs, camels and dogs (various types were bred specifically for food, while most were used as pets). Turtles and fish were taken from streams and lakes. Commonly hunted game, such as owl, pheasant, magpie, sika deer, and Chinese bamboo partridge were consumed. Seasonings included sugar, honey, salt and soy sauce. Beer and wine were regularly consumed.</td>\n      <td>Where were turtles acquired from?</td>\n      <td>{'text': ['streams and lakes'], 'answer_start': [533]}</td>\n    </tr>\n  </tbody>\n</table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(squad_dataset[\"train\"], num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2. Preprocess the data\n",
    "\n",
    "Before we feed the data to a model for fine-tuning, there is some preprocessing needed: \n",
    "- Tokenize the input text\n",
    "- Put it in the format expected by the model\n",
    "- Generate other inputs the model requires\n",
    "\n",
    "To do all of this, we need to instantiate a tokenizer that is compatible with the model we want to use, i.e., `distilbert-base-uncased`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\" # If loaded locally, make sure you have the model downloaded first\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly call this tokenizer on two sentences (e.g., question and context):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [101, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 102, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Architecturally, the school has a Catholic character.', 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important step in QA is to deal with very **long documents**. If longer than the maximum input size of model, then removing part of context might result in losing the answer.\n",
    "\n",
    "To handle this, we will allow a long document to give several input *features*, each of length shorter than the maximum size. \n",
    "\n",
    "Also, in case the answer is split between two features, we allow some overlap between features, controlled by `doc_stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine on one long example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(squad_dataset[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = squad_dataset[\"train\"][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without truncation, its length is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "396"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example['question'], example['context'])['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we truncate, the resulting length is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "384"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never want to truncate the question, so we specify `truncation='only_second`. \n",
    "\n",
    "Now, we further tell the tokenizer to return the overlaping features, by setting `return_overflowing_tokens=True` and `stride=doc_stride`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[384, 157]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "\n",
    "print([len(x) for x in tokenized_example[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the two features decoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(1)\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we nned to find out in which of the two features the answer is, and where exactly it starts and ends.\n",
    "\n",
    "Thankfully, the tokenizer can help us by returning the `offset_mapping` that gives the start and end character of each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "print(offsets[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, the very first token (`[CLS]`) has `(0, 0)` because it doesn't correspond to any part of the question/answer.\n",
    "\n",
    "The second token corresponds to the span from character 0 to 3 in the context, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens(token_id))\n",
    "\n",
    "token_offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(example[\"question\"][token_offsets[0]:token_offsets[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on to the next step, we just have to distinguish between the offsets for `question` and those for `context`. The `sequence_ids` method can be helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "\n",
    "print('len(sequence_ids):', len(sequence_ids))\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns None for the special tokens; then `0` for tokens from the first sequence (i.e., the `question`), and `1` for tokens from the second sequence (i.e., the `context`).\n",
    "\n",
    "It tells us that we need to find the span of answer among all `1` tokens.\n",
    "\n",
    "Now, we are ready to use `offset_mapping` to find the position of the start and end tokens of the `answer` in a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "answers = example[\"answers\"]\n",
    "ans_start = answers[\"answer_start\"][0]\n",
    "ans_end = ans_start + len(answers[\"text\"][0])\n",
    "\n",
    "print(answers)\n",
    "print('ans_start:', ans_start)\n",
    "print('end_char:', ans_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `token_start_index` and `token_end_index` be the initial search range for the answer span, initialize them properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the position of the first `1` token\n",
    "### START YOUR CODE ###\n",
    "# token_start_index = None\n",
    "token_start_index = next(i for i, x in enumerate(sequence_ids) if x == 1)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('token_start_index:', token_start_index)\n",
    "print('offsets[token_start_index]:', offsets[token_start_index])\n",
    "# Expected output\n",
    "# token_start_index: 16\n",
    "# offsets[token_start_index]: (0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Find the position of the last `1` token\n",
    "### START YOUR CODE ###\n",
    "# token_end_index = None\n",
    "token_end_index = next(i for i, x in enumerate(reversed(sequence_ids)) if x == 1)\n",
    "token_end_index = len(sequence_ids) - 1 - token_end_index\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('token_end_index:', token_end_index)\n",
    "print('offsets[token_end_index]:', offsets[token_end_index])\n",
    "# Expected output\n",
    "# token_end_index: 382\n",
    "# offsets[token_end_index]: (1665, 1669)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, detect if `ans_start` and `ans_end` is within the initial search range. \n",
    "\n",
    "If they do, then find the start and end indices of tokens, whose offsets encompass `ans_start` and `ans_end`, repectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "token_start_index = 16\n",
    "token_end_index = 382 # reset\n",
    "\n",
    "# Detect if the answer is within the initial search range\n",
    "### START YOUR CODE ###\n",
    "if ans_start < offsets[token_start_index][0] or ans_end > offsets[token_end_index][1]:\n",
    "    print('The answer is not in this feature.')\n",
    "### END YOUR CODE ###\n",
    "else:\n",
    "    # Find the start and end indices of the tokens, whose offsets encompass the ans_start and ans_end\n",
    "    ### START YOUR CODE ###\n",
    "    start_position = None\n",
    "    end_position = None\n",
    "    for i, (start, end) in enumerate(offsets):\n",
    "        if ans_start >= start and ans_start < end:\n",
    "            start_position = i\n",
    "        if ans_end > start and ans_end <= end:\n",
    "            end_position = i\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Test\n",
    "print(start_position, end_position)\n",
    "print(offsets[start_position], offsets[end_position])\n",
    "\n",
    "# Expected output\n",
    "# 23 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can double check that it is indeed the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
