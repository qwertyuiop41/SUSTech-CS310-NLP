{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSz5jzj61nHc"
   },
   "source": [
    "## CS310 Natural Language Processing\n",
    "## Assignment 5 (part 1): Pretraining BERT with Masked Language Modeling and Next Sentence Prediction on Toy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8kZmr4ItGUj"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We start by assigning a raw text for training. In fact, the pretraining corpus of Bert including 33B words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6YMNvc8tbA9"
   },
   "outputs": [],
   "source": [
    "text = (\n",
    "        'Hello, how are you? I am Romeo.\\n'\n",
    "        'Hello, Romeo! My name is Juliet. Nice to meet you.\\n'\n",
    "        'Nice meet you too. How are you today?\\n'\n",
    "        'Great. My baseball team won the competition.\\n'\n",
    "        'Oh Congratulations, Juliet\\n'\n",
    "        'Thank you Romeo'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the following step, it is important to remember that `BERT` takes special tokens during training. Here is a table explaining the purpose of various tokens:\n",
    "\n",
    "| Token      | Purpose |\n",
    "| ----------- | ----------- |\n",
    "| [CLS]      | The first token is always classification      |\n",
    "| [SEP]  |   Separates two sentences      |\n",
    "| [PAD]   |  Use to truncate the sentence with equal length.       |\n",
    "| [MASK]   |    Use to create a mask by replacing the original word.     |\n",
    "\n",
    "These tokens should be included in the word dictionary where each token and word in the vocabulary is assigned with an index number. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhX8b1ydtrVf"
   },
   "outputs": [],
   "source": [
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "word_types = sorted(list(set(\" \".join(sentences).split())))\n",
    "\n",
    "# Add the special tokens to the vocabulary\n",
    "word_to_id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_types):\n",
    "    word_to_id[w] = i + 4\n",
    "id_to_word = {i: w for i, w in enumerate(word_to_id)}\n",
    "VOCAB_SIZE = len(word_to_id)\n",
    "\n",
    "tokens_list = []\n",
    "for sentence in sentences:\n",
    "    tokens = [word_to_id[s] for s in sentence.split()]\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "# Test\n",
    "pprint(tokens_list)\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# [[10, 11, 5, 28, 12, 4, 20],\n",
    "#  [10, 20, 16, 17, 13, 14, 18, 24, 15, 28],\n",
    "#  [18, 15, 28, 26, 11, 5, 28, 25],\n",
    "#  [9, 16, 6, 21, 27, 23, 7],\n",
    "#  [19, 8, 14],\n",
    "#  [22, 28, 20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q03SGkfIu_Kd"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 30 # maximum of width of a batch\n",
    "MAX_PRED = 5  # maxium number predictions to make in a batch\n",
    "\n",
    "batch_size = 6\n",
    "n_layers = 6 # number of Encoder of Encoder Layer\n",
    "n_heads = 12 # number of heads in Multi-Head Attention\n",
    "d_model = 768 # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2 # number of segments, ex) sentence A and sentence B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next step will be to create masking. \n",
    "\n",
    "As mentioned in the original paper, BERT randomly assigns masks to 15% of the sequence. But keep in mind that you don’t assign masks to the special tokens. For that, we will use conditional statements.\n",
    "\n",
    "We first replace 15% of the words with [MASK] tokens, and once that is done, we will add padding. \n",
    "\n",
    "Padding is to make sure that all the sentence pairs in the batch are of equal length. \n",
    "\n",
    "For example, if we take the sentence : “*The dog is barking at the tree. The cat is walking.*”\n",
    "\n",
    "then with padding, it will look like this: \n",
    "\n",
    "“`[CLS]` The dog is barking at the tree. `[CLS]` The cat is walking. `[PAD]` `[PAD]` `[PAD]`” \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtyOOmRntu8w"
   },
   "outputs": [],
   "source": [
    "def make_batch(tokens_list: List[int], batch_size: int, word_to_id: Dict):\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    \n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "        sent_a_index, sent_b_index= random.randrange(len(tokens_list)), random.randrange(len(tokens_list))\n",
    "        tokens_a, tokens_b= tokens_list[sent_a_index], tokens_list[sent_b_index]\n",
    "\n",
    "        input_ids = [word_to_id['[CLS]']] + tokens_a + [word_to_id['[SEP]']] + tokens_b + [word_to_id['[SEP]']]\n",
    "        segment_ids = [1] * (1 + len(tokens_a) + 1) + [2] * (len(tokens_b) + 1)\n",
    "\n",
    "        # The following code is used for the Masked Language Modeling (MLM) task.\n",
    "        n_pred =  min(MAX_PRED, max(1, int(round(len(input_ids) * 0.15)))) # Predict at most 15 % of tokens in one sentence\n",
    "        masked_candidates_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word_to_id['[CLS]'] and token != word_to_id['[SEP]']]\n",
    "        random.shuffle(masked_candidates_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in masked_candidates_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            ### START YOUR CODE ###\n",
    "            # Throw a dice to decide if you want to replace the token with [MASK], random word, or remain the same\n",
    "            pass\n",
    "            ### END YOUR CODE ###\n",
    "\n",
    "        # Make zero paddings\n",
    "        n_pad = MAX_LEN - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "\n",
    "        # Zero padding (100% - 15%) of thetokens\n",
    "        if MAX_PRED > n_pred:\n",
    "            n_pad = MAX_PRED - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "\n",
    "        # The following code is used for the Next Sentence Prediction (NSP) task.\n",
    "        ### START YOUR CODE ###\n",
    "        # Decide if the is_next label is positive or negative, by comparing sent_a_index and sent_b_index\n",
    "        # Don't forget to increment the positive/negative count\n",
    "        pass\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating attention mask\n",
    "BERT needs attention masks. And these should be in a proper format. The following code will help you create masks. \n",
    "\n",
    "It will convert the `[PAD]` to `1` and elsewhere `0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_attn_mask(seq_q, seq_k):\n",
    "    batch_size, seq_len = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=seq_len), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, seq_len, len_k)  # batch_size x seq_len x len_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1PGksqBNuZM"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "random.seed(0)\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "sample = 2\n",
    "\n",
    "print('sampled text:')\n",
    "print([id_to_word[w.item()] for w in input_ids[sample] if id_to_word[w.item()] != '[PAD]'])\n",
    "print()\n",
    "print('input_ids:', input_ids[sample])\n",
    "print('segment_ids:', segment_ids[sample])\n",
    "print('masked_tokens:', masked_tokens[sample])\n",
    "print('masked_pos:', masked_pos[sample])\n",
    "print('is_next:', is_next[sample].item())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# sampled text:\n",
    "# ['[CLS]', '[MASK]', 'my', 'baseball', 'team', 'won', 'the', 'competition', '[SEP]', 'oh', 'congratulations', 'juliet', '[SEP]']\n",
    "\n",
    "# input_ids: tensor([ 1,  3, 16,  6, 21, 27, 23,  7,  2, 19,  8, 14,  2,  0,  0,  0,  0,  0,\n",
    "#          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
    "# segment_ids: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#         0, 0, 0, 0, 0, 0])\n",
    "# masked_tokens: tensor([ 9, 19,  0,  0,  0])\n",
    "# masked_pos: tensor([1, 9, 0, 0, 0])\n",
    "# is_next: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding layer\n",
    "The embedding is the first layer in BERT that takes the input and creates a lookup table. The parameters of the embedding layers are learnable, which means when the learning process is over the embeddings will cluster similar words together. \n",
    "\n",
    "The embedding layer also preserves different relationships between words such as: semantic, syntactic, linear, and since BERT is bidirectional it will also preserve contextual relationships as well. \n",
    "\n",
    "In the case of BERT, it creates three embeddings for \n",
    "- Token, \n",
    "- Segments,\n",
    "- Position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qnay0LTDjE4S"
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(VOCAB_SIZE, d_model, padding_idx=0)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(MAX_LEN, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments + 1, d_model, padding_idx=0)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "The encoder has two main components: \n",
    "\n",
    "- Multi-head Attention\n",
    "- Position-wise feed-forward network. \n",
    "\n",
    "The work of the encoder is to find representations and patterns from the input and attention mask. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention\n",
    "\n",
    "This is the first of the main components of the encoder. \n",
    "\n",
    "The attention model takes as input a sequence `x` and a corresponding attention mask `attn`. \n",
    "\n",
    "Query, Key, and Value are computed from `x` and `atten`, based on which the contextual is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUX_eM_E1B8p"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x seq_len x =seq_len]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        # x: [batch_size x seq_len x d_model]\n",
    "        residual, batch_size = x, x.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(x).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x seq_len x d_k]\n",
    "        k_s = self.W_K(x).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x seq_len x d_k]\n",
    "        v_s = self.W_V(x).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x seq_len x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x seq_len x seq_len]\n",
    "\n",
    "        # context: [batch_size x n_heads x seq_len x d_v], attn: [batch_size x n_heads x seq_len x seq_len]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x seq_len x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x seq_len x d_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-Wise Feed Forward Network\n",
    "\n",
    "The output from the multihead goes into the feed-forward network and that concludes the encoder part.\n",
    "\n",
    "Let’s take a breath and revise what we’ve learned so far:\n",
    "\n",
    "The input goes into the embedding and as well attention function. Both of which are fed into the encoder which has a multi-head function and a feed-forward network. \n",
    "The multi-head function itself has a function that operates the embeddings and attention mask using a dot product operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GQFL_Va4N4Y"
   },
   "outputs": [],
   "source": [
    "# Activation function: GELU (Gaussian Error Linear Unit)\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use `Multi-head Attention` and `Position-Wise Feed Forward Network` to construct the Encoder. \n",
    "\n",
    "*Hint*: Call `self.enc_self_attn` and `self.pos_ffn` in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgmfjTqw4Qnw"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        ### START YOUR CODE ###\n",
    "        enc_outputs, attn = None, None\n",
    "        ### END YOUR CODE ###\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "enc_layer = EncoderLayer()\n",
    "enc_self_attn_mask = get_pad_attn_mask(input_ids, input_ids)\n",
    "embedding = Embedding()\n",
    "enc_inputs = embedding(input_ids, segment_ids)\n",
    "enc_outputs, attn = enc_layer(enc_inputs, enc_self_attn_mask)\n",
    "\n",
    "print('enc_outputs:', enc_outputs.size())\n",
    "print('attn:', attn.size())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# enc_outputs: torch.Size([6, 30, 768])\n",
    "# attn: torch.Size([6, 12, 30, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling all the components\n",
    "Let’s continue from where we left, i.e. the output from the encoder.\n",
    "\n",
    "The encoder yields two outputs: \n",
    "\n",
    "- One which comes from the feed-forward layer and \n",
    "- the Attention mask. \n",
    "\n",
    "It’s key to remember that BERT does not explicitly use a decoder. Instead, it uses the output and the attention mask to get the desired result. \n",
    "\n",
    "Although the decoder section in the transformers is replaced with a shallow network which can be used for classification as shown in the code below.\n",
    "Also, BERT outputs two results: one for the classifier and the other for masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ0TJ84W4SZw"
   },
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "         # for NSP task\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "\n",
    "        # for MLM task\n",
    "        self.fc2 = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight # decoder is shared with embedding layer\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_pad_attn_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "            # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "\n",
    "        # Use the representation of [CLS] to produce logits for NSP task\n",
    "        ### START YOUR CODE ###\n",
    "        logits_clsf = None\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Gather the representations of masked tokens to produce logits for MLM task\n",
    "        ### START YOUR CODE ###\n",
    "        logits_lm = None\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return logits_lm, logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "model = BERT()\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "\n",
    "print('logits_lm:', logits_lm.size())\n",
    "print('logits_clsf:', logits_clsf.size())\n",
    "\n",
    "# You should expect to see the following results:\n",
    "# logits_lm: torch.Size([6, 5, 29])\n",
    "# logits_clsf: torch.Size([6, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll start the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8UAG3SEP4UbU",
    "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss() # You can also try two separate losses for each task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch(tokens_list, batch_size, word_to_id)\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    logits_lm, logits_clsf = None, None \n",
    "    # Hint: Check the shape of logits_lm and decide if post-processing is needed\n",
    "    loss = None\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can observe a quick decrease in the loss, then you are on the right track. It should stablize to around 2.xx after a few hundred epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD3K8T6B4YJp",
    "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "sample = 1\n",
    "\n",
    "# Predict mask tokens and is_next\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, is_next = map(torch.LongTensor, zip(batch[sample]))\n",
    "# print(text)\n",
    "print([id_to_word[w.item()] for w in input_ids[0] if id_to_word[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "predicted_ids = logits_lm.argmax(dim=2).squeeze().data.numpy()\n",
    "n_masked_tokens = torch.sum(masked_tokens[0]!=0).item()\n",
    "print('masked tokens ground truth: ',[id_to_word[pos.item()] for pos in masked_tokens[0][:n_masked_tokens]])\n",
    "print('predicted masked tokens: ',[id_to_word[pos] for pos in predicted_ids[:n_masked_tokens]])\n",
    "\n",
    "predicted_is_next = logits_clsf.argmax(dim=1).data.numpy()[0]\n",
    "print('is_next ground truth : ', True if is_next else False)\n",
    "print('predicted is_next: ',True if predicted_is_next else False)\n",
    "\n",
    "# There is “correct” answer to this task, best of luck with your training!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
